\section{Modeling Procedures via Channelized Hypergraphs}
\phantomsection\label{sFour}
\p{Assuming we have a suitable Source Code Ontology, software 
procedures can be seen from two perspectives.  On the 
one hand, they are examples of well-formed code graphs: 
annotated graph structures convey the lexical symbols, 
input/output parameters (via different \q{abstractions}, 
in the sense of \mOldLambda{}-abstraction, subject to 
relevant channel protocols), and calls to other procedures, 
through which any given procedure's functionality is 
achieved.  On the other hand, we can see procedures as 
instances of function-like types, where the types carried 
in each channel determine the type of the procedure itself, 
as a functional value.  Although these two perspectives are 
usually mutually consistent, the notion of functional 
values is more general than procedures which are expressly 
implemented in computer code.  In particular, as I briefly 
mentioned earlier, sometimes functional values are denoted 
via inter-function operators (like the composition 
\fOfG{}) rather than by giving an explicit implementation.  
We can say that functions defined via operators 
(like \Ofop{}) lack a \q{function body}.  
}
\p{Going forward, I will generally use the term \i{procedure} 
with reference to function-like type instances that are 
defined \i{with} function bodies: that is, they are 
associated with sections of code that supply the procedure's 
implementation, and can be represented via code-graphs.  
I will use the term \i{function} more generally for 
instances of function-like types, irregardless of their 
provenance.  In particular, functions are \i{values} 
\mdash{} instances of types in a relevant type-system 
\TyS{} \mdash{} whereas I will not usually discuss procedures 
as \q{values}.  On the other hand, code-graphs capture 
the implementations through which function-like types 
are (mostly) populated with concrete values.   
}
\p{To model the general maxim that any coding assumptions 
made (but not verified) by one procedure \mdash{} say, \ProcOne{} \mdash{} 
should be tested by other procedures which call \ProcOne{}, we need 
a systematic outline capturing the notion of procedures calling 
other procedures, in the course of their own implementation.  
Here I propose to model these details via channels and 
interrelationships between channels.  Moreover, channels 
can be seen as structures on \i{graphs}, as well as 
runtime information flows, so that channels are applicable 
for both static and dynamic program analysis.    
}
\p{One consequence of my graph-oriented 
approach is that the technical distinctions between 
procedures and function-values (in general) have to be 
duly observed.  There are some relevant complications appertaining to 
the general picture of source-code segments instantiating 
function-like types.  I will briefly review these issues now, 
before pivoting to more macro-scale themes concerning 
Requirements Engineering via code models.
}
\vspace{-.1em}
%\spsubsection{Initializing Function-Typed Values}
\subsection{Initializing Function-Typed Values}
\p{Although in general function-typed values are \i{initialized}
from code-graphs that blueprint their implementation,
this glosses over several different mechanisms by which
function-typed values may be defined:
\begin{enumerate}\eli{}  \phantomsection\label{funconstr}In the simplest case, there is
a one-to-one relationship
between a code graph and an implemented function (\fFun{}, say).
If \fFun{} is polymorphic, in this case, it must be an example
of subtype (or \q{runtime}) polymorphism where the declared types of \fFun{}'s
parameters are actually instantiated, at runtime, by values
of their subtypes.
\eli{}  A different situation (\q{compile-time} polymorphism) applies
to generic code as in \Cpp{}
templates.  Here, a single code-graph generates multiple function
bodies, which differ only by virtue of their expected types.
For example, a templated \sortfn{} function will generate
multiple function bodies \mdash{} one for integers, say, one for strings,
etc.  These functions may be structurally similar, but they have
different signatures by virtue of working with different types.  This
means that symbols used in the function-bodies may refer to
different functions even though the symbols themselves do not vary
between function-bodies (since, after all, they come from the
same node in a single code-graph).  That is, the code-graphs
rely on symbol-overloading for function names
to achieve a kind of polymorphism, where one code-graph
yields multiple bodies.
\pseudoIndent{}
In this compile-time polymorphism,
symbols are resolved to the proper overload-implementation
at compile-time, whereas in runtime polymorphism this
decision is deferred until the runtime-polymorphic function
is actually being executed.  The key difference is that
runtime-polymorphic functions are \i{one} function-typed value,
which can work for diverse types only via subtyping \mdash{}
or via more exotic forms of indirection, like
using function-pointers in place of function symbols; whereas
compile-time-polymorphic (i.e., templated) functions are
\i{multiple} values, which share the same code-graph
representation but are otherwise unrelated.
\eli{}  \label{ops}A third possibility for producing function-like 
values is to define operators on function-like types themselves, which transform
function-like values to other function-like values, by analogy
to how arithmetic operations transform numbers to other
numbers.  As will be discussed below, this may or may not be
different from initializing function-like values via code-graphs.
For instance, given the composition operator \Ofop{},
\fDotOfg{} may or may not be treated as only a convenient
shorthand for a code graph spelling out something like \fgx{}.
\eli{}  \label{Curry}Finally, as a special case of operators on function-typed values,
one function may be obtained from another by \q{Currying}, that is,
fixing the value of one or more of the original function's
arguments.  For example, the \inc{} (\q{increment}) function which adds
\litOne{} to a value is a special case of addition, where the added value
is always \litOne{}.  Here again, Currying may or may not be
treated as a function-value-initialization process different from ones
starting from code-graphs.
\end{enumerate}
}
\p{The differences between how languages may process the \i{initialization}
of function-type values, which I alluded to in (\ref{ops}) and (\ref{Curry}), 
reflect differences in how function-like values are internally represented.
We \i{might} treat all initializations of these
values as via code-graphs (in practice, compiled down via an 
Abstract Syntax Tree or graph to some Intermediate Representation or byte-code).  
Suppose we have an \addFun{} function
and want to define an \inc{} function, as in \incimpl{}.  Even if a language has
a special Currying notation, that notation could translate behind-the-scenes to
an explicit function body, like the code at the end of the last sentence.
Alternatively, however, a language engine may also note that \inc{} is derived from \addFun{}
and can be wholly described by a handle denoting \addFun{}
(a pointer, say) along with a designation of the fixed value: in
other words, \addOne{}.  Instead of initializing \inc{} from a code-graph,
the language can represent it via a two-part data structure like
\addOne{} \mdash{} but only if the language \i{can} represent
function-typed values as compound data structures.
}
\p{Let's assume a language can always represent \i{some} function-typed values,
ones that are obtained from code-graphs, via pointers to
(or some other unique identifier for) an internal
memory area where at least \i{some} compiled function bodies are stored.
The interesting question is whether \i{all}  function-typed values
are represented in this manner and, in either case, the
consequences for the semantics of functional types \mdash{} semantic
issues such as \fOfg{} composition operators and Currying
(and also, as I will argue, Dependent Types).
}
%\spsubsection{Addressability and Implementation}
\subsubsection{Addressability and Implementation}
\p{Talk about polymorphism in a language like
\Cpp{} covers several distinct language features: achieving
code reuse by templating on type symbols is internally very different
from using virtual methods calls.  The key difference \mdash{} highlighted
by the contrast between runtime- and compile-time polymorphism \mdash{} is
that there are some function implementations which actually
compile to \i{single} functions, meaning in
particular that their compiled code has a single place in memory and
that they may be invoked through function pointers.  Conversely,
what appears in written code as one function body may actually be
duplicated, somewhere in the compiler workflow, generating multiple
function-like values.  The most common cases of such duplication
are templated code as discussed above (though there are
more exotic options, e.g. via \Cpp{} macros and/or
repeated file \codeinclude{}s).  Implementations of the first sort I will
call \q{addressable}, whereas those of the second 
produce multiple addressable values.  These concepts prove to be consequential
in the abstract theory of types, although for non-obvious reasons.
}
\p{To see why, consider first that type systems are intrinsically
pluralistic: there are numerous details whereby the type system
underlying one computing environment can differ from those employed
by other environments.  So there is no single, universal
\q{Type Expression Language}.  One role of any given
\TXL{} is to model what its corresponding language
recognizes as a type, or \mdash{} better \mdash{} a
\i{potential} type.  A \TXL{} expression which designates
a (unique) type is well-formed if it unambiguously
describes a type that \i{could} exist.  Such an
expression does not, however, implement the
type on its own, or mandate that the type be implemented;
it would merely affirm that the type so designated
is implementable within the target language.
}
\p{As a concrete example, consider a type described in
English as: \q{the type inhabited by functions 
which take, as one parameter, a Unicode string, and,
as the second parameter, an unsigned integer less than the
length of the string}.  A \TXL{} version of this specification
would only be valid if the requirements thereby described
can be satisfied, in the target language, via type-checking
alone.
}
\p{For a more in-depth example, if in \Cpp{} I
assert \q{\templateTMyList{}}, it would then be consistent with
a \Cpp{}-specific \TXL{}
to describe a type as \MyListInt{} (assume this will be implemented 
as a list of integers).  However,
the type \MyListInt{} is not, without further code, actually implemented.
It is a \i{possible} type because its description conforms to a relevant
\TXL{}, but not an \i{actual} type.  If a programmer supplies
a templated implementation for \TMyList{},
then the compiler can derive a \q{specialization} of the
template for a specific \TType{} \mdash{} or the programmer can specialize
\MyList{} on \int{} (or any other chosen type) manually.  
But in either case the actualization of
\TMyList{} will depend on an implementation (either a templated implementation
that works for multiple types or a specialization for a
single type); this is separate and apart from \TMyList{} being
a valid \i{expression} denoting a \i{possible} type.
}
\p{Templates and specialization add complexity to discussions
about types, because compilers may automatically instantiate
concrete types from templated code \i{unless} programmers
supply specializations which deviate from the template.
As a result, in a local segment of a source file it may be impossible
to know whether or not the code concretizing a templated type 
is automatically generated from a template.
Another complication is that compilers may derive
\i{default implementations} of types' constructors, unless
these are coded explicitly.  Taking these two considerations
together, it can be difficult in a code base to, 
given a type, find which code-segments correspond to
that types' constructors.
}
\p{As an analytic device, here I assume that every implementable
type can be associated with a procedure I will call a
\i{co-constructor}, whose role is to wrap constructor-calls
in a readily identifiable code body.  Co-constructors are
\q{ordinary} procedures in the sense that they are 
\q{addressable}.  Specifically, addressable procedures 
have these properties:
\begin{enumerate}\item{} You can take their address (assuming we are dealing
with a language that supports function pointers in the
first place).
\item{} They have a corresponding (possibly templated) location in
source code (and therefore a code-graph).  For co-constructors,
this location can be marked as
such \mdash{} it should be straightforward to identify all co-constructor
implementations in a code base.
\item{} They can be exposed to scripting engines and
runtime reflection; so co-constructors enable type-instances
to be created via scripts and other
runtime-introspection capabilities.
\end{enumerate}
Operationally, co-constructors are similar to
\i{factory procedures} or \i{object factories}
(see e.g. \cite[esp. pages 32-35]{ChochlikNaumann},
\cite[esp. pages 35-36]{JeremiahDangler},
\cite{DawidIreno}, \cite{McNattBieman}),
which similarly delegate to constructors but
can be used in contexts where constructors
cannot, e.g. where it is necessary to address
the factory through a pointer (note that in \Cpp{}
you may not take the address of an actual constructor).
}
\p{Insofar as co-constructors are \i{addressable}, they
provide an indirect mechanism for designating their
corresponding type.  I will use the term \i{preconstructor}
to mean a function-pointer holding the address of a
co-constructor, or some similar data structure which
uniquely identifies a co-constructor.  A preconstructor thereby
holds a compact value which is associated with exactly
one type.  A valid preconstructor, in particular, serves as proof
that a given type is implemented \mdash{} it confirms the
existence of at least one fully implemented constructor
for that type, indicating that the type is \i{actual} and
not just \i{potential}.
}
\p{Suppose certification
requires that the function which displays the gas level on a car's dashboard
never attempts to display a value above \litOH{} (intended to mean \q{One Hundred percent},
or completely full).  One way to ensure this specification is to declare
the function as taking a \i{type} which, by design, will only ever include
whole numbers in the range \ZeroToOneHundred{}.  Thus, a type system may support
such a type by including in its \TXL{} notation for \q{range-delimited} types,
types derived from other types by declaring a fixed range of allowed values.
A notation might be, say, \IntZToOH{}, for integers in the \ZeroToOneHundred{}
range \mdash{} or, more generally expressions like \TVOneToVTwo{}, meaning a \i{type} derived
from \TType{} but restricted to the range spanned by \VOne{} and \VTwo{} (assumed to be
values of \TType{} \mdash{} notice that a \TXL{} supporting this notation must
consequently support some notation of specific values, like numeric literals).
}
\p{However, merely describing range-delimited types' desired space of
values does not provide a full implementation specification.
What should happen if
someone tries to construct an \IntZToOH{} value with the number, say,
\litOHO{}?  What about with values taken from an external
source, like a web \API{}, where it cannot be formally
proven that the values fall in the proper range?  These
question point to implementation choices that transcend
formal designations.  This is why \TXL{} expressions
should be seen as just articulating \i{potential} types,
because bringing types into actuality will usually
call for engineering choices that transcend type
theory \i{per se}.  Once types \i{are} implemented,
co-constructors serve as tangible witness to
types' actualization, and preconstructors are
convenient proxies referring to those types.\footnote{Similar issues are sometimes addressed by a
\i{modal} type theory (cf., e.g., \cite{MurdochGabbay})
where (in one interpretation) a \i{logical}
assertion about a type may be \i{possible} but not necessary
(the modality ranging over \q{computing environments}, which
act like \q{possible worlds}).
}
}
\p{Reasoning abstractly about functions and types needs to be differentiated from
reasoning about available, implemented types (and functions defined 
on them).  Consider function pointers: what is the address of \fofg{}
if that expression is interpreted in and of itself
as evaluating to a functional value?\footnote{\label{fofgplausible}In my perspective
here, \fofg{} may be a \i{plausible} value, but it is
not an \i{actual} value without being implemented,
whether via a code graph (spelling out the equivalent of \lambdaxfgx{})
or some indirect/behavioral description (analogous to \inc{}
represented as \addOne{}).
}  This suggests
that a composition operator does not work in function-like
types quite like arithmetic operators in numeric types
(which is not unexpected insofar as functional values,
internally, are more like pointers than numbers-with-arithmetic).\footnote{Of course, languages are free to implement
functions behind the scenes to expand (say) \fofg{}, but
then \fofg{} is just syntactic sugar (even if its purpose
is not just to neaten source code, but also to inspire programmers
toward thinking of function-composition in quasi-arithmetic ways).
}  To put it differently, an \addressOf{} operator
\i{may} be available for \fofg{} if it is available for \fFun{} and
\gFun{}, but this depends on language design; it is not an
abstract property of type systems.
}
\p{A similar discussion applies to \q{Currying} \mdash{} the proposal
that types \tOnetoTwotoThree{} and  \tOnetoTwoTOThree{} are
equivalent, in that fixing one value as argument to a
binary function yields a new unary function.  Again,
since the Curried function is not necessarily implemented,
there is a \i{modal} difference between \tOnetoTwotoThree{}
and  \tOnetoTwoTOThree{}.  Languages \i{may} be engineered
to silently Curry any function on demand, but purported
\tOnetoTwotoThree{} and \tOnetoTwoTOThree{} 
equivalence is not a \i{necessary} feature of type systems.
}
\p{To the extent that both mathematical and programming concepts have a place here, we
find a certain divergence in how the word \q{function} is used.  If I say that
\q{there exists a function from \tOne{} to \tTwo{}}, where \tOne{} and \tTwo{} are
(not necessarily different) types, then this statement has two possible interpretations.
One is that, mathematically, I can assume the existence of a \tOneTotTwo{} mapping
by appeal to some sort of logic; the other is that a \tOneTotTwo{} function actually
exists in code.  This is not just a \q{metalanguage} difference projected
from how the discourse of mathematical type theory is used to different ends than discourses
about engineered programming languages, which are social as well as digital-technical
artifacts.  Instead, we can make the difference exact: when a function-value 
is keyed to a procedure, it is bound to a segment of code subject to 
analysis and to alternative representations (such as code graphs).  
}
\p{Since co-constructors are \i{addressable}, they 
cannot \mdash{} at least not within the framework I have 
discussed thus far \mdash{} be \q{temporary} 
function-values analogous to \fOfg{}.  
This means that \i{types} cannot be temporary values.  
More precisely, a type system may be constrained 
by the proposition that \i{no type can be created} 
whose co-constructors would have to be temporary 
values \mdash{} or, to put it differently, 
no type can be created whose co-constructors are 
not procedures that can be mapped to source-code 
segments (and thereby to code-graphs).   
}
\p{Notice that co-constructors then are not just 
function-like values; co-constructors have to be 
in that subspace of function-like values initialized 
via code-graphs, rather than via some quasi-arithmetic 
inter-function operator like \fOfG{}.
This then limits what we can do with 
Dependent Types, typestate, and other \q{expressive} 
type mechanisms.  I will call this the 
\q{metaconstructor} problem: insofar as co-constructors 
are function-like values, they (in principle) need 
their own constructors \mdash{} call these \q{metaconstructors}.  
We can stipulate that metaconstructors \mdash{} constructors of 
co-constructors \mdash{} have to be derived from code 
graphs (they cannot be temporary values), but 
this renders certain advanced type-theoretic 
features inaccessible to our applied type systems.  
Conversely, we can accept the idea of constructors 
being (potentially) temporary values, but this 
interferes with the idea of preconstructors being 
referential proxies for types themselves 
(unless types also are, potentially, temporary 
constructs, which creates a new set of problems).  
I will now explain this choice in greater depth.
}
