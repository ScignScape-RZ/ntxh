
`section.Cognitive Transform Grammar and Transform-Pairs`

`p.
The idea that inter-word pairs are a foundational linguistic 
unit %-- from which larger aggregates can be built up recursively 
%-- is an central tenet of Dependency Grammar.  Here I will 
generalize this perspective outside (but not excluding) 
grammar, to overall semantic, pragmatic, and even 
extralinguistic relations indicated via interword relations.    
`p`

`p.
In some cases word-relations can still be theorized mostly via 
syntax.  Consider hypothetical, example sentences like 

`sentenceList,
`sentenceItem; \label{itm:having} His having lied in the past damages his credibilty in the present.
`sentenceItem; \label{itm:whether} Voters question whether he is truthful this time around. 
`sentenceList`

In (\ref{itm:having}), `i.having` is necessary to syntactically transform its ground 
`i.lied` from a verb-form to a noun (something which can be inserted into a 
possessive clause).  Analogously, in (\label{ref:whether}) `i.whether` modifies 
`i.is` (since this is the head of a subordinate clause), wrapping a propositional 
clause into a noun so that it furnishes a direct object to the verb 
`i.question`/.  The essential transformation in these cases is 
motivated by grammatic considerations, particularly part-of-speech: 
a verb and a subordinate, propositionally complete clause (in \label{ref:having} and 
\label{ref:whether}, respectively) need (for syntactic propriety) to be 
modified so as to play a role in a site where a noun is expected 
(in effect, they need to be bundled into a noun-phrase).
`p`

`p.
The relevant transforms here %-- signified by `i.having` and `i.whether` %-- have a 
semantic dimension also, and we can speculate that the syntactic 
rules (requiring a verb or propositional-clause to be transformed into a 
noun) are actually driven by semantic considerations.  Conceptually, 
for example, `i.his having lied` packages a verb into a possessive 
context because the sentence is not foregrounding a specific lying-event 
but rather the fact of the existence of such occasions.  We cannot perhaps 
`q.possess` an event, but we possess (as part of our nature or history) 
the fact of past occurances, viz., events in the form of things we 
have done.  In this sense `i.his having lied` marks a conceptual transformation, 
from events qua occurants to events (as factical givens) qua states or 
possessions, and the grammatical norm %-- how we cannot just say 
`q.his lied` %-- is epiphenomenal to the conceptual logic here; 
the erroneous `q.his lied` sounds flawed because it does not 
match a coherent conceptual pattern in how events and states fit together.  
But, still, the syntactic requirement %-- the expectation 
that a noun or noun-phrase serve as the ground of a possessive 
adjective, or the direct object of a verb %-- manifests these 
underlying conceptual patterns in the order of everyday language.  
Syntactic patterns become entrenched `i.because` they are comfortable 
translations of conceptual schema, but `i.as` entrenched we hear 
these patterns as grammatically correct, not just as 
conceptually well-formed.  Likewise, we hear errata like 
`q.his lied` as `i.ungrammatical`/, not as conceptually incongruous.
`p`

`p.
I contend, therefore, that many conceptually-motivated word-pairing 
patterns become syntactically entrenched and thus engender a class 
of transform-pairs where the crucial, surface-level tranformation 
is syntactic, often in the form of translations between parts of speech, 
or between morphological classes (singular/plural, object/location, etc.).  
Consider locative constructions like 

`sentenceList,
`sentenceItem; \label{itm:grandma} Let's go to Grandma.
`sentenceItem; \label{itm:lawyers} Let's go to the lawyers.
`sentenceItem; \label{itm:press} Let's go to the press.
`sentenceList`

Here nouns like `i.Grandma`/, `i.the lawyers`/, and `the press` are 
used at sites in the surrounding sentence-forms that call for a designation 
of place %-- this compels us to read the nouns as describing a place, 
even while they are not intrinsically spatial or geographical 
(e.g., Grandma is associated with the place where she lives).  
`p`

`p.
In (\ref{itm:press}) and perhaps (\ref{itm:lawyers}), this locative 
figuring may be metaphorical: going `i.to the press` does not necessarily 
mean going to the newspaper's offices.  Indeed, each of these 
usages are to some degree conventionalized: going `i.to Grandma` 
is subtler than going `i.to Grandmas house`/, because the former 
construction implies that you are going to a `i.place` 
(`q.Grandma` is proxy for her house, say), but also Grandma is actually 
there, and that seeing her is the purpose of your visit.  In other words, 
the specific `i.go to Grandma` formation carries a supply of 
situational expectations.  There are analogous implications in 
(\ref{itm:lawyers}) and (\ref{itm:press}) %-- going `i.to the press` 
means trying to get some news story or information published.  
But the underlying manipulation of concepts, which structures the 
canonical situations implicated in (\ref{itm:grandma})-(\ref{itm:press}), 
is organized around the locative grammatical form as binding noun-concepts to a 
locative interpretation.  However metaphorical or imbued with additional 
situational implications, a person-to-location or institution-to-location 
mapping is the kernel conceptual operation around which the further 
expectations are organized.  Accordingly, the locative case qua 
grammatic phenomenon signals the operation of these situational 
conventions, and the syntactic norms in turn are manifest via 
word-pairings, such as `i.to Grandma`/.  
`p`

`p.
In short, a transform-pair like `i.to Grandma` can be analyzed in 
several registers; we can see it as the straightforward 
syntactic rendering of a locative construction (via inter-word morphology, 
insofar as English has no locative case-markers) or explore further 
situational implications.  In these examples, though, there is an 
obvious grammatic account of pairs' transformations, notwithstanding 
that there are also more semantic and conceptual accounts.  
Part-of-speech transforms (like `i.having lied`/) and 
case transforms (like `i.to Grandma`/) are mandated by 
syntactic norms and therefore can be absorbed into conventional 
grammatic models, such as Dependency Grammar: the head/dependent 
pairings in `i.having lied` and `i.to Grandma` are each 
covered by specific relations within the theory's inventory 
of possible inter-word connections.  So a subset of 
transform-pairs overlaps with (or can be associated with) 
corresponding Dependency Grammar pairings. 
`p`

`p.
Another potential embedding of transform-pairs into 
formal models can be motivated by Type Theory.  
This analysis can proceed on several levels, but 
in general terms we can assume that parts of speech 
form a functional type system (as elucidated, say, in 
Combinatory Categorial Grammar).  For instance, we can recognize 
nouns and propositions (sentences or sentence-parts forming 
logically complete clauses) as primitive types, and 
treat other parts of speech as akin to `q.functions` between 
other types.  For instance, a verb combines with a noun to 
form a proposition, or complete idea: `i.go` acts on 
`i.We` to yield the proposition `i.We go`/.  Schematically, 
then, verbs are akin to functions that map nouns to propositions.  
Similarly, adjectives map nouns to nouns, and 
adverbs map verbs to other verbs (here I use `q.noun` or `q.verb` to 
mean a linguistic unit which is functionally a noun, or verb; 
in this sense a noun-phrase is a kind of noun %-- i.e., 
a linguistic unit whose `i.type` is nominal).  
This provides a type-theoretic architecture through 
which transform-pairs can be analyzed.  For instance, 
an adverb modifies a verb; so an adverb in a transform 
pair must have a verb as a ground.  Moreover, the `q.product` 
of that transform is also a verb, in the sence that the 
abverb-verb pair, parsed as a phrase, can only be 
situated in grammatic contexts where a verb is expected. 
`p`

`p.
In effect, we can apply type-theoretic models to both 
parts of a transform-pair and to the pair as a whole, 
producing structural requirements on how words link up 
into transform-pairs.  We can then see an entire sentence 
as built up from a chain of such pairs, with the rules 
of this construction expressed type-theoretically.  
Given, say, `i.his having lied flagrantly` we can identify a 
chain of pairs `i.flagrantly`/-`i.lied`/, 
`i.having`/-`i.flagrantly`/, and `his`/-`i.having`/, where 
the `q.outcome` of one transform becomes subject to a 
subsequent transform.  So `i.flagrantly` modifies `i.lied` 
by expression measure and emphasis, adding conceptual detail; 
grammatically the outcome is still a verb.  Then `i.having`/, 
as I argued earlier, applies a tranform that maps this 
verb-outcome to a noun, which is then tranformed by 
the possessive `i.his`/.  Each step in the chain is 
governed by type-related requirements: the output of one 
transform must be type-compatible with the modifier for 
the next transform.  This induces a notion of `i.type-checking` 
transform-chains, which is analogous to how type-checking 
works in formal settings like computer programming languages, 
Typed Lambda Calculus, and Dimensional Analysis. 
`p`

`p.
Type-Theoretic Semantics holds out the hope that many 
syntactic and semantic rules can `q.fall out` as 
a direct consequence of type-checking requirements.  
A correct sentence is one built up from a chain of 
transforms where the outcome of one transform is type-compatible 
with the modifier of the next one, with the eventual or 
`q.root` transform, which completes the sentence, yielding a 
proposition.  This expresses, in linguistic terms like 
`q.outcome` and `q.modifier`/, the same type-checking 
requirements as in Lambda Calculus, which would use 
more mathematical vocabulary: the output of one function must 
be type-compatible with the input parameter of the next 
function, insofar as this output is to be substituted for a 
lambda-abstracted symbol in the outer function's formula.  
`p` 

`p.
This gloss actually understates the explanatory power of 
type-theoretic models for linguistics, since I have 
mentioned only very coarse-grained type classifications 
(noun, proposition, verb, adjective, adverb); more 
complex type-theoretic constructions come into play 
when this framework is refined to consider plural/singular, 
classes of nouns, and so forth, establishing a basis for 
more sophisticated structures adapted to language from 
formal type theory, like type-coersions and 
dependent types (I will revisit these theories 
in a later section).  Here, though, I will just point out 
that Dependecy Grammar and Type-Theoretic Semantics can 
often overlap in their analysis of word-pairs (inter-word 
relations is not centralized in type-oriented methodology as much 
as in Dependecy Grammar, but type concepts can certainly 
be marshaled toward word-pair analysis).     
`p`

`p.
Even though Dependency and Type-Theoretic analyses will often reinforce 
one another, they can offer distinct perspectives on 
how pairs aggregate to form complete phrases and sentences.  
In the transform-pair `i.having lied`/, `i.lied` is clearly the 
more significant word semantically.  This is reflected in 
`i.having` being annotated (at least according to the Universal Dependency 
framework) as auxiliary, and the dependent element of the pair, while 
`i.lied` is the head.  Then `i.lied` is also connected to 
`i.his`/, establishing a verb-subject relation.  So `i.lied` becomes 
the nexus around which other, supporting sentence elements are 
connected.  This is a typical pattern in Dependecy Grammar parses, where 
the most semantically significant sentence elements also tend to be 
the most densely connected (if we treat the parse-diagram as a 
graph, these nodes tend to have the highest `q.degree`/, a measure of 
nodes' importance to the degree as this is reflected in how many 
other nodes connect to it).  Indeed, by counting word connections we 
can get a rough estimation of semantic importance, distinguishing 
`q.central` and `q.peripheral` elements.  These are not 
standard terms, but they suggest a norm in Dependecy Grammar that the 
structure of parse-graphs generally reflects semantic priority: 
the central `q.spine` of a graph, so to speak, captures the 
primary signifying intentions of the original sentence, while the 
more peripheral areas capture finer details or syntactic auxiliaries 
whose role is for grammatical propriety more than meaningful content.    
`p`

`p.
Conversely, a type-theoretic analysis might incline us to quetion this
sense of semantic core versus periphery: in the case of 
`i.his having lied`/, the transform `i.having` supplies the outcome 
which is content for the possessive `i.has`/.  If we see the sentence 
as a cognitive unfolding, a series of mental adjustments toward an 
ever-more-precise reading of speaker intent, then each step in the 
tranformation contributes consequential details to the final 
understanding.  Moreover, `i.lied` is only present in the transformation 
signified by `i.his` insofar as it has in turn been transformed by 
`i.having`/: each the modifier in a transform-pair has a degree of 
temporal priority because `i.its` effects are directly preesent 
in the context of the following transformation.  This motivates a 
flavor of Dependency Grammar where the head/dependent ordering is 
inverted: a seemingly auxiliary component (like the function-word 
to a content-word) can be notated as the head because its 
output serves as `q.input` to a subsequent transform.  In the analogy 
to Lambda Calculus, `i.his having lied` would be graphed with 
`i.having` being the head for `i.lied`/, and `i.his` the head for 
`i.having`/, reflecting the relation of functions to their arguments.  
In lisp-like code, this could be written functionally as 
(his (having lied)), showing `i.having` as one function, and 
`i.his` as a second one, the former's output being the latter's input.     
`p`

`p.
Implicitly, then, Type-Theoretic Semantics and Dependency Grammar 
can connote different perspectives on semantic inportance and 
the unfolding of linguistic understanding.  I will explore 
this distinction further below, with explicit juxtaposition of 
parse graphs using the two methods.  I contend, however, that the 
distinction reflects a manifest duality in linguistic meaning: 
we can treat a linguistic artifact as an unfolding process or 
as a static signification with more central and more minute 
parts.  Both of these aspects coexist: on the one hand, we 
understading sentences via an unfolding cognitive process; 
on the other hand, this cognition includes forming a mental 
review of the essential points of the sentence, a collation of 
key ideas such as (for \ref{itm:having})
`i.his`/, `i.lied`/, `i.damages`/, and `i.credibiliy`/.  
Given this two-toned cognitive status %-- part dynamic process, 
part static outline %-- it is perhaps understandable that 
different methodologies for deconstructing a sentence into 
word-pair aggregates would converge on different structural 
norms for how the pairs are interrelated, internally and to one another.
`p`

`p.
This analysis, which I will extend later, has considered transform-pairs 
from a syntactic angle %-- in the sense that I have highlighted pairs 
which obviously come to the fore via grammatic principles.  As I indicated, 
I believe the notion of transform-pairs cuts across both syntax 
and semantics, so I will pivot to some analyses which attend more 
to the semantic dimension.
`p`

`subsection.Semantic Transform-Pairs`
`p.

`p`

