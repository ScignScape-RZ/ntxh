
`section.Cognitive Transform Grammar and Transform-Pairs`

`p.
The idea that inter-word pairs are a foundational linguistic 
unit %-- from which larger aggregates can be built up recursively 
%-- is an central tenet of Dependency Grammar.  Here I will 
generalize this perspective outside (but not excluding) 
grammar, to overall semantic, pragmatic, and even 
extralinguistic relations indicated via interword relations.    
`p`

`p.
In some cases word-relations can still be theorized mostly via 
syntax.  Consider hypothetical, example sentences like 

`sentenceList,
`sentenceItem; \label{itm:having} His having lied in the past damages his credibilty in the present.
`sentenceItem; \label{itm:whether} Voters question whether he is truthful this time around. 
`sentenceList`

In (\ref{itm:having}), `i.having` is necessary to syntactically transform its ground 
`i.lied` from a verb-form to a noun (something which can be inserted into a 
possessive clause).  Analogously, in (\label{ref:whether}) `i.whether` modifies 
`i.is` (since this is the head of a subordinate clause), wrapping a propositional 
clause into a noun so that it furnishes a direct object to the verb 
`i.question`/.  The essential transformation in these cases is 
motivated by grammatic considerations, particularly part-of-speech: 
a verb and a subordinate, propositionally complete clause (in \label{ref:having} and 
\label{ref:whether}, respectively) need (for syntactic propriety) to be 
modified so as to play a role in a site where a noun is expected 
(in effect, they need to be bundled into a noun-phrase).
`p`

`p.
The relevant transforms here %-- signified by `i.having` and `i.whether` %-- have a 
semantic dimension also, and we can speculate that the syntactic 
rules (requiring a verb or propositional-clause to be transformed into a 
noun) are actually driven by semantic considerations.  Conceptually, 
for example, `i.his having lied` packages a verb into a possessive 
context because the sentence is not foregrounding a specific lying-event 
but rather the fact of the existence of such occasions.  We cannot perhaps 
`q.possess` an event, but we possess (as part of our nature or history) 
the fact of past occurances, viz., events in the form of things we 
have done.  In this sense `i.his having lied` marks a conceptual transformation, 
from events qua occurants to events (as factical givens) qua states or 
possessions, and the grammatical norm %-- how we cannot just say 
`q.his lied` %-- is epiphenomenal to the conceptual logic here; 
the erroneous `q.his lied` sounds flawed because it does not 
match a coherent conceptual pattern in how events and states fit together.  
But, still, the syntactic requirement %-- the expectation 
that a noun or noun-phrase serve as the ground of a possessive 
adjective, or the direct object of a verb %-- manifests these 
underlying conceptual patterns in the order of everyday language.  
Syntactic patterns become entrenched `i.because` they are comfortable 
translations of conceptual schema, but `i.as` entrenched we hear 
these patterns as grammatically correct, not just as 
conceptually well-formed.  Likewise, we hear errata like 
`q.his lied` as `i.ungrammatical`/, not as conceptually incongruous.
`p`

`p.
I contend, therefore, that many conceptually-motivated word-pairing 
patterns become syntactically entrenched and thus engender a class 
of transform-pairs where the crucial, surface-level tranformation 
is syntactic, often in the form of translations between parts of speech, 
or between morphological classes (singular/plural, object/location, etc.).  
Consider locative constructions like 

`sentenceList,
`sentenceItem; \label{itm:grandma} Let's go to Grandma.
`sentenceItem; \label{itm:lawyers} Let's go to the lawyers.
`sentenceItem; \label{itm:press} Let's go to the press.
`sentenceList`

Here nouns like `i.Grandma`/, `i.the lawyers`/, and `the press` are 
used at sites in the surrounding sentence-forms that call for a designation 
of place %-- this compels us to read the nouns as describing a place, 
even while they are not intrinsically spatial or geographical 
(e.g., Grandma is associated with the place where she lives).  
`p`

`p.
In (\ref{itm:press}) and perhaps (\ref{itm:lawyers}), this locative 
figuring may be metaphorical: going `i.to the press` does not necessarily 
mean going to the newspaper's offices.  Indeed, each of these 
usages are to some degree conventionalized: going `i.to Grandma` 
is subtler than going `i.to Grandmas house`/, because the former 
construction implies that you are going to a `i.place` 
(`q.Grandma` is proxy for her house, say), but also Grandma is actually 
there, and that seeing her is the purpose of your visit.  In other words, 
the specific `i.go to Grandma` formation carries a supply of 
situational expectations.  There are analogous implications in 
(\ref{itm:lawyers}) and (\ref{itm:press}) %-- going `i.to the press` 
means trying to get some news story or information published.  
But the underlying manipulation of concepts, which structures the 
canonical situations implicated in (\ref{itm:grandma})-(\ref{itm:press}), 
is organized around the locative grammatical form as binding noun-concepts to a 
locative interpretation.  However metaphorical or imbued with additional 
situational implications, a person-to-location or institution-to-location 
mapping is the kernel conceptual operation around which the further 
expectations are organized.  Accordingly, the locative case qua 
grammatic phenomenon signals the operation of these situational 
conventions, and the syntactic norms in turn are manifest via 
word-pairings, such as `i.to Grandma`/.  
`p`

`p.
In short, a transform-pair like `i.to Grandma` can be analyzed in 
several registers; we can see it as the straightforward 
syntactic rendering of a locative construction (via inter-word morphology, 
insofar as English has no locative case-markers) or explore further 
situational implications.  In these examples, though, there is an 
obvious grammatic account of pairs' transformations, notwithstanding 
that there are also more semantic and conceptual accounts.  
Part-of-speech transforms (like `i.having lied`/) and 
case transforms (like `i.to Grandma`/) are mandated by 
syntactic norms and therefore can be absorbed into conventional 
grammatic models, such as Dependency Grammar: the head/dependent 
pairings in `i.having lied` and `i.to Grandma` are each 
covered by specific relations within the theory's inventory 
of possible inter-word connections.  So a subset of 
transform-pairs overlaps with (or can be associated with) 
corresponding Dependency Grammar pairings. 
`p`

`p.
Another potential embedding of transform-pairs into 
formal models can be motivated by Type Theory.  
This analysis can proceed on several levels, but 
in general terms we can assume that parts of speech 
form a functional type system (as elucidated, say, in 
Combinatory Categorial Grammar).  For instance, we can recognize 
nouns and propositions (sentences or sentence-parts forming 
logically complete clauses) as primitive types, and 
treat other parts of speech as akin to `q.functions` between 
other types.  For instance, a verb combines with a noun to 
form a proposition, or complete idea: `i.go` acts on 
`i.We` to yield the proposition `i.We go`/.  Schematically, 
then, verbs are akin to functions that map nouns to propositions.  
Similarly, adjectives map nouns to nouns, and 
adverbs map verbs to other verbs (here I use `q.noun` or `q.verb` to 
mean a linguistic unit which is functionally a noun, or verb; 
in this sense a noun-phrase is a kind of noun %-- i.e., 
a linguistic unit whose `i.type` is nominal).  
This provides a type-theoretic architecture through 
which transform-pairs can be analyzed.  For instance, 
an adverb modifies a verb; so an adverb in a transform 
pair must have a verb as a ground.  Moreover, the `q.product` 
of that transform is also a verb, in the sence that the 
abverb-verb pair, parsed as a phrase, can only be 
situated in grammatic contexts where a verb is expected. 
`p`

`p.
In effect, we can apply type-theoretic models to both 
parts of a transform-pair and to the pair as a whole, 
producing structural requirements on how words link up 
into transform-pairs.  We can then see an entire sentence 
as built up from a chain of such pairs, with the rules 
of this construction expressed type-theoretically.  
Given, say, `i.his having lied flagrantly` we can identify a 
chain of pairs `i.flagrantly`/-`i.lied`/, 
`i.having`/-`i.flagrantly`/, and `his`/-`i.having`/, where 
the `q.outcome` of one transform becomes subject to a 
subsequent transform.  So `i.flagrantly` modifies `i.lied` 
by expression measure and emphasis, adding conceptual detail; 
grammatically the outcome is still a verb.  Then `i.having`/, 
as I argued earlier, applies a tranform that maps this 
verb-outcome to a noun, which is then tranformed by 
the possessive `i.his`/.  Each step in the chain is 
governed by type-related requirements: the output of one 
transform must be type-compatible with the modifier for 
the next transform.  This induces a notion of `i.type-checking` 
transform-chains, which is analogous to how type-checking 
works in formal settings like computer programming languages, 
Typed Lambda Calculus, and Dimensional Analysis. 
`p`

`p.
Type-Theoretic Semantics holds out the hope that many 
syntactic and semantic rules can `q.fall out` as 
a direct consequence of type-checking requirements.  
A correct sentence is one built up from a chain of 
transforms where the outcome of one transform is type-compatible 
with the modifier of the next one, with the eventual or 
`q.root` transform, which completes the sentence, yielding a 
proposition.  This expresses, in linguistic terms like 
`q.outcome` and `q.modifier`/, the same type-checking 
requirements as in Lambda Calculus, which would use 
more mathematical vocabulary: the output of one function must 
be type-compatible with the input parameter of the next 
function, insofar as this output is to be substituted for a 
lambda-abstracted symbol in the outer function's formula.  
`p` 

`p.
This gloss actually understates the explanatory power of 
type-theoretic models for linguistics, since I have 
mentioned only very coarse-grained type classifications 
(noun, proposition, verb, adjective, adverb); more 
complex type-theoretic constructions come into play 
when this framework is refined to consider plural/singular, 
classes of nouns, and so forth, establishing a basis for 
more sophisticated structures adapted to language from 
formal type theory, like type-coersions and 
dependent types (I will revisit these theories 
in a later section).  Here, though, I will just point out 
that Dependecy Grammar and Type-Theoretic Semantics can 
often overlap in their analysis of word-pairs (inter-word 
relations is not centralized in type-oriented methodology as much 
as in Dependecy Grammar, but type concepts can certainly 
be marshaled toward word-pair analysis).     
`p`

`p.
Even though Dependency and Type-Theoretic analyses will often reinforce 
one another, they can offer distinct perspectives on 
how pairs aggregate to form complete phrases and sentences.  
In the transform-pair `i.having lied`/, `i.lied` is clearly the 
more significant word semantically.  This is reflected in 
`i.having` being annotated (at least according to the Universal Dependency 
framework) as auxiliary, and the dependent element of the pair, while 
`i.lied` is the head.  Then `i.lied` is also connected to 
`i.his`/, establishing a verb-subject relation.  So `i.lied` becomes 
the nexus around which other, supporting sentence elements are 
connected.  This is a typical pattern in Dependecy Grammar parses, where 
the most semantically significant sentence elements also tend to be 
the most densely connected (if we treat the parse-diagram as a 
graph, these nodes tend to have the highest `q.degree`/, a measure of 
nodes' importance to the degree as this is reflected in how many 
other nodes connect to it).  Indeed, by counting word connections we 
can get a rough estimation of semantic importance, distinguishing 
`q.central` and `q.peripheral` elements.  These are not 
standard terms, but they suggest a norm in Dependecy Grammar that the 
structure of parse-graphs generally reflects semantic priority: 
the central `q.spine` of a graph, so to speak, captures the 
primary signifying intentions of the original sentence, while the 
more peripheral areas capture finer details or syntactic auxiliaries 
whose role is for grammatical propriety more than meaningful content.    
`p`

`p.
Conversely, a type-theoretic analysis might incline us to quetion this
sense of semantic core versus periphery: in the case of 
`i.his having lied`/, the transform `i.having` supplies the outcome 
which is content for the possessive `i.has`/.  If we see the sentence 
as a cognitive unfolding, a series of mental adjustments toward an 
ever-more-precise reading of speaker intent, then each step in the 
tranformation contributes consequential details to the final 
understanding.  Moreover, `i.lied` is only present in the transformation 
signified by `i.his` insofar as it has in turn been transformed by 
`i.having`/: each the modifier in a transform-pair has a degree of 
temporal priority because `i.its` effects are directly preesent 
in the context of the following transformation.  This motivates a 
flavor of Dependency Grammar where the head/dependent ordering is 
inverted: a seemingly auxiliary component (like the function-word 
to a content-word) can be notated as the head because its 
output serves as `q.input` to a subsequent transform.  In the analogy 
to Lambda Calculus, `i.his having lied` would be graphed with 
`i.having` being the head for `i.lied`/, and `i.his` the head for 
`i.having`/, reflecting the relation of functions to their arguments.  
In lisp-like code, this could be written functionally as 
(his (having lied)), showing `i.having` as one function, and 
`i.his` as a second one, the former's output being the latter's input.     
`p`

`p.
Implicitly, then, Type-Theoretic Semantics and Dependency Grammar 
can connote different perspectives on semantic inportance and 
the unfolding of linguistic understanding.  I will explore 
this distinction further below, with explicit juxtaposition of 
parse graphs using the two methods.  I contend, however, that the 
distinction reflects a manifest duality in linguistic meaning: 
we can treat a linguistic artifact as an unfolding process or 
as a static signification with more central and more minute 
parts.  Both of these aspects coexist: on the one hand, we 
understading sentences via an unfolding cognitive process; 
on the other hand, this cognition includes forming a mental 
review of the essential points of the sentence, a collation of 
key ideas such as (for \ref{itm:having})
`i.his`/, `i.lied`/, `i.damages`/, and `i.credibiliy`/.  
Given this two-toned cognitive status %-- part dynamic process, 
part static outline %-- it is perhaps understandable that 
different methodologies for deconstructing a sentence into 
word-pair aggregates would converge on different structural 
norms for how the pairs are interrelated, internally and to one another.
`p`

`p.
This analysis, which I will extend later, has considered transform-pairs 
from a syntactic angle %-- in the sense that I have highlighted pairs 
which obviously come to the fore via grammatic principles.  As I indicated, 
I believe the notion of transform-pairs cuts across both syntax 
and semantics, so I will pivot to some analyses which attend more 
to the semantic dimension.
`p`

`subsection.Semantic Analyses of Transform-Pairs`
`p.
In the simplest cases, a transform-pair represents a modifier 
adding conceptual detail to a ground, like `i.black dogs` 
from `i.dogs`/.  But the nature of this added detail 
%-- and its evident relation to surface language %-- can be very varied.  
Compare between examples like:

`sentenceList,
`sentenceItem; \label{itm:black} I saw my neighbor's two black dogs.
`sentenceItem; \label{itm:rescued} I saw my neighbor's two rescued dogs.
`sentenceItem; \label{itm:latest} I saw my neighbor's two latest dogs.
`sentenceList`

Whereas (\ref{itm:black}) presents a fairly straightforward conceptual 
tranformation, the detailing in (\ref{itm:rescued}) is a lot subtler; 
mentioning `i.rescued` dogs makes no reference to perceptual qualities, 
but rather implies intricate situational background.  The term 
`i.rescued dogs` strongly suggests that the dogs were adopted by their 
current owner, probably after an animal-welfare organization 
found them abandoned, or removed them from a prior abusive owner.  
This kind of backstory is packaged up, as a kind of 
situational prototype, in the conventionalized phrase 
`i.rescued dogs`/, implying a level of specificity more 
precise than the ajective `i.rescued` alone implies.  
Correspondingly, the verb `i.to rescue` when applied to 
dogs suggest more information than in more 
generic contexts.
`p`

`p.
The phrase `i.latest dogs` carries implications in its own 
right; we assume the neighbor had owned other dogs before.  
Of course `q.latest` implies some temporal order, but the 
understood time-scale depends on context.  
If we hear talk about a `i.vet`/s two latest dogs, we would presumably 
interpret this in terms of patients the vet has seen over the course of 
a day: 

`sentenceList,
`sentenceItem; \label{itm:vet} We're going after the vet's two latest dogs.
`sentenceItem; \label{itm:organization} I'm concerned about the rescue organization's 
two latest dogs.
`sentenceList`
 
Understanding the relevant time-frame depends on understading the relation 
between the dogs and the possessive antcedent.  In (\ref{itm:latest}) 
the neighbor (in a typical case) actually owns the dogs, so the 
situational context grounding the modifier `i.latest` would be understood 
against the normal time-scale for dog ownership (at least several years).  
In (\ref{itm:vet}), the vet only `q.possesses` the dogs in the sense 
of endeavoring to examine them, a process of minutes or hours.  
In (\ref{itm:organization}), the implication of the `i.organization's` 
possessive `visavis; rescued dogs is that the group endeavors 
to rehabilate and find permanent homes for the rescuees.  So in each 
case `i.latest` implies a succession of dogs, leading over time to 
two most recent ones, but the implied time-frame for our conceptualizing 
this sequence can be minutes-to-hours, or days-to-months, or years. 
`p`

`p.
We should also observe that the implied time-frames and backstories in 
(\ref{itm:rescued}-\ref{itm:organization}) are not directly signified via 
morphosyntax or lexical resources alone.  The word `i.rescued` only 
carries the `i.rescued dog` backstory when used in a context 
involving the dogs' eventual owners; in some context the more 
generic meaning of `i.rescue` could supercede:  

`sentenceList,
`sentenceItem; \label{itm:boatmen} Boatmen rescued dogs from the flooded streets.
`sentenceItem; \label{itm:firemen} Firemen rescued dogs from the burning building.
`sentenceList`

Neither (\ref{itm:boatmen}) nor (\ref{itm:firemen}) imply that the dogs were abandoned, or 
will have new owners, or be sent to a shelter, or that their rescuers are 
members of an animal-welfare organization %-- in short, no element of 
the conventionalized backstory usually invoked by `i.rescued dogs` is present.  
Analogously, there is no lexical subdivision for `i.latest` which regulates 
the variance in time-frames among (\ref{itm:latest}-\ref{itm:organization}).  
It is only by inferring a likely situational background that 
conversants will make time-scale assumptions based on one situation 
involving dog ownership, another involving veterinary exams, and a 
third involving animal-welfare rehabilitation.
`p`

`p.
That is to say, the time-scale inference I have analyzed is essentially 
`i.extralinguistic`/: there is no specific `i.linguistic` knowledge 
(lexical or grammatical, or even pragmatic inferences in the sense of 
deictic or anaphora resolution) which warrants the situational classification 
of (\ref{itm:latest}-\ref{itm:organization}) into different time scales.  
Instead, the inference is driven by (to some degree socially or culturally 
specific) background-knowlege about phenomena like veterinary clinics 
or animal rescue groups.  Whether or not the nuances in `i.rescued dogs` 
are similarly extra-linguistic is an interesting question %-- we can 
argue that the phrase is now entrenched as a `i.de facto` lexical 
entrant in its own right, so the role of `i.rescued` is not only to 
lend adjectival detail but to construct a recurring phrase with a 
distinct meaning, like `i.red card` (in football) or `i.stolen base` 
(in baseball).  Lexical entrenchment is, I would argue, an 
intra-linguistic phenomenon, in the sense that understanding entrenched 
phrases is akin to familiarity with specific word-senses, which is a 
properly linguistic kind of knowledge.  But even in that case, entrenchment 
is only possible because the phrase has a signifying precision more 
rigorous than its purely linguistic composition would imply.  
There are, in short, extralinguistic considerations governing `i.when` 
phrases are candidates for entrenchment, and a language-user's 
ability to learn the conventionalized meaning (which I believe 
is an intra-linguistic cognitive development) depends on their 
having the relevant (extra-linguistic) background knowledge. 
`p`

`p.

`p`

`p.

`p`

