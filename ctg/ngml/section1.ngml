
`section.Cognitive Transform Grammar and Transform-Pairs`

`p.
The idea that inter-word pairs are a foundational linguistic 
unit %-- from which larger aggregates can be built up recursively 
%-- is an central tenet of Dependency Grammar.  Here I will 
generalize this perspective outside (but not excluding) 
grammar, to overall semantic, pragmatic, and even 
extralinguistic relations indicated via interword relations.    
`p`

`p.
In some cases word-relations can still be theorized mostly via 
syntax.  Consider hypothetical, example sentences like 

`sentenceList,
`sentenceItem; \label{itm:having} His having lied in the past damages his credibilty in the present.
`sentenceItem; \label{itm:whether} Voters question whether he is truthful this time around. 
`sentenceList`

In (\ref{itm:having}), `i.having` is necessary to syntactically transform its ground 
`i.lied` from a verb-form to a noun (something which can be inserted into a 
possessive clause).  Analogously, in (\label{ref:whether}) `i.whether` modifies 
`i.is` (since this is the head of a subordinate clause), wrapping a propositional 
clause into a noun so that it furnishes a direct object to the verb 
`i.question`/.  The essential transformation in these cases is 
motivated by grammatic considerations, particularly part-of-speech: 
a verb and a subordinate, propositionally complete clause (in \label{ref:having} and 
\label{ref:whether}, respectively) need (for syntactic propriety) to be 
modified so as to play a role in a site where a noun is expected 
(in effect, they need to be bundled into a noun-phrase).
`p`

`p.
The relevant transforms here %-- signified by `i.having` and `i.whether` %-- have a 
semantic dimension also, and we can speculate that the syntactic 
rules (requiring a verb or propositional-clause to be transformed into a 
noun) are actually driven by semantic considerations.  Conceptually, 
for example, `i.his having lied` packages a verb into a possessive 
context because the sentence is not foregrounding a specific lying-event 
but rather the fact of the existence of such occasions.  We cannot perhaps 
`q.possess` an event, but we possess (as part of our nature or history) 
the fact of past occurances, viz., events in the form of things we 
have done.  In this sense `i.his having lied` marks a conceptual transformation, 
from events qua occurants to events (as factical givens) qua states or 
possessions, and the grammatical norm %-- how we cannot just say 
`q.his lied` %-- is epiphenomenal to the conceptual logic here; 
the erroneous `q.his lied` sounds flawed because it does not 
match a coherent conceptual pattern in how events and states fit together.  
But, still, the syntactic requirement %-- the expectation 
that a noun or noun-phrase serve as the ground of a possessive 
adjective, or the direct object of a verb %-- manifests these 
underlying conceptual patterns in the order of everyday language.  
Syntactic patterns become entrenched `i.because` they are comfortable 
translations of conceptual schema, but `i.as` entrenched we hear 
these patterns as grammatically correct, not just as 
conceptually well-formed.  Likewise, we hear errata like 
`q.his lied` as `i.ungrammatical`/, not as conceptually incongruous.
`p`

`p.
I contend, therefore, that many conceptually-motivated word-pairing 
patterns become syntactically entrenched and thus engender a class 
of transform-pairs where the crucial, surface-level tranformation 
is syntactic, often in the form of translations between parts of speech, 
or between morphological classes (singular/plural, object/location, etc.).  
Consider locative constructions like 

`sentenceList,
`sentenceItem; \label{itm:grandma} Let's go to Grandma.
`sentenceItem; \label{itm:lawyers} Let's go to the lawyers.
`sentenceItem; \label{itm:press} Let's go to the press.
`sentenceList`

Here nouns like `i.Grandma`/, `i.the lawyers`/, and `the press` are 
used at sites in the surrounding sentence-forms that call for a designation 
of place %-- this compels us to read the nouns as describing a place, 
even while they are not intrinsically spatial or geographical 
(e.g., Grandma is associated with the place where she lives).  
`p`

`p.
In (\ref{itm:press}) and perhaps (\ref{itm:lawyers}), this locative 
figuring may be metaphorical: going `i.to the press` does not necessarily 
mean going to the newspaper's offices.  Indeed, each of these 
usages are to some degree conventionalized: going `i.to Grandma` 
is subtler than going `i.to Grandmas house`/, because the former 
construction implies that you are going to a `i.place` 
(`q.Grandma` is proxy for her house, say), but also Grandma is actually 
there, and that seeing her is the purpose of your visit.  In other words, 
the specific `i.go to Grandma` formation carries a supply of 
situational expectations.  There are analogous implications in 
(\ref{itm:lawyers}) and (\ref{itm:press}) %-- going `i.to the press` 
means trying to get some news story or information published.  
But the underlying manipulation of concepts, which structures the 
canonical situations implicated in (\ref{itm:grandma})-(\ref{itm:press}), 
is organized around the locative grammatical form as binding noun-concepts to a 
locative interpretation.  However metaphorical or imbued with additional 
situational implications, a person-to-location or institution-to-location 
mapping is the kernel conceptual operation around which the further 
expectations are organized.  Accordingly, the locative case qua 
grammatic phenomenon signals the operation of these situational 
conventions, and the syntactic norms in turn are manifest via 
word-pairings, such as `i.to Grandma`/.  
`p`

`p.
In short, a transform-pair like `i.to Grandma` can be analyzed in 
several registers; we can see it as the straightforward 
syntactic rendering of a locative construction (via inter-word morphology, 
insofar as English has no locative case-markers) or explore further 
situational implications.  In these examples, though, there is an 
obvious grammatic account of pairs' transformations, notwithstanding 
that there are also more semantic and conceptual accounts.  
Part-of-speech transforms (like `i.having lied`/) and 
case transforms (like `i.to Grandma`/) are mandated by 
syntactic norms and therefore can be absorbed into conventional 
grammatic models, such as Dependency Grammar: the head/dependent 
pairings in `i.having lied` and `i.to Grandma` are each 
covered by specific relations within the theory's inventory 
of possible inter-word connections.  So a subset of 
transform-pairs overlaps with (or can be associated with) 
corresponding Dependency Grammar pairings. 
`p`

`p.
Another potential embedding of transform-pairs into 
formal models can be motivated by Type Theory.  
This analysis can proceed on several levels, but 
in general terms we can assume that parts of speech 
form a functional type system (as elucidated, say, in 
Combinatory Categorial Grammar).  For instance, we can recognize 
nouns and propositions (sentences or sentence-parts forming 
logically complete clauses) as primitive types, and 
treat other parts of speech as akin to `q.functions` between 
other types.  For instance, a verb combines with a noun to 
form a proposition, or complete idea: `i.go` acts on 
`i.We` to yield the proposition `i.We go`/.  Schematically, 
then, verbs are akin to functions that map nouns to propositions.  
Similarly, adjectives map nouns to nouns, and 
adverbs map verbs to other verbs (here I use `q.noun` or `q.verb` to 
mean a linguistic unit which is functionally a noun, or verb; 
in this sense a noun-phrase is a kind of noun %-- i.e., 
a linguistic unit whose `i.type` is nominal).  
This provides a type-theoretic architecture through 
which transform-pairs can be analyzed.  For instance, 
an adverb modifies a verb; so an adverb in a transform 
pair must have a verb as a ground.  Moreover, the `q.product` 
of that transform is also a verb, in the sence that the 
abverb-verb pair, parsed as a phrase, can only be 
situated in grammatic contexts where a verb is expected. 
`p`

`p.
In effect, we can apply type-theoretic models to both 
parts of a transform-pair and to the pair as a whole, 
producing structural requirements on how words link up 
into transform-pairs.  We can then see an entire sentence 
as built up from a chain of such pairs, with the rules 
of this construction expressed type-theoretically.  
Given, say, `i.his having lied flagrantly` we can identify a 
chain of pairs `i.flagrantly`/-`i.lied`/, 
`i.having`/-`i.flagrantly`/, and `his`/-`i.having`/, where 
the `q.outcome` of one transform becomes subject to a 
subsequent transform.  So `i.flagrantly` modifies `i.lied` 
by expression measure and emphasis, adding conceptual detail; 
grammatically the outcome is still a verb.  Then `i.having`/, 
as I argued earlier, applies a tranform that maps this 
verb-outcome to a noun, which is then tranformed by 
the possessive `i.his`/.  Each step in the chain is 
governed by type-related requirements: the output of one 
transform must be type-compatible with the modifier for 
the next transform.  This induces a notion of `i.type-checking` 
transform-chains, which is analogous to how type-checking 
works in formal settings like computer programming languages, 
Typed Lambda Calculus, and Dimensional Analysis. 
`p`

`p.
Type-Theoretic Semantics holds out the hope that many 
syntactic and semantic rules can `q.fall out` as 
a direct consequence of type-checking requirements.  
A correct sentence is one built up from a chain of 
transforms where the outcome of one transform is type-compatible 
with the modifier of the next one, with the eventual or 
`q.root` transform, which completes the sentence, yielding a 
proposition.  This expresses, in linguistic terms like 
`q.outcome` and `q.modifier`/, the same type-checking 
requirements as in Lambda Calculus, which would use 
more mathematical vocabulary: the output of one function must 
be type-compatible with the input parameter of the next 
function, insofar as this output is to be substituted for a 
lambda-abstracted symbol in the outer function's formula.  
`p` 

`p.
This gloss actually understates the explanatory power of 
type-theoretic models for linguistics, since I have 
mentioned only very coarse-grained type classifications 
(noun, proposition, verb, adjective, adverb); more 
complex type-theoretic constructions come into play 
when this framework is refined to consider plural/singular, 
classes of nouns, and so forth, establishing a basis for 
more sophisticated structures adapted to language from 
formal type theory, like type-coersions and 
dependent types (I will revisit these theories 
in a later section).  Here, though, I will just point out 
that Dependecy Grammar and Type-Theoretic Semantics can 
often overlap in their analysis of word-pairs (inter-word 
relations is not centralized in type-oriented methodology as much 
as in Dependecy Grammar, but type concepts can certainly 
be marshaled toward word-pair analysis).     
`p`

`p.
Even though Dependency and Type-Theoretic analyses will often reinforce 
one another, they can offer distinct perspectives on 
how pairs aggregate to form complete phrases and sentences.  
In the transform-pair `i.having lied`/, `i.lied` is clearly the 
more significant word semantically.  This is reflected in 
`i.having` being annotated (at least according to the Universal Dependency 
framework) as auxiliary, and the dependent element of the pair, while 
`i.lied` is the head.  Then `i.lied` is also connected to 
`i.his`/, establishing a verb-subject relation.  So `i.lied` becomes 
the nexus around which other, supporting sentence elements are 
connected.  This is a typical pattern in Dependecy Grammar parses, where 
the most semantically significant sentence elements also tend to be 
the most densely connected (if we treat the parse-diagram as a 
graph, these nodes tend to have the highest `q.degree`/, a measure of 
nodes' importance to the degree as this is reflected in how many 
other nodes connect to it).  Indeed, by counting word connections we 
can get a rough estimation of semantic importance, distinguishing 
`q.central` and `q.peripheral` elements.  These are not 
standard terms, but they suggest a norm in Dependecy Grammar that the 
structure of parse-graphs generally reflects semantic priority: 
the central `q.spine` of a graph, so to speak, captures the 
primary signifying intentions of the original sentence, while the 
more peripheral areas capture finer details or syntactic auxiliaries 
whose role is for grammatical propriety more than meaningful content.    
`p`

`p.
Conversely, a type-theoretic analysis might incline us to quetion this
sense of semantic core versus periphery: in the case of 
`i.his having lied`/, the transform `i.having` supplies the outcome 
which is content for the possessive `i.has`/.  If we see the sentence 
as a cognitive unfolding, a series of mental adjustments toward an 
ever-more-precise reading of speaker intent, then each step in the 
tranformation contributes consequential details to the final 
understanding.  Moreover, `i.lied` is only present in the transformation 
signified by `i.his` insofar as it has in turn been transformed by 
`i.having`/: each the modifier in a transform-pair has a degree of 
temporal priority because `i.its` effects are directly preesent 
in the context of the following transformation.  This motivates a 
flavor of Dependency Grammar where the head/dependent ordering is 
inverted: a seemingly auxiliary component (like the function-word 
to a content-word) can be notated as the head because its 
output serves as `q.input` to a subsequent transform.  In the analogy 
to Lambda Calculus, `i.his having lied` would be graphed with 
`i.having` being the head for `i.lied`/, and `i.his` the head for 
`i.having`/, reflecting the relation of functions to their arguments.  
In lisp-like code, this could be written functionally as 
(his (having lied)), showing `i.having` as one function, and 
`i.his` as a second one, the former's output being the latter's input.     
`p`

`p.
Implicitly, then, Type-Theoretic Semantics and Dependency Grammar 
can connote different perspectives on semantic inportance and 
the unfolding of linguistic understanding.  I will explore 
this distinction further below, with explicit juxtaposition of 
parse graphs using the two methods.  I contend, however, that the 
distinction reflects a manifest duality in linguistic meaning: 
we can treat a linguistic artifact as an unfolding process or 
as a static signification with more central and more minute 
parts.  Both of these aspects coexist: on the one hand, we 
understading sentences via an unfolding cognitive process; 
on the other hand, this cognition includes forming a mental 
review of the essential points of the sentence, a collation of 
key ideas such as (for \ref{itm:having})
`i.his`/, `i.lied`/, `i.damages`/, and `i.credibiliy`/.  
Given this two-toned cognitive status %-- part dynamic process, 
part static outline %-- it is perhaps understandable that 
different methodologies for deconstructing a sentence into 
word-pair aggregates would converge on different structural 
norms for how the pairs are interrelated, internally and to one another.
`p`

`p.
This analysis, which I will extend later, has considered transform-pairs 
from a syntactic angle %-- in the sense that I have highlighted pairs 
which obviously come to the fore via grammatic principles.  As I indicated, 
I believe the notion of transform-pairs cuts across both syntax 
and semantics, so I will pivot to some analyses which attend more 
to the semantic dimension.
`p`

`subsection.Semantic Analyses of Transform-Pairs`
`p.
In the simplest cases, a transform-pair represents a modifier 
adding conceptual detail to a ground, like `i.black dogs` 
from `i.dogs`/.  But the nature of this added detail 
%-- and its evident relation to surface language %-- can be very varied.  
Compare between examples like:

`sentenceList,
`sentenceItem; \label{itm:black} I saw my neighbor's two black dogs.
`sentenceItem; \label{itm:rescued} I saw my neighbor's two rescued dogs.
`sentenceItem; \label{itm:latest} I saw my neighbor's two latest dogs.
`sentenceList`

Whereas (\ref{itm:black}) presents a fairly straightforward conceptual 
tranformation, the detailing in (\ref{itm:rescued}) is a lot subtler; 
mentioning `i.rescued` dogs makes no reference to perceptual qualities, 
but rather implies intricate situational background.  The term 
`i.rescued dogs` strongly suggests that the dogs were adopted by their 
current owner, probably after an animal-welfare organization 
found them abandoned, or removed them from a prior abusive owner.  
This kind of backstory is packaged up, as a kind of 
situational prototype, in the conventionalized phrase 
`i.rescued dogs`/, implying a level of specificity more 
precise than the ajective `i.rescued` alone implies.  
Correspondingly, the verb `i.to rescue` when applied to 
dogs suggest more information than in more 
generic contexts.
`p`

`p.
The phrase `i.latest dogs` carries implications in its own 
right; we assume the neighbor had owned other dogs before.  
Of course `q.latest` implies some temporal order, but the 
understood time-scale depends on context.  
If we hear talk about a `i.vet`/s two latest dogs, we would presumably 
interpret this in terms of patients the vet has seen over the course of 
a day: 

`sentenceList,
`sentenceItem; \label{itm:vet} We're going after the vet's two latest dogs.
`sentenceItem; \label{itm:organization} I'm concerned about the rescue organization's 
two latest dogs.
`sentenceList`
 
Understanding the relevant time-frame depends on understading the relation 
between the dogs and the possessive antcedent.  In (\ref{itm:latest}) 
the neighbor (in a typical case) actually owns the dogs, so the 
situational context grounding the modifier `i.latest` would be understood 
against the normal time-scale for dog ownership (at least several years).  
In (\ref{itm:vet}), the vet only `q.possesses` the dogs in the sense 
of endeavoring to examine them, a process of minutes or hours.  
In (\ref{itm:organization}), the implication of the `i.organization's` 
possessive `visavis; rescued dogs is that the group endeavors 
to rehabilate and find permanent homes for the rescuees.  So in each 
case `i.latest` implies a succession of dogs, leading over time to 
two most recent ones, but the implied time-frame for our conceptualizing 
this sequence can be minutes-to-hours, or days-to-months, or years. 
`p`

`p.
We should also observe that the implied time-frames and backstories in 
(\ref{itm:rescued}-\ref{itm:organization}) are not directly signified via 
morphosyntax or lexical resources alone.  The word `i.rescued` only 
carries the `i.rescued dog` backstory when used in a context 
involving the dogs' eventual owners; in some context the more 
generic meaning of `i.rescue` could supercede:  

`sentenceList,
`sentenceItem; \label{itm:boatmen} Boatmen rescued dogs from the flooded streets.
`sentenceItem; \label{itm:firemen} Firemen rescued dogs from the burning building.
`sentenceList`

Neither (\ref{itm:boatmen}) nor (\ref{itm:firemen}) imply that the dogs were abandoned, or 
will have new owners, or be sent to a shelter, or that their rescuers are 
members of an animal-welfare organization %-- in short, no element of 
the conventionalized backstory usually invoked by `i.rescued dogs` is present.  
Analogously, there is no lexical subdivision for `i.latest` which regulates 
the variance in time-frames among (\ref{itm:latest}-\ref{itm:organization}).  
It is only by inferring a likely situational background that 
conversants will make time-scale assumptions based on one situation 
involving dog ownership, another involving veterinary exams, and a 
third involving animal-welfare rehabilitation.
`p`

`p.
That is to say, the time-scale inference I have analyzed is essentially 
`i.extralinguistic`/: there is no specific `i.linguistic` knowledge 
(lexical or grammatical, or even pragmatic inferences in the sense of 
deictic or anaphora resolution) which warrants the situational classification 
of (\ref{itm:latest}-\ref{itm:organization}) into different time scales.  
Instead, the inference is driven by (to some degree socially or culturally 
specific) background-knowlege about phenomena like veterinary clinics 
or animal rescue groups.  Whether or not the nuances in `i.rescued dogs` 
are similarly extra-linguistic is an interesting question %-- we can 
argue that the phrase is now entrenched as a `i.de facto` lexical 
entrant in its own right, so the role of `i.rescued` is not only to 
lend adjectival detail but to construct a recurring phrase with a 
distinct meaning, like `i.red card` (in football) or `i.stolen base` 
(in baseball).  Lexical entrenchment is, I would argue, an 
intra-linguistic phenomenon, in the sense that understanding entrenched 
phrases is akin to familiarity with specific word-senses, which is a 
properly linguistic kind of knowledge.  But even in that case, entrenchment 
is only possible because the phrase has a signifying precision more 
rigorous than its purely linguistic composition would imply.  
There are, in short, extralinguistic considerations governing `i.when` 
phrases are candidates for entrenchment, and a language-user's 
ability to learn the conventionalized meaning (which I believe 
is an intra-linguistic cognitive development) depends on their 
having the relevant (extra-linguistic) background knowledge. 
`p`

`p.
If we consider then the contrast between transform-pairs like 
`i.black dogs`/, `i.rescued dogs`/, and `i.latest dogs`/, 
the similar grammatic constructions %-- indeed similar semantic 
constructions, in that each pair has an adjective modifying a 
straightforward plural noun (`i.dogs` designates a similar 
concept in each case; this is not a case of surface grammar 
hiding semantic diversity, like `i.strong wine` vs. 
`i.strong opinion` or `i.long afternoon` vs. `i.long snake`/) 
%-- package transforms whose cognitive resolution spans a 
range of linguistic and extralinguistic considerations.  
Straightforward adjectival modification in `i.black dogs` gives 
way to lexical entrenchment in `i.rescued dogs` which, as 
I argued, carries significant extra-linguistic background knowledge 
even though possession of this knowledge is packaged into basic 
linguistic familiarity with `i.rescued dogs` as a signifying unit; 
and in the case of `i.latest dogs` the morphosyntactic evocation of 
temporal presedence and two different multiplicities (the latest 
dogs and earlier ones) is fleshed out by 
extra-linguistic estimations of time scale.  
The same surface-level linguistic structures, in short, can 
(or so such examples argue for) lead conversants on a 
cognitive trajectory in which linguistic and extra-linguistic factors 
interoperate in many different ways.  
`p`

`p.
This diversity should call into question the ability of conventional 
syntactic and semantic analysis to elucidate sentence-meanings with 
any precision or granularity.  Lexical and morphosyntactic 
observations may certainly reflect details which `i.contribute` to 
sentence-meanings, but the overall understading of each sentence in 
context depends on holistic, interpretive acts by competent 
language users in light of extra-linguistic, socially mediated 
background knowledge and situational understanding.  Contextuality 
applies here not only in the pragmatic sense that pronoun resolution, 
say, depends on discursive context (who is `i.her` in `i.her dogs`/); 
more broadly, transcending even pragmatics, context describes 
presumptive familiarity with conceptual structures like veterinary 
clinics, animal shelters, and any other real-world domain which 
provides an overall system wherein particular lexical significations 
can be standardized.  Without the requisite conceptual background it 
is hard to analyze how speakers can make sense even of 
well-established variations in word-sense, like `i.treat` as in 
a veterinarian treating a dog, a doctor treating a wound, a carpenter 
treating a piece of wood, or how an actor treats a part.  
These senses have lexical specificity only in the domain-specific 
contexts of medicine, carpentry, theater, and so forth.  
`p`

`p.
The problem of holistic cognitive interpretation (as requisite for 
sentence-meanings) can be seen even more baldly in examples where 
semantic readings bifurcate in ways wholly dependent on 
extra-linguitic conceptualization. Consider for instance: 

`sentenceList,
`sentenceItem; \label{itm:boroughs} All New Yorkers live in one of five boroughs.
`sentenceItem; \label{itm:commute} All New Yorkers complain about how long it 
takes to commute to New York City.
`sentenceList`

In (\ref{itm:boroughs}), `i.New Yorkers` refers specifically to everyone who lives 
in the City of New York, since the five boroughs collectively span the 
whole of city.  In (\ref{itm:commute}), by contrast, we should understand 
`i.New Yorkers` as referring to residents of the metropolitan area `i.outside` 
the city itself (who commute `i.to` the city); and moreover `i.All` should 
be read less then literally: we do not here the speaker in (\ref{itm:commute}) 
committing to the proposition that `i.every single` New Yorker complains.  
So both `i.All` and `i.New Yorkers` have noticeably different meanings in the 
two sentences.  However, I cannot find any purely linguistic mechanism 
(lexical, semantic, syntactic, morphological) which would account for 
these difference as linguistic signifieds `i.per se`/: the actual differences 
depend on conversants knowing some details about New York geography, and 
also general cultural background.  It does not make too much sense to 
commute to a place where you already live, so our conventional picture of 
the word `i.commute` constrains our interpretation of (\ref{itm:commute}) %-- 
but this depends on `i.commute` having a specific meaning, of traveling 
in to a city, usually from a suburban home, on a regular basis; a meaning 
in turn indebted to the norms of the modern urban lifestyle (it would be 
hard derive an analogous word-sense in the language spoken by a nomadic 
tribe, or a pre-industrial agrarian community).  Likewise, 
reading `i.All` in (\ref{itm:boroughs}) as `i.literally` `q.all` 
depends on knowing that the five boroughs are in fact the whole of 
the city's territory.
`p`

`p.
Given that in everyday speech quantifiers like `i.all` or `i.every` 
are often only approximate %-- and that designations like 
`i.New Yorker` are often used imprecisely, with not-identical 
alternative meanings intended on a case-by-case basis %-- 
these kind of examples point to signifying ambiguities that 
can easily arise as a consequence.  Often extra-linguistic 
considerations resolve the ambiguity by rejecting one or another 
(otherwise linguistically plausible) reading as non-sensical.  
Consider: 

`sentenceList,
`sentenceItem; \label{itm:beat} The Leafs failed to beat the Habs for the first time this year.
`sentenceItem; \label{itm:consecutive} The Leafs failed to win two consecutive games for the 
first time this year.
`sentenceItem; \label{itm:goal} The Leafs failed to score a goal for the 
first time this year.
`sentenceList`
  
Sentence (\ref{itm:beat}) has two competing readings: either the Toronto Maple Leafs 
won `i.all` or `i.none` of their previous games, in the relevant year, against the 
Montreal Canadiens.  The differense is whether `i.for the first time this year` 
attaches to `i.beat` or to `i.fail`/.  In (\ref{itm:consecutive}), 
on the other hand, the only sensible interpretation is that the Leafs had 
not yet won two games: while it is logically accurate to describe a 
team on a long winning streak as repeatedly winning two consecutive games, it would 
be very unexpected for (\ref{itm:consecutive}) to be used in a case where the Leafs 
lost their first game of the season, after a three-plus-game winning streak.  
And in (\ref{itm:goal}) any hockey fan would understand that the Leafs, for the first 
time, failed to score a goal; even though there is no linguistic rule 
foreclosing the reading such that the Leafs have not scored a goal at all 
%-- i.e., that for yet another game they failed to score a goal.    
`p`

`p.
These variations %-- the degree to which superficial ambiguity is actually 
perceived by competent language-users as presenting competing plausible 
meanings %-- depend on background factors; the contingencies of 
hockey fix how potential ambiguities resolve out because one or another 
alternative is extralinguistically incoherent.  But these cases point to 
how linguistic criteria alone, no matter how broadly understood, 
cannot necessarily predict in what sense linguistic structurations 
have empirically plausible meanings %-- or whether they have 
sensible meanings at all.
`p`

`p.
Notice however that the examples have alternate versions which are less 
subtle or ambiguous, which shows that the complications are not 
localized in the communicated ideas themselves, but in their 
typical linguistic encoding: 

`sentenceList,
`sentenceItem; \label{itm:allresidents} All residents of the city of New York live in one of five boroughs.
`sentenceItem; \label{itm:manyresidents} Many residents of the New York metropolitan area 
complain about how long it takes to commute to New York City.
`sentenceItem; \label{itm:leafshabs} For the first time this year, the Leafs failed to beat 
the Habs.
`sentenceList`

These versions are more logically transparent, in that their propositional 
content is more directly modeled by the structure of the sentences. 
Indeed, hearers unfamiliar with New York or with hockey might find these 
versions easier to understand; more context-neutral and journalistic.  
But perhaps for this reason the `q.journalistic` versions actually 
sound stilted or non-idiomatic for everyday discource.  
`p`

`p.
In short, even if sentences have a basically transparent 
logical content, `i.how` sentences holistically signify this 
content does not always emerge straightforwardly from semantic 
or syntactic structures on their own.  I think this weakens the 
case for semantic paradigms which concentrate on logically-structured 
content which appears to be signified through sentences 
%-- even if we grant that this propositional ground of meaning 
is real, it does not follow that propositional contents are 
designated by purely linguistic means, rather than by 
a cohort of cognitive processes many of which are extra-linguistic.  
This is the basis of my proposing `q.logicomorphic` qualities 
as one axis for evaluating sentences, which I will now discuss further.
`p`

`subjection.Logical Structure versus Sentence Structure`
`p.
Let us grant in general that particular sentences can be 
mapped to distinct, relatively transparent propositional 
contents.  In some cases sentences expresses propositional 
attitudes to such content (requests, commands, questioning) 
rather than unadorned locutionary assertions.  To 
properly respond to speech-acts, however (even ones with 
illocutionary force) conversants need to derive the 
content which is logically conjured via the discource, 
either as the speaker's primary intent or as a condition 
for that intent.  In effect, a proposition like `i.the window 
is closed` furnishes logical content to accertions like 
`i.The window is closed now` but also statements of 
belief (`i.I think the window is closed`/) or 
requests or opinions (`i.The window should be closed`/; 
`i.Pleace close the window`/). 
`p`

`p.
Philosophical treatments of language often imply that such 
propositional contents are the `i.essential` meanings within 
language; that analyzing semantic forms via logical 
structure is the core of a rigorous theory of semantics.  
It is certainly true that many elements of language 
can be translated, or deemed as conventionalized encodings 
for, structures in predicate logic %-- invocations of 
multiplicity and quantification; logical connectives between 
propositions; negations, modalities, and possibilia.  
This provides an analytic matrix wherein `i.some` 
sentences' structures may be analyzed.  I will argue, 
however, that in typical cases logical forms are 
invoked only indirectly %-- which calls into quection 
the applicability of logical analysis as explanatory 
vehicles for `i.linguistic` analysis in itself 
(as opposed to more general cognitive/extralinguistic 
processing). 
`p`

`p.
There are several cognitive operations requisite for grasping 
sentence-meanings as a logical gestalt: figuring individuals 
or multiplicities as conceptual fosi (verb subjects or objects); 
establishing relationships between individuals and multiplicities 
or amond multiplicities (member/part of, larger/smaller, 
overlap/disjoint); predicating properties to individuals 
or multiplicities; quantification; logical conjunction or disjunction, 
between predicates (also negation).  In some cases we can find 
these operations fairly directly encoded in explicit language form 
%-- sentences which are precise in figuring multiplicities numerically, 
or through unambiguous use of determiners like `i.all` and `i.every`/; 
which are structured to avoid scope ambiuities; which use transparent 
semantic resources to describe verb subjects and objects; and 
so forth.  In the most recent Universal Dependencies Shared Task corpus 
we can find examples like:  

`sentenceList,
`sentenceItem; \label{itm:tumour} It is the most common tumour found in babies, occurring in one of every 35,000 births.\udref{en_pud}{n01051007}
`sentenceItem; \label{itm:Dengue} Dengue fever is a leading cause of illness and death in the tropics and subtropics, with as many as 100 million people infected each year.
\udref{en_partut-ud-train}{en_partut-ud-1498}
`sentenceItem; \label{itm:TalibanKarzai} Many Taliban living in Afghanistan voted for President Karzai.
\udref{en_ewt-ud-train}{w03002048}
`sentenceItem; \label{itm:Ashraf} Most of the girls I was meeting had grown up in Mujahedeen schools in Ashraf, where they lived separated from their parents.
\udref{en_ewt-ud-train}{weblog-blogspot.com_rigorousintuition_20060511134300_ENG_20060511_134300-0112}
`sentenceItem; \label{itm:ChinaSpace} Most experts believe China intends to develop a small space station of its own over the next several years. 
\udref{en_ewt-ud-train}{newsgroup-groups.google.com_FOOLED_7554c5ce34a5a49e_ENG_20051012_144800-0020}
`sentenceItem; \label{itm:tastings} Check out their wine tastings every Friday night! 
\udref{en_ewt-ud-train}{reviews-322225-0005}
`sentenceItem; \label{itm:tag} For each start tag , there is a corresponding end tag.
\udref{en_lines-ud-train}{166}
`sentenceItem; \label{itm:Warhol} Each collection donated by the Andy Warhol Photographic Legacy Program holds Polaroids of well-known celebrities.
\udref{en_gum-ud-train}{GUM_news_warhol-35}
`sentenceList`

These sentences have straightforward logical structure, in terms of 
how they establish topical foci (`i.one of every 35,000 births`/, 
`i.as many as 100 million people`/, 
`i.Many Taliban`, `i.every Friday night`/, `i.For each start tag`/), 
and how predicates or references are bound together to create more 
precise significations (`i.the tropics and subtropics`/, 
`i.in Mujahedeen schools in Ashraf`/, `i.a corresponding end tag`/).
Properties ascribed to subject foci are neatly drawn, both in 
conveying the property intended and its bearer, accoriding to 
the sentence's terms: `i.the most common tumour found in babies`/, 
`i.a leading cause of illness and death`/, 
`i.China intends to develop a small space station`/, 
`i.holds Polaroids of well-known celebrities`/.  With 
aggregate foci and/or quantification, there is an unambiguous 
framing of predication and quantifier scope %-- Each collection 
has its set of Polaroids; the set of Karzai voters, Dengue infections, 
birth tumours, etc., are crisply figured.
`p`

`p.
For many philosophers of language, identiying similar 
logical structuration is an intrinsic aspect of 
coming to terms with human language in general.  This paradigm 
also reinforces the goal of `AI; Natural Language Processing, 
because computers can certainly engage in the kind of 
symbolic-logical reasoning outlining signified meanings in cases 
where language reciprocates propositional morphology very clearly.  
The problem is that language artifacts very often cloak their 
logical core, such that examples like (\ref{itm:tumour}-\ref{itm:Warhol}) 
are not representarive of language as a whole.  Logical patterns 
may certainly be present, but they are not necessarily 
structurally reproduced in surface-level formations; rather a 
sentences' propositional content may depend on a subtle 
interpretive trajectory.  I will present examples throughout this 
paper, but a few further corpus items are reasonable 
case-studies: 


`sentenceList,
`sentenceItem; \label{itm:ants} A furry black band of ants led up a cupboard door to some scrap that had flicked from a plate.
\udref{en_lines-ud-train}{2157}
`sentenceItem; \label{itm:current-waiting-period} The current waiting period is eight weeks.
\udref{en_pud}{n01050009}
`sentenceItem; \label{itm:immersed} I think that's why they immersed themselves in pattern and colour.
\udref{en_pud}{n01087018}
`sentenceItem; \label{itm:princess} With her appearance finalized, Jasmine became Disney's first non-white princess as opposed to being of European heritage.
\udref{en_pud}{w01119076}


`sentenceList`

`p`

`p.

`p`

