\section{Hub Applications and Gatekeeper Code} 
%\phantomsection\label{sOne}
\p{To begin, I will speak in general terms about hub applications and 
about the unique coding challenges which derive from CyberPhysical 
technologies' unique networking and safety requirements.
Implementing software hubs 
introduces technical challenges which are distinct from 
manufacturing CyberPhysical devices themselves \mdash{} in particular, 
devices are usually narrowly focused on a particular 
kind of data and measurement, while software hubs are 
multi-purpose applications that need to understand and 
integrate data from a variety of different kinds of 
devices.  CyberPhysical software hubs also 
present technical challenges that are different from 
other kinds of software applications, even if 
these hubs are one specialized domain in the larger 
class of user-focused software.
}
\p{Any software application provides human users with tools to 
interactively and visually access data and computer 
files, either locally (data encoded on the \q{host} computer running 
the software) or remotely (data accessed over a network).  
Computer programs can be generally classified as 
\i{applications} (which are designed with a priority 
to User Experience) and \i{background processes} 
(which often start and maintain their state automatically 
and have little input or visibility to human users, 
except for special troubleshooting circumstances).  
Applications, in turn, can be generally classified as 
\q{web applications} (where users usually see one 
resource at a time, such as a web page displaying some 
collection of data, and where data is usually stored 
on remote servers) and \q{native applications} 
(which typically provide multiple windows and 
Graphical User Interface components, and 
which often work with data and files saved 
locally \mdash{} i.e., saved on the filesystem 
of the host computer).  
Contemporary software design also recognizes
\q{hybrid} applications which combine features 
of web and of native (desktop) software.   
}
\p{Within this taxonomy, the typical CyberPhysical 
software hub should be classified as a native, 
desktop-style application, representing the 
state of networked devices through 
special-purpose Graphical User Interface 
(\GUI{}) components.  Networked CyberPhysical 
devices are not necessarily connected to the 
Internet, nor communicate via Internet 
protocols.  In many cases, software hubs will 
access device data through securitized, closed-circuit 
mechanisms which (barring malicious intrusion) ensure 
that only the hub application can read or alter 
devices' state.  Accordingly, an application reading 
device data is fundamentally different than a 
web application obtaining information from an Internet 
server.\footnote{It may be appropriate for some device data \mdash{} either 
in real time or retroactively \mdash{} to be shared 
with the public via Internet connections, but 
this is an additional feature complementing 
the monitoring software's primary oversight roles.
}  CyberPhysical networks are designed to 
prioritize real-time connections between device and 
software points, and minimize network latency.  
Ideally, human monitors should be able 
(via the centralized software) to alter device state 
almost instantaneously.  Moreover, in contrast to 
Internet communications with the \TCP{} protocol, 
data is canonically shared between devices and 
software hubs in complete units \mdash{} rather than 
in packets which the software needs to reassemble.  
These properties of CyberPhysical networks imply 
that software design practices for monitoring 
CyberPhysical Systems are technically different 
than requirements for web-based components, such as 
\HTTP{} servers.    
}
\p{At the same time, we can assume that an increasing quantity of 
CyberPhysical data \i{will} be shared via the World Wide Web.  
This reflects a confluence of societal and technological 
forces: in the bioinformatics domain, public demand is rising for access
both to conventional medical information and to real-time health-related 
data (often via \q{wearable} sensors and other technologies that, 
when properly deployed, can promote health-conscious lifestyles).  
Similarly, the public demands greater transparency for 
civic and environmental data, and science is learning how to 
use CyberPhysical technology to track ecological conditions and 
urban infrastructure \mdash{} analysis of traffic patterns, for 
instance, can help civic planners optimize public transit routes 
(which benefit both the public and the environment).
}
\p{Meanwhile, parallel to the rise of accessible health or civic data,
companies are bringing to market an increasing 
array of software products and \q{apps} which access and 
leverage this data.  These applications do not necessarily 
fit the profile of \q{hub software}, but they 
can still benefit from the rigorous data models 
exemplified via hub libraries.  
}
\p{To cite a concrete example, Teixeira \i{et. al.} 
consider \q{smart appliance} refrigerators which are aware of 
the door being left open, and even may track the expiration date of 
items inside, with user notifications on a smart phone.  
On the other hand, consider an industrial 
refrigerator used in a food-processing plant or warehouse.  We would 
expect that such an appliance would likewise provide data about 
current temperature, air flow, electricity usage, when the doors 
are opened, and so forth; on the other hand, the software for 
accessing this data would probably run in a nearby control room, 
and so would be implemented on a desktop 
(rather than smart phone) operating system.  
We might also expect that quality controls in the industrial 
setting would be more rigorous than for home appliances.  
But this also means that data models formulated in the 
industrial setting can be adapted for the home context, 
which would sampling simpler data structures from 
more complex ones.  That is to say, standardized data 
models can arise from industrial needs and eventually 
be incorporated into personal or smart-home appliances, 
which may be a more likely progression than standardization 
driven by the \q{personal} market alone.
}
\p{The refrigerator example also introduces User Interface 
design issues.  A simple indicator showing whether doors 
are open is straightforward, but an interface listing 
food items in a refrigerator is much more complicated.  
Assuming (for the sake of this rather futuristic 
discussion) that a refrigerator can detect signals 
emanating from food items (or, more precisely, 
their containers), we can envision a list of 
items (maybe with their expiration dates) 
presented as a \GUI{} component, maybe 
with one food item per line 
(lines perhaps showing a picture, text description, 
dates, etc.).  This would require cross-referencing 
numeric codes, which might be broadcast by the 
items \i{inside} the refrigerator, against a 
database that would load images, descriptions, and 
any other \GUI{} content relevant to that item.  One question 
is then where this database would be hosted, 
and how it would be 
updated (insofar as food companies develop new products 
fairly often; any static database could quickly get 
outdated).  Food companies (or some middleware agent) 
would have to agree on a common format so that the 
refrigerator's access software can integrate data from 
many brands.  Since a refrigerator can hold many items, 
the \GUI{} would also need enough screen space 
(and maybe a multi-level design) for users to comfortably 
browse many artifacts; perhaps a line-by-line window 
supplemented with separate dialog windows for each item.  
It would be difficult to provide this advanced a \GUI{} 
via a phone app or an in-kitchen touch-screen.
}
\p{This points to another important difference between 
hub applications and isolated access points (such 
as phone apps specific to single devices).  
With app-based access to 
CyberPhysical data, engineers are pressured to 
condense User Interface component to small-screen scales.  
In the current example, an app would perhaps refrain from 
listing all items in the fridge, and instead just show the 
user a notification when an item is near expiry.  When 
adapting to phone-like usage patterns (limited screen, 
brief but frequent user engagement), designers 
manifestly try to offer brief but curated 
snippets of information, rather than comprehensive access into a 
data space.  That is, software can try to anticipate 
which pieces of data carry the most user interest \mdash{} expiry 
dates are most likely important to users when items are 
near perishing \mdash{} so that interface design is organized 
around these select nuggets, rather than all the information 
which a user could theoretically access.  This also means 
that data mining, Artificial Intelligence, and other 
techniques for anticipating users' needs becomes 
proportionately more consequential: if the whole 
\GUI{} design is premised on \AI{}, then \AI{} ceases 
to become a useful supplement but becomes a 
make-or-break capability for the application to be 
successful from a User Experience point of view.  
If an app cannot practically share \i{all} data, 
it has to guess what data users most want to see.
}
\p{Conversely, if we assume that hub applications would prototypically 
adopt the \q{look and feel} of native desktop applications, 
then they can present a responsive, comprehensive overview 
of data structures to their users.  Applications can 
take advantage of more screen space, with secondary 
application windows and other interactive features that 
we associate with native \GUI{} components.  This arguably 
can render \AI{} less important, because it is easier for 
users to interact with the software on their own terms.  
As Teixera \i{et. al.} put it, \q{some authors argue that 
the number of interactions between users and the smart home 
must be kept to a minimum}, but 
\q{to remove obstacles in the adoption 
of smart home systems ... preserving the autonomy of the 
user may seem like the most sensible course of action}.  
The more data that can be shown, the less need for 
software to filter information on users' behalf.     
}
\p{Hub applications are therefore examples of what 
Teixera \i{et. al.} call \q{user-centric} design.  
In this guise, hubs 
have at least three key responsibilities: 
\begin{enumerate}\item{} To present device and system data for human 
users, in graphical, interactive formats suitable 
for humans to oversee the system and intervene 
as needed.
\item{} To validate device and system data ensuring 
that the system is behaving correctly and predictably.
\item{} To log data (in whole or in part) for subsequent 
analysis and maintenance.
\end{enumerate}
Once software receives device data, it needs to 
marshal the information between different formats, 
exposing the data in the different contexts of 
\GUI{} components, database storage, and 
analytic review.  Consider the example of a 
temperature reading, with \GPS{} device location and 
timestamp data (therefore a four-part structure 
giving temperature at one place and time).  
The software needs, in a typical scenario, to do 
several things with this information: it has 
to check the data to confirm it fits within 
expected ranges (because malformed data can indicate 
physical malfunction in the devices or the network).  
It may need to show the temperature reading to a 
human user via some visual or textual indicator.  
And it may need to store the reading in a database 
for future study or troubleshooting.  In these 
tasks, the original four-part data structure is 
transformed into new structures which are 
suitable for verification-analytics, \GUI{} programming, 
and database persistence, respectively.     
}
\p{The more rigorously that engineers understand and document 
the morphology of information across these different  
software roles, the more clearly we can 
define protocols for software design and user expectations.  
Careful design requires answering many technical questions: 
how should the application respond if it encounters 
unexpected data?  How, in the presence of erroneous data, 
can we distinguish device malfunction from coding error?  
How should application users and/or support staff 
be notified of errors?  What is the optimal Interface Design 
for users to identify anomalies, or identify situations 
needing human intervention, and then be able to 
perform the necessary actions via the software?  
What kind of database should hold system data retroactively, 
and what kind of queries or analyses should engineers 
be able to perform so as to study system data, to access the 
system's past states and performance? 
}
\p{I believe that the software development community has neglected 
to consider general models of CyberPhysical software
which could answer these kinds of questions in a rigorous, 
theoretically informed manner.  There is of course a robust 
field of cybersecurity and code-safety, which establishes 
Best Practices for different kinds of computing projects.  
Certainly this established knowledge can and does influence 
the implementation of software connected to CyberPhysical 
systems no less than any other kind of software.  But 
models of programming Best Practices are often associated 
with specific coding paradigms, and therefore reflect 
implementations' programming environment more than they 
reflect the empirical domain targeted by a particular 
software project.
}
\p{For example, Object-Oriented Programming, 
Functional Programming, and Web-Based Programming present 
different capabilities and vulnerabilities and therefore
each have their own \q{Best Practices}.  As a result, 
our understanding of how to deploy robust, well-documented 
and well-tested software tends to be decentralized
among distinct programming styles and development 
environments.  External analysis of a code base \mdash{} e.g., searching 
for security vulnerabilities (attack routes for malicious code) 
\mdash{} are then separate disciplines with their own methods 
and paradigms.  Such dissipated wisdom is unfortunate if 
we aspire to develop integrated, broadly-applicable models 
of CyberPhysical safety and optimal application 
design, models which transcend paradigmatic 
differences between coding styles and roles 
(in the sense that implementation, testing, and code 
review, for instance, are distinct technical roles).
}
\p{Because CyberPhysical devices are intrinsically \i{networked} 
\mdash{} whether over special wireless networks or the World Wide Web \mdash{} 
there is an enlarged \q{surface area} for vulnerability.  Moreover, 
because they are often worn by people
or used in a domestic setting, they tend carry personal (e.g., location)
information, making network security protocols especially important
(\cite{RonaldAshri}, \cite{AbhishekDwivedi}, \cite{LalanaKagal},
\cite{TakeshiTakahashi}, \cite{MozhganTavakolifard},
\cite{BhavaniThuraisingham}).  In brief, the dangers
of coding errors and software vulnerabilities, in CyberPhysical
Systems like the Internet of Things (\IoT{}), are even more pronounced
than in other application domains.  While it is
unfortunate if a software crash causes someone to lose data,
for example, it is even more serious if a CyberPhysical \q{dashboard}
application were to malfunction and leave physical, networked
devices in a dangerous state.
}
\p{It is also helpful at this point to distinguish cyber 
\i{security} from \i{safety}.  When these concepts are 
separated, \i{security} generally refers to
preventing \i{deliberate}, \i{malicious} intrusion into 
CyberPhysical networks.  Cyber \i{safety} refers to preventing 
unintended or dangerous system behavior due to innocent human 
error, physical malfunction, or incorrect programming.  
Malicious attacks \mdash{} in particular the risks of 
\q{cyber warfare} \mdash{} are prominent in the 
public imagination, but innocent coding errors or design 
flaws are equally dangerous.  Incorrect data readings, 
for example, led to recent Boeing 737 MAX jet accidents 
causing over 300 fatalities (plus the worldwide grounding
of that airplane model and billions of dollars in losses 
for the company).  Software failures either 
in runtime maintenance or anticipatory risk-assessment 
have been identified as contributing factors to 
high-profile accidents like Chernobyl \cite{MikhailMalko} 
and the Fukushima nuclear reactor 
meltdown \cite{JoonEonYang}.
A less tragic but noteworthy 
case was the 1999 crash of NASA's US \$125 million 
Mars Climate Orbiter.  This crash was caused by 
software malfunctions which in turn were caused 
by two different software components producing 
incompatible data \mdash{} in particular, using 
incompatible scales of measurement 
(resulting in an unanticipated mixture of 
imperial and metric units).  In general, it 
is reasonable to assume that coding errors 
are among the deadliest and costliest sources 
of man-made injury and property damage. 
}
\p{Given the risks of undetected data corruption, seemingly 
mundane questions about how CyberPhysical applications verify 
data \mdash{} and respond to apparent anomalies \mdash{} 
become essential aspects of planning and development.  
Consider even a simple data aggregate like 
blood pressure (combining systolic and 
diastolic measurements).  Empirically, systolic pressure is 
always greater than diastolic.  Software systems 
need commensurately to agree on a protocol for encoding the 
numbers to ensure that they are in the correct order, and that they 
represent biologically plausible measurements.  
How should a particular software component test that 
received blood pressure data is accurate?  Should it 
always test that the systolic quantity is indeed 
greater than the diastolic, and that both numbers 
fall in medically possible ranges?  How should the 
component report data which fails this test?  If 
such data checking is not performed \mdash{} on the 
premise that the data will be proofed elsewhere 
\mdash{} then how can this assumption be 
justified?
}
\p{In general, how can engineers identify, in a 
large and complex software system, all the points 
where data is subject to validation tests; and 
then by modeling the overall system in term 
of these check-points ensure that all needed 
verifications are performed at least one time?  
Continuing the blood-pressure example, 
how would a software procedure that \i{does} 
check the integrity of the systolic/diastolic 
pair indicate for the overall system model 
that it performs that particular verification?  
Conversely, how would a procedure which does 
\i{not} perform that verification indicate 
that this verification must be performed 
elsewhere in the system, to guarantee that 
the procedure's assumptions are satisfied?    
} 
\p{These questions are important not only for objective, 
measurable assessments of software quality, but 
also for people's more subjective trust in the reliability 
of software systems.  In the modern world we 
allow software to be a determining factor in 
systems' behavior, in places
where malfunction can be fatal \mdash{} airplanes, hospitals, 
electricity grids, trains carrying toxic chemicals, 
highways and city streets, etc.  
Consider the model of \q{Ubiquitous Computing} pertinent to the
book series to which this volume (and hence
this chapter) belongs.  As explained in the
series introduction:\footnote{\url{https://sites.google.com/view/series-title-ausah/home?authuser=0}
} 
\begin{dquote}U-healthcare systems ... will allow physicians to remotely diagnose, access, and monitor critical patient's symptoms and will enable real time communication with patients.  [This] 
series will contain systems based on the four future ubiquitous sensing for healthcare (USH) principles, namely i) proactiveness, where healthcare data transmission to healthcare providers has to be done proactively to enable necessary interventions, ii) transparency, where the healthcare monitoring system design should transparent, iii) awareness, where monitors and devices should be tuned to the context of the wearer, and iv) trustworthiness, where the personal health data transmission over a wireless medium requires security, control and authorize access.
\end{dquote}
Observe that in this scenario, patients will have to 
place a level of trust in Ubiquitous Health technology comparable 
to the trust that they place in human doctors and other 
health professionals.   
}
\p{All of this should cause software engineers and developers to 
take notice.  Modern society places trust in doctors 
for well-rehearsed and legally scrutinized reasons: 
physicians need to rigorously prove their competence 
before being allowed to practice medicine, and 
this right can be revoked due to malpractice.  Treatment 
and diagnostic clinics need to be licenced, 
and pharmaceuticals (as well as medical equipment) are subject 
to rigorous testing and scientific investigation before being 
marketable.  Notwithstanding \q{free market} ideologies, 
governments are aggressively involved in regulating 
medical practices; commercial practices (like marketing) are 
constrained, and operational transparency 
(like reporting adverse outcomes) is mandated, more so 
than in most other sectors of the economy.  This level of 
oversight \i{causes} the public to trust that clinicians' 
recommendations are usually correct, or that medicines are 
usually beneficial more than harmful.  
}
\p{The problem, as software becomes an increasingly central feature 
of the biomedical ecosystem, is that no commensurate oversight 
framework exists in the software world.
Biomedical \IT{} regulations tend to be ad-hoc and narrowly domain-focused. 
For example, code bases in the United States which manage HL-7 
data (the current federal Electronic Medical Record format) must 
meet certain requirements, but there is no comparable framework 
for software targeting other kinds of health-care information.  
This is not only \mdash{} or not primarily \mdash{} an issue of 
lax government oversight.  The deeper problem is that 
we do not have a clear picture, in the framework of 
computer programming and software development, of 
what a robust regulatory framework would look like: what 
kind of questions it would ask; what steps a company could 
follow to demonstrate regulatory compliance; what indicators 
the public should consult to check that any software 
that could affect their medical outcomes is properly vetted.  
And, outside the medical arena, similar comments could be 
made regarding software in CyberPhysical settings like 
transportation, energy (power generation and electrical 
grids), physical infrastructure, environmental protections, 
government and civic data, and so forth 
\mdash{} settings where software errors threaten personal
and/or property damages.
}
\p{In short, the public has a relatively inchoate 
idea of issues related to cyber safety, security, and 
privacy: we (collectively) have an informal impression that 
current technology is failing to meet the public's desired 
standards, but there is no clear picture of what 
\IT{} engineers can or should do to improve the technology 
going forward.  Needless to say, software should prevent 
industrial catastrophes, and private financial data 
should not be stolen by crime syndicates.  But, beyond these 
obvious points, it is not clearly defined how 
the public's expectations for safer and more secure 
technology translates to low-level programming practices.  
How should developers earn public trust, and 
when is that trust deserved?  Maxims like \q{try to avoid 
catastrophic failure} are too self-evident to be useful.  
We need more technical structures to identify 
which coding practices are explicitly recommended, 
in the context of a dynamic where engineers need 
to earn the public trust, but also need to define 
the parameters for where this trust is warranted.    
Without software safety models rooted in low-level 
computer code, software safety can only be ex-post-facto 
engineered, imposing requirements relatively late in the 
development cycle and checking code externally, via 
code review and analysis methods that are beyond
the core development process.  While such secondary 
checking is important, it cannot replace software built 
with an eye to safety from the ground up. 
}
\p{This chapter, then, is written from the viewpoint that 
cyber safety practices have not been clearly articulated 
at the level of software implementation itself, 
separate and apart from institutional or governmental oversight.  
Regulatory oversight is only effective in proportion to 
scientific clarity \visavis{} desired outcomes and how 
technology promotes them.  Drugs and treatment protocols, 
for instance, can be evaluated through \q{gold standard} 
double-blind clinical trials \mdash{} alongside statistical 
models, like \q{five-sigma} criteria, which measure 
scientists' confidence that trial results are truly 
predictive, rather than results of random chance.  This package 
of scientific methodology provides a framework which can 
then be adopted in legal or legislative contexts.  
With respect to medications, policy makers can stipulate that 
pharmaceuticals should be tested in double-blind trials, 
with statistically verifiable positive results, before 
being approved for general-purpose clinical use.  Such a  
well-defined policy approach \i{is only possible} because 
there are biomedical paradigms which define how treatments 
can be tested to maximize the chance that positive test 
results predict similar results for the 
general patient population.
}
\p{Analogously, a general theory of cyber safety should 
be a software-design issue before it becomes a 
policy or contractual issue.  Software engineering and 
programming language design needs its own evaluative 
guidelines; its own analogs to double-blind trials 
and five-sigma confidence.  It is at the region of
low-level software design \mdash{} of actual source code 
in its local implementation and holistic integration 
\mdash{} that engineers can develop technical \q{best practices} 
which then provide substance to regulative oversight.  
Stakeholders or governments can recommend (or require) that 
certain practices adopted, but only if engineers 
have identified practices which are believed, 
on firm theoretical ground, to effectuate safer, 
more robust software.  
}
\subsection{Gatekeeper Code}
\p{There are several design principles which can help ensure 
safety in large-scale, native/desktop-style \GUI{}-based 
applications.  These include:
\begin{enumerate}\item{}  Identify operational relationships between types.  
Suppose \calS{} is a data structure modeled via type \caltypeT{}.  
This type can then be associated with a type (say, 
\typeTp{}) of \GUI{} components which visually display 
values of type \caltypeT{}.  A simple data structure 
may have \GUI{} representation via small \q{widgets} 
embedded in other components (consider a thermometer icon 
to display temperature).  Conversely, if \calS{} has many component 
parts, its corresponding \GUI{} type may need to span its 
own application window, with a collection of nested textual 
or graphical elements.  There may also be a type 
(say, \typeTpp{}) representing \caltypeT{}-values in a format 
suitable for database persistence.  Application code should 
explicitly indicate these sorts of inter-type relationships.
\item{}  Identify coding assumptions which determine the validity 
of typed values and of function calls.  For each 
application-specific data type, consider whether every 
computationally possible instance of that type is actually 
meaningful for the real-world domain which the type represents.  
For instance, a type representing blood pressure has a subset 
of values which are biologically meaningful \mdash{} where systolic 
pressure is greater than diastolic and where both numbers are 
in a sensible range.  Likewise, for every procedure defined 
on application-specific data types, consider whether the procedure 
might receive arguments that are computationally feasible but 
empirically nonsensical.  Then, establish a protocol for 
acting upon erroneous data values or procedure parameters.  
How should the error be handled, without disrupting the 
overall application?
\item{}  Identify points in the code base which represent new data 
being introduced into the application, or code which can materially 
affect the \q{outside world}.  Most of the code behind \GUI{} 
software will manage data being transferred between different 
parts of the system, internally.  However, there will be 
specific code sites \mdash{} e.g., specific procedures \mdash{} which 
receive new data from external sources, or respond to 
external signals.  A simple example is, for desktop applications, 
the preliminary code which runs when users click a mouse button.  
In the CyberPhysical context, an example might be code which 
is activated when motion-detector sensors signal something moving 
in their vicinity.  These are the \q{surface} points where data 
\q{enters the system}.
\pseudoIndent{}
Conversely, other code points localize 
the software's capabilities to initiate external effects.  For 
instance, one consequence of users clicking a mouse button might 
be that the on-screen cursor changes shape.  Or, motion detection 
might trigger lights to be turned on.  In these cases the software 
is hooked up to external devices which have tangible capabilities, 
such as activating a light-source or modifying the on-screen cursor.  
The specific code points which leverage such capabilities 
represent data \q{leaving the system}.  
\pseudoIndent{}
In general, it is important to identify points where data 
\q{enters} and \q{leaves} the system, and to distinguish 
these points from sites where data is transferred 
\q{inside} the application.  This helps ensure that 
incoming data and external effects are properly vetted.  
Several mathematical frameworks have been developed 
which codify the intuition of software components as 
\q{systems} with external data sources and effects, 
extending the model of software as self-contained 
information spaces: notably, Functional-Reactive Programming 
(see e.g. \cite{WolfgangJeltsch}, \cite{JenniferPaykin},
\cite{PaykinKrishnaswami}) and the theory of
Hypergraph Categories 
(\cite{InteractingConceptualSpaces}, \cite{BrendanFong}, 
\cite{BrendanFongThesis}, \cite{AleksKissinger}). 
\end{enumerate}
Methods I propose in this chapter are applicable to each 
of these concerns, but for purposes of exposition I 
will focus on the second issue: testing 
type instances and procedure parameters for fine-grained 
specifications (more precise than strong typing alone). 
}
\p{Strongly-typed programming language offer some guarantees on 
types and procedures: a function which takes an integer will 
never be called on a value that is \i{not} an integer 
(e.g., the character-string \q{\lclc{46}} instead of the \i{number} 
\lclc{46}).  Likewise, a type where one field is an integer 
(representing someone's age, say), will never be instantiated 
with something \i{other than} an integer in that field.  
Such minimal guarantees, however, are too coarse for 
safety-conscious programming.  Even the smallest 
(8-bit) unsigned integer type would permit someone's age to 
be \lclc{255} years, which is surely an error.  So any 
safety-conscious code dealing with ages needs to check that 
the numbers fall in a range narrower than built-in 
types allow on their own, or to ensure that such checks are 
performed ahead of time.   
}
\p{The central technical challenge of safety-conscious coding 
is therefore to \i{extend} or \i{complement} each programming 
languages' built-in type system so as to represent 
more fine-grained assumptions and specifications.  
While individual tests may seem straightforward on a 
local level, a consistent 
data-verification architecture \mdash{} how this coding dimension 
integrates with the totality of software features and 
responsibility \mdash{} can be much more complicated.  
Developers need to consider several overarching questions, 
such as: 
\begin{itemize}\item{} Should data validation be included in the same 
procedures which operate on (validated) data, or 
should validation be factored into separate procedures?
\item{} Should data validation be implemented at the type 
level or the procedural level?  That is, should specialized 
data types be implemented that are guaranteed only to 
hold valid data?  Or should procedures work with more 
generic data types, and perform validations on a case-by-case 
basis?
\item{} How should incorrect data be handled?  In CyberPhysical software, 
there may be no obvious way to abort an operation in the 
presence of corrupt data.  Terminating the application may not be 
an option; silently canceling the desired operation or trying to substitute 
\q{correct} or \q{default} data may be unwise; and 
presenting technical error messages to human users may be confusing.  
\end{itemize}
These questions do not have simple answers.  As such, we 
should develop a rigorous theoretical framework so as to 
codify the various options involved \mdash{} what architectural 
decisions can be made, and what are the strengths and weaknesses 
of different solutions.
}
\p{I will use the term \i{gatekeeper code} for any code which checks 
programming assumptions more fine-grained than strong typing 
alone allows \mdash{} for example, that someone's age is not reported 
as \lclc{255} years, or that systolic pressure is not recorded as 
less than diastolic.  I will use the term \i{fragile code} for
code which \i{makes} programming assumptions \i{without itself} 
verifying that such assumptions are obeyed.  Fragile code is 
especially consequential when incorrect data would cause the 
code to fail significantly \mdash{} to crash the application, 
enter an infinite loop, or any other nonrecoverable scenario.
}
\p{Note that \q{fragile} is not a term of criticism \mdash{} some algorithms 
simply work on a restricted space of values, and it is inevitable 
that code implementing such algorithms will only behave properly 
when provided values with the requisite properties.  It is necessary 
to ensure that such algorithms are \i{only} called with 
correct data.  But insofar as testing of the data lies outside 
the algorithms themselves, the proper validation has to occur 
\i{before} the algorithms commence.  In short, \i{fragile} and
\i{gatekeeper} code often has to be paired off: for each 
segment of fragile code which \i{makes} assumptions, there should 
be a corresponding segment of gatekeeper code which
\i{checks} those assumptions.  
}
\p{In that general outline, however, there is room for a variety 
of coding styles and paradigms.  Perhaps these can be broadly 
classified into three groups: 
\begin{enumerate}\item{} Combine gatekeeper and fragile code in one procedure.
\item{} Separate gatekeeper and fragile code into different procedures.
\item{} Implement narrower types so that gatekeeper code is 
called when types are first instantiated.
\end{enumerate}
Consider a function which calculates the difference between 
systolic and diastolic blood pressure, returning an unsigned
integer.  If this code were called with malformed data wherein 
systolic and diastolic are inverted, the difference would 
be a negative number, which (under binary conversion to an 
unsigned integer) would come out as a potentially 
extremely large positive number (as if the patient had 
blood pressure in, say, the tens-of-thousands).  This nonsensical 
outcome indicates that the basic calculation is fragile.  
We then have three options: test \q{systolic-greater-than 
diastolic} \i{within the procedure}; require that this test 
be performed prior to the procedure being called; 
or use a special data structure configured such that 
systolic-over-diastolic can be confirmed as soon as 
any blood-pressure value is constructed in the system.
}
\p{There are strengths and weaknesses of each option.  
Checking parameters at the start of a procedure makes 
code more complex and harder to maintain, and also 
makes updating the code more difficult.  The 
blood-pressure case is a simple example, but in real 
situations there may be more complex data-validation 
requirements, and separating code which \i{checks} 
data from code which \i{uses} data, into different 
procedures, may simplify subsequent code maintenance.
If the \i{validation} code needs to be modified 
\mdash{} and if it is factored into its its own procedure \mdash{}  
this can be done without modifying the 
code which actually works on the data (reducing the 
risk of new coding errors).  In short, factoring 
\i{gatekeeper} and \i{fragile} code into separate 
procedures exemplifies the programming principle of 
\q{separation of concerns}.  On the other hand, 
such separation creates a new problem of ensuring that 
the gatekeeping procedure is always called.  
Meanwhile, using special-purpose, narrowed data types 
adds complexity to the overall software if these data types
are unique to that one code base, and therefore 
incommensurate with data provided by external sources.  
In these situations the software must transform data between 
more generic and more specific representations before 
sharing it (as sender or receiver), which makes 
the code more complicated.  
} 
\p{In the specific CyberPhysical context, gatekeeping is especially 
important when working with device data.  Such data is 
almost always constrained by the physical construction of 
devices and the kinds of physical quantities they measure 
(if they are sensors) or their physical capabilities 
(if they are \q{actuators}, devices that cause changes in their 
environments).  For sensors, it is an empirical question what 
range of values can be expected from properly functioning 
devices (and therefore what validations can check that the 
device is working as intended).  For actuators, it should 
be similarly understood what range of values guarantee 
safe, correct behavior.  For any device then we can 
construct a \i{profile} \mdash{} an abstract, mathematical 
picture of the space of \q{normal} values associated with 
proper device performance.  Gatekeeping code can 
then ensure that data received from or sent to devices 
fits within the profile.  Defining device profiles, and 
explicitly notating the corresponding gatekeeping code, 
should therefore be an essential pre-implementation planning 
step for CyberPhysical software hubs.  
}
\p{Fragile code is not necessarily a sign of poor design.  Sometimes
implementations can be optimized for special circumstances, and
optimizations are valuable and should be used wherever possible.  Consider an
optimized algorithm that works with two lists that must be the same size.
Such an algorithm should be preferred over a less efficient
one whenever possible \mdash{} which is to say, whenever dealing with two
lists which are indeed the same size.  Suppose this algorithm is
included in an open-source library intended to be shared among many different
projects.  The library's engineer might, quite reasonably, deliberately
choose not to check that the algorithm is invoked on same-sized lists
\mdash{} checks that would complicate the code, and sometimes slow the
algorithm unnecessarily.  It is then the responsibility of code that
\i{calls} whatever procedure implements the algorithm to ensure that it
is being employed correctly \mdash{} specifically, that this
\q{client} code does \i{not} try
to use the algorithm with \i{different-sized} lists.  Here \q{fragility} is
probably well-motivated: accepting that algorithms are sometimes
implemented in fragile code can make the code cleaner, its intentions
clearer, and permits their being optimized for speed.
}
\p{The opposite of fragile code is sometimes called \q{robust} code.
While robustness is desirable in principle, code which simplistically
avoids fragility may be harder to maintain than deliberately fragile but
carefully documented code.  Robust code often has to check for many
conditions to ensure that it is being used properly, which can make
the code harder to maintain and understand.  The hypothetical
algorithm that I contemplated last paragraph
could be made robust by \i{checking}
(rather than just \i{assuming}) that it is invoked with same-sized lists.
But if it has other requirements \mdash{} that the lists are non-empty,
and so forth \mdash{} the implementation can get padded with a chain of
preliminary \q{gatekeeper} code.  In such cases the gatekeeper
code may be better factored into a different procedure, or expressed
as a specification which engineers must study before attempting to
use the implementation itself.
}
\p{Such transparent declaration of coding assumptions and specifications can
inspire developers using the code to proceed attentively,
which can be safer in the long run than trying to avoid fragile code
through engineering alone.  The takeaway is that while \q{robust} is
contrasted with \q{fragile} at the smallest scales (such as
a single procedure), the overall goal is systems and components that are robust at the
largest scale \mdash{} which often means accepting \i{locally} fragile
code.  Architecturally, the ideal design may combine
individual, \i{locally fragile} units with rigorous documentation and gatekeeping.
So defining and declaring specifications is
an intrinsic part of implementing code bases which are both robust
and maintainable.
}
\p{Unfortunately, specifications are often created
only as human-readable documents, which might have a semi-formal
structure but are not actually machine-readable.
There is then a disconnect between features \i{in the code itself} that
promote robustness, and specifications intended for \i{human} readers
\mdash{} developers and engineers.  The code-level and
human-level features promoting robustness will tend to overlap partially
but not completely, demanding a complex evaluation of where gatekeeping
code is needed and how to double-check via
unit tests and other post-implementation examinations.  This is the
kind of situation \mdash{} an impasse, or partial but incomplete overlap,
between formal and semi-formal specifications \mdash{} which many programmers
hope to avoid via strong type systems.
}
\p{Most programming language will provide some basic (typically relatively
coarse-grained) specification semantics, usually
through type systems and straightforward code observations
(like compiler warnings about unused or uninitialized variables).
For sake of discussion, assume that all languages have distinct
compile-time and run-time stages (though these may be opaque to
the codewriter).  We can therefore distinguish compile-time
tests/errors from run-time tests and errors/exceptions.
This permits us to formulate 
questions like: how
should code requirements be expressed?  How and to
what extent should requirements be tested by the language
engine itself \mdash{} and beyond that how can the language help coders implement
more sophisticated gatekeepers than the language natively offers?
What checks can and should be compile-time or run-time?  How
does \q{gatekeeping} integrate with the overall semantics and
syntax of a language?
}
\p{Given the maxim that
procedures should have single and narrow roles \mdash{} \q{separation 
of concerns} \mdash{} note that \i{validating} input
is actually a different role than \i{doing} calculations.  This is 
why procedures with fine requirements might be split into two: a
gatekeeper that validates input before a fragile procedure is called,
separate and apart from that procedure's own implementation.
A related idea is overloading fragile procedures: for example,
a function which takes one value can be overloaded in terms
of whether the value fits in some prespecified range.  These two
can be combined: gatekeepers can test inputs and call one of several
overloaded functions, based on which overload's specifications are
satisfied by the input.
}
\p{But despite their potential elegance, mainstream programming languages
do not supply much language-level support for expressing
groups of fine-grained functions along these lines.  Advanced 
type-theoretic constructs \mdash{} including Dependent Types,
typestate, and effect-systems \mdash{} model requirements with more precision
than can be achieved via conventional type systems alone.  Integrating these
paradigms into core-language type systems permits data validation 
to be integrated with general-purpose type checking, without the need for
static analyzers or other \q{third party} tools (that is, projects maintained
orthogonally to the actual language engineering; i.e., to
compiler and runtime implementations).  Unfortunately, 
these advanced type systems are also more complex to implement.  
If software language engineers aspire to make Dependent Types and 
similar advanced constructs part of their core language, 
creating compilers and runtime engines for these languages 
becomes proportionately more difficult.
} 
\p{If these observations are correct, I maintain that it is a worthwhile
endeavor to return to the theoretical drawing board, with the goal 
of improving programming language technology itself.  
Programming languages are, at one level, artificial 
\i{languages} \mdash{} they allow humans to communicate 
algorithms and procedures to computer processors, and 
to one another.  But programming languages are also 
themselves engineering artifacts.  It is a complex
project to transform textual source-code \mdash{} which is 
human-readable and looks a little bit like natural 
language \mdash{} into binary instructions that computers 
can execute.  For each language, there is a stack 
of tools \mdash{} parsers, compilers, and/or runtime libraries 
\mdash{} which enable source code to be executed 
according to the language specifications.  
Language design is therefore constrained by 
what is technically feasible for these supporting tools.  
Practical language design, then, is an interdisciplinary 
process which needs to consider both the dimension of 
programming languages as communicative media and 
as digital artifacts with their own engineering challenges 
and limitations.
}
%\spsubsection{Core Language vs. External Tools}
\subsection{Core Language vs. External Tools}
\p{Because of programming languages' engineering limitations, 
such as I just outlined, software projects should not 
necessarily rely on core-language features for 
responsible, safety-conscious programming.
Academic and experimental languages tend to have 
more advanced features, and to embody more 
cutting-edge language engineering, compared to mainstream 
programming languages.  However, it is not always feasible 
or desirable to implement important software with 
experimental, non-mainstream languages.  By their nature, 
such projects tend to produce code that must be understood by 
many different developers and must remain usable years into 
the future.  These requirements point toward 
well-established, mainstream languages \mdash{} and 
mainstream development techniques overall \mdash{} as opposed to 
unfamiliar and experimental methodologies, even if those 
methodologies have potential for safer, more productive 
coding in the future.   
}
\p{In short, methodologies for safety-conscious coding can be 
split between those which depend on core-language features, 
and those which rely on external, retroactive analysis 
of sensitive code.  On the one hand, some languages and projects
prioritize specifications that are intrinsic to the language and integrate
seamlessly and operationally into the language's foundational
compile-and-run sequence.  Improper code (relative to specifications)
should not compile, or, as a last resort, should fail gracefully at run-time.
Moreover, in terms of programmers' thought processes, the
description of specifications should be intellectually continuous
with other cognitive processes involved in composing code, such
as designing types or implementing algorithms.  For sake of 
discussion, I will call this paradigm \q{internalism}.  
}
\p{The \q{internalist} mindset seeks to integrate data 
validation seamlessly with other language features.  
Malformed data should be flagged via similar mechanisms 
as code which fails to type-check; and errors should 
be detected as early in the development process as possible.   
Such a mindset is evident in passages like this (describing
the Ivory programming language):
\begin{dquote}Ivory's type system is shallowly embedded within Haskell's
type system, taking advantage of the extensions provided by [the
Glasgow Haskell Compiler].  Thus, well-typed Ivory programs
are guaranteed to produce memory safe executables, \i{all without
writing a stand-alone type-checker} [my emphasis].  In contrast, 
the Ivory syntax is \i{deeply} embedded within Haskell.
This novel combination of shallowly-embedded types and 
deeply-embedded syntax permits ease of development without sacrificing
the ability to develop various back-ends and verification tools [such as]  
a theorem-prover back-end.  All these back-ends share the
same AST [Abstract Syntax Tree]: Ivory verifies what it compiles.
\cite[p. 1]{ivory}.
\end{dquote}   In other words, the creators of Ivory are promoting the
fact that their language buttresses via its type system 
\mdash{} and via a mathematical precision suitable for 
proof engines \mdash{} 
code guarantees that for most languages require external
analysis tools.
}
\p{Contrary to this \q{internalist} philosophy, other approaches
(perhaps I can call them \q{externalist}) favor a neater separation
of specification, declaration and testing from the core language,
and from basic-level coding activity.  In particular \mdash{} according to 
the \q{externalist} mind-set \mdash{} most of the more important or complex
safety-checking does not natively integrate with the
underlying language, but instead requires
either an external source code analyzer, or 
regulatory runtime libraries, or some combination of the two.  
Moreover, it is unrealistic
to expect all programming errors to be avoided with enough proactive planning,
expressive typing, and safety-focused paradigms: any complex
code base requires some retroactive design, some combination
of unit-testing and mechanisms (including those
third-party to both the language and the projects whose code is
implemented in the language) for externally
analyzing, observing, and higher-scale testing for the code,
plus post-deployment monitoring.
}
\p{As a counterpoint to the features cited as benefits to the
Ivory language, which I identified as representing the 
\q{internalist} paradigm, consider Santanu Paul's Source Code Algebra (\SCA{})
system described in \cite{SantanuPaul} and
\cite{GiladMishne}, \cite{TillyEtAl}:
\begin{dquote}Source code files are processed using
tools such as parsers, static analyzers, etc. and the necessary information
(according to the SCA data model) is stored in a repository.  A user interacts
with the system, in principle, through a variety of high-level languages, or
by specifying SCA expressions directly.  Queries are mapped to SCA expressions,
the SCA optimizer tries to simplify the expressions, and finally, the SCA
evaluator evaluates the expression and returns the results to the user.\nl{}
We expect that many source code queries will be expressed using high-level
query languages or invoked through graphical user interfaces.  High-level queries
in the appropriate form (e.g., graphical, command-line, relational, or
pattern-based) will be translated into equivalent SCA expressions.  An SCA
expression can then be evaluated using a standard SCA evaluator, which
will serve as a common query processing engine.  The analogy from
relational database systems is the translation of SQL to expressions based on
relational algebra. \cite[p. 15]{SantanuPaul}
\end{dquote}
So the \i{algebraic} representation of source code is favored
here because it makes computer code available
as a data structure that can be processed via \i{external}
technologies, like \q{high-level languages}, query languages, and
graphical tools.  The vision of an optimal development environment
guiding this kind of project is opposite, or at least
complementary, to a project like Ivory: the whole point
of Source Code Algebra is to pull code verification \mdash{} the
analysis of code to build trust in its safety and robustness
\mdash{} \i{outside} the language itself and into the surrounding
Development Environment ecosystem.
}
\p{These philosophical differences (what I dub \q{internalist} vs. \q{externalist}) 
are normative as well as descriptive:
they influence programming language design, and how languages in turn influence
coding practices.  One goal of language design is to produce languages 
which offer rigorous guarantees \mdash{} fine-tuning the languages' 
type system and compilation model to maximize the level of detail 
guaranteed for any code which type-checks and compiles.  
Another goal of language design is to define syntax and 
semantics permitting valid source code to be analyzed 
as a data structure in its own right.  Ideally, 
languages can aspire to both goals.  In practice, however, 
achieving both equally can be technically difficult.  
The internal representations conducive to strong type and 
compiler guarantees are not necessarily amenable to 
convenient source-level analysis, and vice-versa.    
}
\p{Language engineers, then, have to work with
two rather different constituencies.  One community of
programmers tends to prefer that specification and validation be
integral to/integrated with the language's type system and
compile-run cycle (and standard runtime environment); whereas
a different community prefers to treat code evaluation
as a distinct part of the development process, something logically, operationally,
and cognitively separate from hand-to-screen codewriting
(and may chafe at languages restricting certain code constructs
because they can theoretically produce coding errors, even when
the anomalies involved are trivial enough to be tractable for
even barely adequate code review).  One challenge for language engineers is
accordingly to serve both communities.  We can, for example, aspire to
implement type systems which are sufficiently
expressive to model many specification, validation, and
gatekeeping scenarios, while also anticipating that language code
should be syntactically and semantic designed to be
useful in the context of external tools (like
static analyzers) and models (like Source Code
Algebras and Source Code Ontologies).
}
\p{The techniques I discuss here work toward these goals on two levels.  First, I
propose a general-purpose representation of computer code in terms
of Directed Hypergraphs, sufficiently rigorous to codify a
theory of functional types as types whose values are (potentially) initialized from
formal representations of source code \mdash{} which is to say, in the present
context, code graphs.  Next, I
analyze different kinds of \q{lambda abstraction} \mdash{} the idea of
converting closed expressions to open-ended formulae by asserting that
some symbols are \q{input parameters} rather than fixed values, as in
Lambda Calculus \mdash{} from the perspective of
axioms regulating
how inputs and outputs may be passed to and obtained from
computational procedures.  I bridge these topics \mdash{} Hypergraphs
and Generalized Lambda Calculi \mdash{} by taking abstraction as a
feature of code graphs wherein some hypernodes are singled out
as procedural
\q{inputs} or \q{outputs}.  The basic form of this model
\mdash{} combining what are essentially two otherwise unrelated
mathematical formations, Directed Hypergraphs and
(typed) Lambda Calculus \mdash{} is laid out in
Sections \sectsym{}\hyperref[sTwo]{\ref{sTwo}}
and \sectsym{}\hyperref[sThree]{\ref{sThree}}.
}
\p{Following that sketch-out, I engage a more rigorous study of
code-graph hypernodes as \q{carriers} of runtime values, some of
which collectively form \q{channels} concerning values which
vary at runtime between different executions of a function body.
Carriers and channels piece together to form
\q{Channel Groups} that describe structures with meaning both
within source code as an organized system (at \q{compile time}
and during static code analysis) and at runtime.  Channel Groups
have four different semantic interpretations, varying via the
distinctions between runtime and compile-time and between
\i{expressions} and (function) \i{signatures}.
I use the framework of Channel Groups to identify
design patterns that achieve many goals of
\q{expressive} type systems while being implementationally
feasible given the constraints of mainstream programming
languages and compilers.
}
