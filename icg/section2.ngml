`section.Linguistic Structure the Accretion of Detail`
`p.
As I see it, the primary task for linguistic analysis 
is to document how linguistic constructions 
signify propositional content.  The centrality of 
`i.cognitive` linguistics, and cognitive grammar, 
derives from observing how constructions do 
not in the general case just `q.transparently` or 
`q.passively` connote predicate structures, 
the way that `AdjN; patterns mechanically 
call up predicate-subject predications.  
Instead of recapitulating predicate structure 
directly, language structure reflects 
propositional or `i.interpretive` attitudes, 
narrative, causative, or integrative 
interpretations which convey how 
speakers track and convey situations.  
In short, rather than seeing linguistic 
structure as a passive vehicle for 
rendering logically-ordered ideas, 
we should treat linguistic performances 
as dynamic processes which `i.build up 
to` propositional contents, manifesting 
evolutive principles which (often 
subconsciously) depend on speakers' 
situational immersion as well as 
abstractic syntactic and semantic conventions.   
`p`

`p.
This perspective summarizes my thematic or 
philosophical motivation for the specific 
formalizing approach I take here 
with respect to Cognitive Grammar.  
My goal, as cited earlier, is to develop 
formal models which would be 
recognized as structural variations on 
popular representations in 
formal linguistics %-- dependency grammar, 
constituency grammar, categorial 
grammar, type-theoretic semantics 
%-- while also being faithful to 
the cognitive-grammatic perspective.  
In particular, the structural parameters 
exposed by formal models %-- e.g., 
phrase boundaries or inter-word 
relations %-- should as much as possible have cognitive-grammatic 
interpretations, referring to central notions such as 
cognitive schema, conceptualization, 
and landmark/trajector configurations. 
`p`

`p.
Aside from philosophy, I also have more mundane 
considerations: to properly situate 
cognitive grammar in an overall linguistic 
contexts, it helps to have an intermediate perspective 
which allows the contrasting priorities, methodologies, 
or scientific commitments of cognitive grammar 
alongside other linguistic `q.schools` to be 
assessed together.  One important paradigmatic 
area of contention, I think, is that cognitive 
grammar appears to de-emphasize the logical 
substratum of semantics which, on other 
perspectives, is the central artery of 
language itself.  There is a nontrivial core 
of language within which propositional content 
is encoded in linguistic constructions 
following entrenched rules.  Within `i.this core`/, 
language acts like a sort of `q.meta-logical` 
system: not something logically structured 
%-- in the formal sense of predicate 
(plus maybe modal, temporal, epistemic, etc.) logic 
%-- in itself, but a system whose structural 
principles can be shown to communicate 
logical structures under suitable 
substitutions and transformations.  
Cognitive grammar should indeed accommodate 
these cases; but I think they are neither 
predominant nor prototypical in language 
as a whole, so they should not be reified as a 
the canonical filter for distinguishing semantics 
proper from pragmatics (or from theories 
of extra-linguistic reasoning).   
`p`

`p.
This implies that while linguistic constructions 
encode propositional content, we need a 
detailed (and cognitively `q.active`/) 
theory of how, in formulating and understanding 
linguistic performances, we map constructions to 
their correlative propostions.  
Constructions are not in the general case 
direct `q.wrappers for a given propositional 
arrangement %-- like `AdjN; for predicate-subject; 
instead we have to analyze the internal structure 
of constructions to elucidate how they contribute 
to an overarching communication for rational content.    
`p`

`p.
This interpretation for the rationale behind 
constructional analysis moreover helps 
us posit a cognitive grounding for 
our choice of linguistic representation.  
The frameworks through which language structures 
are described are typically oriented around 
grammatic (and by extension semantic) rules 
%-- insofar as one takes phrases (a mid-level 
scale intermediate between words and sentences) 
as a structural primitive, for example, then 
phrase-descriptions serve the representational 
purpose of delineating what makes valid 
phrases well-formed.  Conversely, if 
we take inter-word relations as the focal 
structural primitive, sentences are well-formed 
if they have a coherent collection of 
such relationships.  More to the point, 
we want to explain `i.why` sentences are 
(felt as) well-formed.  With a focus on 
phrase-structure, we can explain well-formdeness 
in terms of how phrases are nested together.  For 
example, we can say that subphrases have some
singular synopsis which allows them to play some 
expected structural role in a larger phrase 
%-- a verb-phrase standing in for a verb, say.  
Or, in terms of inter-word relations, we 
can identify the structure within a sentence such 
that each word is integrated into the whole 
%-- i.e., each word has relations to some other 
word, with the overall chain of connections spanning the full 
sentence %-- and moreover the sentence 
possesses specific relations (such as a verb to a subject) 
marking that the sentence conveys a complete idea. 
`p`

`p.
Thematically, a linguistic representation is, 
in effect, both a theoretical posit and a 
presentational or pedagogical device illustrating 
a theory.  By `q.linguistic representation` I 
mean specifically, in this context, a 
digram or otherwise annotated rendering of a sentence 
which permits explanatory elements to be 
interspersed among the words, such as 
lines or curves connecting or encircling 
groups of words, as well as testual notations 
such as Part of Speech labels.  The point 
of these visual displays is to convey, 
in effect, a `i.data structure` which supplements 
to original sentence with extra 
theoretically-motivated content, such as 
Parts of Speech associated with individual 
words, relational labels associated with 
word-pairs, and set-constructions on words 
representing phrases.  Such a data structure 
summarizes an account of why the sentence 
is valid, and carries the meaning it does, 
in terms of a grammar and semantics whose 
principles are laid out by the relevant theory: 
so the presence of a labeled word-pair, 
in the context of dependency grammar, 
embodies a theoretical commitment to 
some collection of interword relation-types 
as the backbone of grammar.  That is, 
the theoretical explanation for why grammar 
works as it does in some particular 
sentence can be summarized by visually 
restaging the sentence as a connected 
network %-- that is, a `i.labeled graph`/, 
with Part of Speech labels on words 
and/or `q.dependency` labels (in the sense 
of `q.Universal Depencies` for 
multi-lingual dependency grammar) on 
graph-edges.
`p`

`p.
Such diagrammatic presentations are therefore at 
one level proxies for lengthier textual analyses 
of a sentence's grammatic operation, as some 
theory sees it.  However, in addition to 
this expository role, interword or phrasal 
data structures are `i.posits` of the theory in 
the sense that for each sentence there is 
deemed to be at least one (typically one optimal) 
data structure which captures the sentence's 
functional propriety; how it functions as a 
correct embodiment of the language system. 
To the extent that a sentence has one 
clear meaning, then a systematic linguistic 
theory will generally hold that there is 
one clear mechanism `i.through which` 
the sentence has that meaning.  This 
is not to deny that people may process the 
same sentence a little differently, 
but %-- excluding cases of actual ambiguity or 
obfuscation %-- each person's 
conceptualization of a sentence will be 
similar enough that differences in 
reception (at least among competent speakers) 
can be ignored.
`p`

`p.
In that case, we can not only posit that 
the meaning of `i.the neighbors dogs were 
barking` (spoken in a `q.normal` context) 
is necessarily to report on the vocalizations 
of some canines, but we are permitted to 
assume a necessary marshaling of 
syntactic and semantic processing to 
explain `i.why` every competent speaker 
would read that propositional content into 
that sentence.  So for each sentence there is a 
canonical processing, a structural or processual 
organization which inheres in the sentence as an 
objective feature of how it functions as a 
valid exercise of the language system, no less 
intrinsic than its meaning.  The 
`q.data structures` which annotate sentence are 
implicitly, then, I would argue, targeting 
this processual reification: they are a way of 
putting theoretical flesh on the underlying 
hypothesis that sentences acquire their meaning 
through processually conventionalized routes.  
`p`

`p.
For sake of discussion, I will refer to any 
schematic reconstruction of a sentence 
(or potentially a larger or smaller linguistic 
unit) as a `q.processual data structure`/, 
meaning some formal model representing 
the process through which language 
artifacts acquire their meaning (which in 
turn can be associated with some propositional 
content).  By `i.data structure` I mean 
that for a given sentence we can posit 
a larger structure composed of explanatory 
parameters which document how a given 
syntactic or semantic principle is manifest 
in a given word's, interword-relation's, or 
phrase's functioning within the sentence.  
Alongside phrases, relations, and parts of 
speech %-- that is, these being elements 
of processual data structures which 
attach to words individually or collectively 
%-- parameters can include morphological 
`q.tags` asserted on words (indicating that 
the word is presented in a manner signifying 
case or tense, say); links between words and 
a lexicon; and indications of details such as vocal stress 
patterns, disfluency, intonation, and other performative givens 
which can affect a sentence's meaning even if they are 
not normally treated as part of syntax or semantics.
`p`

`p.
Different theories will recognize a different inventory 
of relevant parameters;   
`p`

`p.

`p`

