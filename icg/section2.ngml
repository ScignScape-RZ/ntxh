`section.Linguistic Structure the Accretion of Detail`
`p.
As I see it, the primary task for linguistic analysis 
is to document how linguistic constructions 
signify propositional content.  The centrality of 
`i.cognitive` linguistics, and cognitive grammar, 
derives from observing how constructions do 
not in the general case just `q.transparently` or 
`q.passively` connote predicate structures, 
the way that `AdjN; patterns mechanically 
call up predicate-subject predications.  
Instead of recapitulating predicate structure 
directly, language structure reflects 
propositional or `i.interpretive` attitudes, 
narrative, causative, or integrative 
interpretations which convey how 
speakers track and convey situations.  
In short, rather than seeing linguistic 
structure as a passive vehicle for 
rendering logically-ordered ideas, 
we should treat linguistic performances 
as dynamic processes which `i.build up 
to` propositional contents, manifesting 
evolutive principles which (often 
subconsciously) depend on speakers' 
situational immersion as well as 
abstractic syntactic and semantic conventions.   
`p`

`p.
This perspective summarizes my thematic or 
philosophical motivation for the specific 
formalizing approach I take here 
with respect to Cognitive Grammar.  
My goal, as cited earlier, is to develop 
formal models which would be 
recognized as structural variations on 
popular representations in 
formal linguistics %-- dependency grammar, 
constituency grammar, categorial 
grammar, type-theoretic semantics 
%-- while also being faithful to 
the cognitive-grammatic perspective.  
In particular, the structural parameters 
exposed by formal models %-- e.g., 
phrase boundaries or inter-word 
relations %-- should as much as possible have cognitive-grammatic 
interpretations, referring to central notions such as 
cognitive schema, conceptualization, 
and landmark/trajector configurations. 
`p`

`p.
Aside from philosophy, I also have more mundane 
considerations: to properly situate 
cognitive grammar in an overall linguistic 
contexts, it helps to have an intermediate perspective 
which allows the contrasting priorities, methodologies, 
or scientific commitments of cognitive grammar 
alongside other linguistic `q.schools` to be 
assessed together.  One important paradigmatic 
area of contention, I think, is that cognitive 
grammar appears to de-emphasize the logical 
substratum of semantics which, on other 
perspectives, is the central artery of 
language itself.  There is a nontrivial core 
of language within which propositional content 
is encoded in linguistic constructions 
following entrenched rules.  Within `i.this core`/, 
language acts like a sort of `q.meta-logical` 
system: not something logically structured 
%-- in the formal sense of predicate 
(plus maybe modal, temporal, epistemic, etc.) logic 
%-- in itself, but a system whose structural 
principles can be shown to communicate 
logical structures under suitable 
substitutions and transformations.  
Cognitive grammar should indeed accommodate 
these cases; but I think they are neither 
predominant nor prototypical in language 
as a whole, so they should not be reified as a 
the canonical filter for distinguishing semantics 
proper from pragmatics (or from theories 
of extra-linguistic reasoning).   
`p`

`p.
This implies that while linguistic constructions 
encode propositional content, we need a 
detailed (and cognitively `q.active`/) 
theory of how, in formulating and understanding 
linguistic performances, we map constructions to 
their correlative propostions.  
Constructions are not in the general case 
direct `q.wrappers for a given propositional 
arrangement %-- like `AdjN; for predicate-subject; 
instead we have to analyze the internal structure 
of constructions to elucidate how they contribute 
to an overarching communication for rational content.    
`p`

`p.
This interpretation for the rationale behind 
constructional analysis moreover helps 
us posit a cognitive grounding for 
our choice of linguistic representation.  
The frameworks through which language structures 
are described are typically oriented around 
grammatic (and by extension semantic) rules 
%-- insofar as one takes phrases (a mid-level 
scale intermediate between words and sentences) 
as a structural primitive, for example, then 
phrase-descriptions serve the representational 
purpose of delineating what makes valid 
phrases well-formed.  Conversely, if 
we take inter-word relations as the focal 
structural primitive, sentences are well-formed 
if they have a coherent collection of 
such relationships.  More to the point, 
we want to explain `i.why` sentences are 
(felt as) well-formed.  With a focus on 
phrase-structure, we can explain well-formdeness 
in terms of how phrases are nested together.  For 
example, we can say that subphrases have some
singular synopsis which allows them to play some 
expected structural role in a larger phrase 
%-- a verb-phrase standing in for a verb, say.  
Or, in terms of inter-word relations, we 
can identify the structure within a sentence such 
that each word is integrated into the whole 
%-- i.e., each word has relations to some other 
word, with the overall chain of connections spanning the full 
sentence %-- and moreover the sentence 
possesses specific relations (such as a verb to a subject) 
marking that the sentence conveys a complete idea. 
`p`

`p.
Thematically, a linguistic representation is, 
in effect, both a theoretical posit and a 
presentational or pedagogical device illustrating 
a theory.  By `q.linguistic representation` I 
mean specifically, in this context, a 
digram or otherwise annotated rendering of a sentence 
which permits explanatory elements to be 
interspersed among the words, such as 
lines or curves connecting or encircling 
groups of words, as well as testual notations 
such as Part of Speech labels.  The point 
of these visual displays is to convey, 
in effect, a `i.data structure` which supplements 
to original sentence with extra 
theoretically-motivated content, such as 
Parts of Speech associated with individual 
words, relational labels associated with 
word-pairs, and set-constructions on words 
representing phrases.  Such a data structure 
summarizes an account of why the sentence 
is valid, and carries the meaning it does, 
in terms of a grammar and semantics whose 
principles are laid out by the relevant theory: 
so the presence of a labeled word-pair, 
in the context of dependency grammar, 
embodies a theoretical commitment to 
some collection of interword relation-types 
as the backbone of grammar.  That is, 
the theoretical explanation for why grammar 
works as it does in some particular 
sentence can be summarized by visually 
restaging the sentence as a connected 
network %-- that is, a `i.labeled graph`/, 
with Part of Speech labels on words 
and/or `q.dependency` labels (in the sense 
of `q.Universal Depencies` for 
multi-lingual dependency grammar) on 
graph-edges.
`p`

`p.
Such diagrammatic presentations are therefore at 
one level proxies for lengthier textual analyses 
of a sentence's grammatic operation, as some 
theory sees it.  However, in addition to 
this expository role, interword or phrasal 
data structures are `i.posits` of the theory in 
the sense that for each sentence there is 
deemed to be at least one (typically one optimal) 
data structure which captures the sentence's 
functional propriety; how it functions as a 
correct embodiment of the language system. 
To the extent that a sentence has one 
clear meaning, then a systematic linguistic 
theory will generally hold that there is 
one clear mechanism `i.through which` 
the sentence has that meaning.  This 
is not to deny that people may process the 
same sentence a little differently, 
but %-- excluding cases of actual ambiguity or 
obfuscation %-- each person's 
conceptualization of a sentence will be 
similar enough that differences in 
reception (at least among competent speakers) 
can be ignored.
`p`

`p.
In that case, we can not only posit that 
the meaning of `i.the neighbors dogs were 
barking` (spoken in a `q.normal` context) 
is necessarily to report on the vocalizations 
of some canines, but we are permitted to 
assume a necessary marshaling of 
syntactic and semantic processing to 
explain `i.why` every competent speaker 
would read that propositional content into 
that sentence.  So for each sentence there is a 
canonical processing, a structural or processual 
organization which inheres in the sentence as an 
objective feature of how it functions as a 
valid exercise of the language system, no less 
intrinsic than its meaning.  The 
`q.data structures` which annotate sentence are 
implicitly, then, I would argue, targeting 
this processual reification: they are a way of 
putting theoretical flesh on the underlying 
hypothesis that sentences acquire their meaning 
through processually conventionalized routes.  
`p`

`p.
For sake of discussion, I will refer to any 
schematic reconstruction of a sentence 
(or potentially a larger or smaller linguistic 
unit) as a `q.processual data structure`/, 
meaning some formal model representing 
the process through which language 
artifacts acquire their meaning (which in 
turn can be associated with some propositional 
content).  By `i.data structure` I mean 
that for a given sentence we can posit 
a larger structure composed of explanatory 
parameters which document how a given 
syntactic or semantic principle is manifest 
in a given word's, interword-relation's, or 
phrase's functioning within the sentence.  
Alongside phrases, relations, and parts of 
speech %-- that is, these being elements 
of processual data structures which 
attach to words individually or collectively 
%-- parameters can include morphological 
`q.tags` asserted on words (indicating that 
the word is presented in a manner signifying 
case or tense, say); links between words and 
a lexicon; and indications of details such as vocal stress 
patterns, disfluency, intonation, and other performative givens 
which can affect a sentence's meaning even if they are 
not normally treated as part of syntax or semantics.
`p`

`p.
Different theories will recognize a different inventory 
of relevant parameters (although a notation may refer 
in ad-hoc ways to details which other theories would represent 
more formally; e.g., words may be boldfaced or capitalized 
to indicate vocal emphasis, even if the relevant theory 
does not usually consider speech patterns).  We can 
therefore adopt a rather abstract, metatheoretical 
perspective by considering which theoretical 
parameters are internally utilized by which theories.  
For their central analysis, for example, theories 
may include only phrase-structures, only interword relations, 
or both; they may include only words as the 
canonical lexical elements or admit certain smaller-scale 
units, like `q.'s` for possessives; they may or may not 
pair stuctural re-presentations of sentences with 
structurally annotated lexicons, where for example an 
identifier code associated with a word in (what I am 
calling) a processual data structure matches the 
word to a lexical entry.  Representational paradigms 
can also differ in how they handle (if at all) 
various communicative units which are not usually 
lexified, such as proper-name acronyms, punctuation, 
interwoven dialog (where a pause or interruption 
at a certain point may be deemed semiotically 
relevant), or speech effects in general.     
`p`

`p.
As I have already suggested, my goal here is to 
propose a formal model which is appropriate 
for cognitive grammar %-- mostly preserving 
the philosophical commitments of key figures 
such as Langacker and Talmy (at least as a goal) 
and yielding representations which help 
to document cognitive activity sited in linguistic 
formations.  Ideally, insofar as cognitive grammar 
would give a particular treatment for a sentence 
%-- focusing on a landmark/trajector configuration, 
let's say %-- the `q.processual data structure` 
used to encapsulate such analysis would 
comprise parameters which thematically notate 
components in the cognitive-grammatic treatment 
and diagram how they relate to particular 
words (or word-pairs or phrases).  
`p`

`p.
Minimally, my proposed repesentations are 
derived from Dependency Grammar in that 
interword relations are taken as the crucial 
theoretical construct (more so than phrases).  
As a result, the `q.processual data structure` 
associated with any sentence can be seen 
as a directed, labeled graph, or 
`q.parse graph`/, with words represented as 
graph nodes.  (I will generally 
argue against isolating non-word lexical 
elements, so e.g. a possessive like 
`i.Warren's` is treated as one node, not two).  
Nodes are then labeled with grammatical 
categories, and edges between nodes labeled 
with notations suggesting the kind 
of relationship obtaining between the incident nodes. 
`p`

`p.
My system of node and edge labels is also influenced 
by categorial grammar; in particular, I adopt the conventional 
that certain grammatic categories are derived from 
other categories.  Insofar as words from one category 
are used in conjunction with a word from another 
category, resulting in a phrase or transoformation 
assignable to a third category (potentially the 
same as the second), the first category is 
derivative on the latter two.  The adjective 
category is derivative on `i.noun` because 
adjectives are modifiers which trigger some 
reconceptualizing of a noun-idea, resulting in 
a new, or at least altered, noun (technically 
perhaps a noun phrase but conceptually a noun).  
Or, consider a preposition like `i.toward`/, 
which combines with a noun to produce a 
form of adverb (a locative designation which 
modifies a verb, as in `i.walk toward the store`/).  
Here the categorial ascription for 
`i.toward` is derived from the `i.noun` and `i.adverb` 
(in a fixed order: `i.toward` conceptually 
transforms a noun `i.to` an adverb).  
`p`

`p.
In general, for a pair of grammatic categories 
(not necessarily distinct) there is a potential 
additional category profiling conceptual 
transitions which produce an instance of 
the second category in the presence of the 
first.  To use more mathematical language, 
the derived category profiles transformations 
which `i.input` the first category and 
`i.output` the second.  This construction is 
similar to the maxim in mathematical type theory 
that functions between two types constitute a 
third type, or in computational type theory 
that procedures with respective tuples of 
input and output types dervice a further type. 
`p`

`p.
A common framework holds that the essential 
linguistic types %-- or grammatic categories 
%-- are `i.nouns` and `i.propositions`/; 
I use the latter term for sentences as well 
as clauses, contained within sentences, that 
embody a complete propositional idea.  All further 
categories can then be derived as `q.transformations` 
that take inputs and produce outputs from these 
core categories, or prior derived categories 
recursively.  In particular, verbs profile 
transformations that produce propositions from nouns.  
In Categorial Combinary Grammar, a verb can 
be characterized via notation like `NSlashP;, 
meaning that a verb (phrase) combines with a noun (phrase)  
`i.preceding` the verb to yield a proposition, e.g., 
a complete sentence: `i.the dogs barked`/.  
This representation would also distinguish combinations 
by word order (the modifier preceding or following 
the modified, or `q.target`/; the two forms of 
sequence marked by using either back or forward 
slashes).  Here I abstract from word-order and use 
an arrow, borrowing from type theory, to represent 
either back or forward slash: so the idea that a 
verb expects a noun to form a proposition would be 
written `VNounToProp; (this notation will be further 
clarified below).    
`p`

`p.
Here I will sketch a framework that combines the 
basic intuitions of categorial and dependency 
grammars: a word whose categorization belongs 
to a derived category serves to transform or 
modify some `q.target` word; this in turn can 
be modeled as a relation `i.between` the modifier and 
the target, and the `q.graph` of these relations 
forms the core of a parse graph.  At the 
same time, I will suggest conceptual interpretation 
of relationships; I do not picture formal models 
as merely observing grammatic `q.behaviors` 
(cf. Langacker Intro p. 93).  The idea that 
adjectives transform nouns into `q.other nouns` 
can be given a strictly behavioral gloss by 
reading it as a structural condition: when occurring 
in well-formed sentences, an adjective links to a noun 
(or noun-phrase) to produce a noun-phrase that substitutes for 
a nominal lexeme in larger phrases.  So for any adjective 
we can `i.find` the noun it modifies and find how the 
resulting phrase is used as a noun in some larger phrase 
%-- the fact that this `q.search` resolves successfully 
is mandated by sentence validity, and failure to 
find the requiste context for the adjective 
is a signal that the enclosing sentence is not, 
in fact, well-formed.   
`p`

`p.
In contrast to a purely behavioral description, however, 
we can interpret these same phenomena conceptually.  
Once we encounter a lexeme which appears adjectival, 
we `i.conceptually` expect to find a noun which it 
modifies.  We cannot conceive `i.black`/, say, 
apart from some object whose surface or appearance 
is black.  Assuming that a sentence originates from a 
speaker's rational and honestly-portrayed interaction 
within a situation, the speaker would have no reason 
to enunciate `i.black` without cognizing the color, 
and therefore cognizing a predicated object.  
Likewise for any other adjective; in this 
sense adjectives create conceptual expectations that 
window onto speakers' rational construals of situations.  
Sentences fulfill expectations by presenting words 
or phrases that complete ideas that seem to have been 
left open by the previously spoken (or written) sentence-fragment.  
`p`

`p.
Sometimes expectations refer backward, as often in 
personal pronouns (`i.I invited John, but he's on vacation`/) 
%-- we expect `i.he` or `i.she` to proxy a referent priorly 
established in the discourse, maybe the sentence.  
In the more general case though they refer forward, 
so we can say that the presence of lexemes from 
`q.derivative` grammatic categories creates an 
expection of a conceptual completion that is then 
satisfied by a subsequently occurring word or phrase.  
Sentences are grammatically correct, then, insofar 
as they `i.do` resolve their midstream expectations.  
Grammatical behavior %-- e.g. that adjectives must be 
followed by nouns %-- is therefore not forced 
to be a theoretical primitive, but instead can be 
analyzed as the conventionalization of conceptual 
expectations and resolutions.  Language, on this 
perspective, reveals certain recurring patterns on 
how expectations are created and then fulfilled; 
when sufficiently entrenched as well-formedness 
criteria, these patterns `i.become` grammatical behavior.  
`p`

`p.
When expectations are left unresolved, we may not 
necessarily experience the results as conceptually 
lacking: hearing someone say only `i.there is a black` 
(with no `q.modifiand`/) probably does not result in out 
feeling a sense of `i.conceptual` incompleteness.  
Rather, we assume that the sentence has been 
interrupted, or that on the face it is grammatically 
wrong (for whatever reason: maybe the speaker is  
incompetent, or was interrupted, or is still deciding 
what to say next).  The fact that we experience 
mismatched expectations as violations of syntactic 
rules, rather than conceptually anomalies, may cloak 
the fact that these expectations are intrinsically 
conceptual.  However, this comports with my theory of 
the origin of grammatic intuitions: we are used 
to the flow of expectations to accord with the 
unfolding of grammar.  We `i.become` used to this 
alignment the more that we immerse ourselves 
in language; as a result, we come to subconsciously 
register the posing and resolving of expectations, 
being more explicitly aware of the linguistic 
materialization of these patterns than their 
conceptual substrate.  Once the agreement between 
the smooth creation and dissolving of expectations' 
incompleteness is disrupted, we `i.experience` this 
defect as an anomaly in linguistic form.  
`p`

`p.
Via such a theory, I hypothesize then that grammatic 
rules are emergent phenomena whose origins lie in 
conceptual expectations, and specifically in patterns 
such that expectations are raised and then satisfied.  
Once these patterns become entrenched 
%-- in particular, via grammatic categories %-- they 
can nevertheless be given a formal treatment.  
I will now examine the specific formal model introduced 
here.   
`p`

`subsection.`
`p.
I will refer to the framework proposed here as 
`q.Cogitive Transform Grammar`/.  On this theory, 
the full grammatic interpretation of a sentence 
can be given, at least in germinal form, through 
relationships between two words.  The 
grammatic pattern, in turn, emerges from cognitive 
relations enacted between paired words.  
In particular, one word is a `i.modifier` and a 
second is a `i.ground` or `i.target`/.  The modifier's 
effect is to reconceive or complete the concept 
(or conceptual nexus) already established for the target.  
`p`

`p.
I adopt certain representational conventions which are, 
as much as possible, motivated by cognitive tendencies 
rather than being formally arbitrary.  Nevertheless, as a 
formal systems, representations in general will sometimes 
have rules driven by a desire to reduce data structures' 
parameters, rather than by deep theoretical reasoning.  
For example, I will generally avoid including phrases 
or any other `q.above-word-scale` elements in parse graphs.  
It would certainly be possible to iconify both words and 
phrases via graph nodes, with one form of inter-node 
relation being the inclusion of a word in a phrase.  
I choose however the leave phrase-hierarches implicit, 
partly for the theoretical reason that phrases are 
(I believe) `i.conceptually` derived from the accumulation 
of conceptual refinements via `q.cognitive transforms`/, 
so I'd like to preserve this intuition in the notation.
`p`

`p.
In a case like `i.toward the store`/, which 
(as suggested earlier) I read as `q.transforming` a noun 
to an adjective, the ground of this transform evidently 
a phrase, `i.the store`/.  I will use `q.ground` 
informally for a concept altered by a modifier, whether 
expressed in a word or phrase.  However, formally speaking 
I represent `q.transforms` as graphs only between words.  
Note that `i.the store` itself represents a transformation: 
the determinant `q.the` serves to clothe the generic 
concept `i.store` into a more context-specific referent.  
The concept then modified by `i.toward` is 
`i.store` only `i.after` it is `q.acted upon` by the 
effect of the `q.the`/-modification.  
Rather than picturing this as a relation between a modifier 
and a phrasal ground, we can also see this situation as a 
`q.chain` of modifications, one presupposing another.      
`p`

`p.
With the analogy of `q.functional` types in mathematics 
or computers, we can see intermediate modifications 
as having an `i.input` and an `i.output`/.  Here, 
`i.the` `q.inputs` a noun (`i.store`/) and `q.outputs` 
an altered version of that concept; in particular, one 
now cognized under the aegis of a singular determinant.  
The output of `i.the` is then an input for 
`i.toward`/, which adds its own alteration to the 
evolving `q.store` concept.  We can accordingly trace 
this chain as a series of steps between individual words, 
each step representing how one word modifies a concept 
priorly modified by its predecessor in the chain.  
This kind of picture helps motivate the idea of modeling 
`q.cognitive transforms` wholly via inter-word relations.  
The graph structure in the case of `i.toward the store` 
is simple, in that the transform evince a linear chain 
backward through the sentence: `StoreTheChain;.  
The arrows in this `q.graph` (subgraph, technically) 
point from the target to the modifier: `i.the` is 
a modifier for `i.store`/, but a target for 
`i.toward`/.  This indicates that `i.the` stands in for a 
transform whose `i.output` becomes `i.toward`/'s `i.input`/.  
`p`

`p.
Carrying these ideas over to a formal model, phrases are 
excluded from direct representation because it may be tacitly 
understood with any intermediate target-modifier pairing 
%-- `q.intermediate` in the sense that the target is 
also a modifier for a different target %-- that the 
`q.arrow` direction represents not just the single target 
word as a `i.ground` for modification, but the collection 
of prior transforms forming a chain leading `i.to` that 
target.  For example, `i.the` is target for `i.toward`/, 
but conceptually this means that the accumulated concept 
resulting from `i.the`/'s own modification of its own 
ground is assumed to be carried along in the 
`i.the`/`i.toward` interaction (notationally, the 
`i.the`/`i.toward` arrow, or directed graph-edge).     
`p`

`p.
The `i.processual data structures` I propose for modeling 
Cognitive Transform Grammar are a variety of hypergraph 
built around edges representing cognitive transforms, 
via modifier/target pairs.  The model stipulates that 
each word can be the target for at most one modifier.  
This means that one can form a chain following from one target 
to another, uniquely, and I stipulate that for any single 
sentence such chains will always lead to a single 
`q.root` word.  At this level of description the parse 
graphs are therefore `i.trees`/.  I will however introduce 
certain additional structures through which parcse-graphs 
actually become a form of hypegraph, albeit with acyclic 
directed edges, that is, with a kernel tree-like structure. 
`p`

`p.
I also stipulate that all the edges in a parse-graph 
%-- all the cognitive transforms, representationally 
%-- can be given a specific ordering.  
At least locally, ordering has a theoretical 
basis as can be seen from the `i.toward the store` 
case: the `i.toward` transform presupposes the one 
for `i.the` in the sense that the latter precedes 
it in the output-to-input chain.  The rationale 
for an `i.overall` ordering, across an entire 
sentence, is more complex, but will hopefully 
emerge in my following discussion. 
`p`

`p.
While each word may have at most one modifier, a modifier 
may transform multiple targets.  Consider 
`i.The dogs were barking very loudly`/.  I will 
present an analysis by considering word-pairs 
(in this sentence, but not necessarily in general, adjacent 
`visavis; sentence order).  Here `i.were barking` 
establishes a past progressive tense: I read this as a 
transform which yields a verb (i.e., a conceptual 
outcome which profiles an event or process according to 
the overall ideational desiderata for verbs, for example 
in Lang Intro. p. 108).  More precisely, `i.barking` is 
(I claim) conceptually adjectival, but by blending this concept 
with a temporal reference frame, we employ a conventionalized 
modification which refocuses the original concept toward 
a profiling of something bearing the concept as an 
act on that thing's part %-- cf. `i.I am hungry for` as a 
less ornate vernacular replacing `i.I hunger for`/.  
When the adjective is a gerund %-- therefore morphologically 
based on a verb %-- the transform `i.to` the verb 
category uncovers the original verb, but with added 
temporal specification.  There is more to say about 
this treatment of progressives, but I will forestall 
the analysis for now.    
`p`

`p.
Moving on within `barking very loudly`/, the 
transform relationship between 
`i.very` and `i.loudly` seems straightforwardly 
adverbial: the modification alters an adverb, 
here adding a measure of incremental emphasis 
(a contrary `q.decremental` transform would be 
hedges like `i.rather` or `i.a little`/).  
Also `i.the` modifies `i.dogs` via singular determinant, 
reiterating my sketch for `i.the store`/.  These, then, 
are basic word-pairs in `i.The dogs were barking very loudly`/.  
Next, `i.very loudly `q.outputs` an adverb 
(it also `q.inputs`/) one, which means the outcome of 
that transformation needs to link with a verb, to become 
its ground %-- specifically, here, the verb concept 
resulting from `i.were barking`/.  So the outcome of 
one transform becomes a modifier for the outcome of 
a second.   
`p`

`p.
Following the principle that targets link in chains 
%-- an edge between one modifier-word and a second 
meaning that the `q.output` of the former's transform 
becomes `q.input` to the latter %-- we here have 
the structure that `i.were`/, which is the modifier 
in `i.were barking`/, is then the target for 
`i.very`/, which is the modifier in `i.very loud`/.  
(This may feel counterintuitive in that 
`q.were` seems like an auxiliary word, so really 
what is being modified here is `i.barking`/: 
semiotically, the key communication is presumably 
`i.barking loundy`/; as I will discuss below, the 
transform structure linking sentences, on this 
model, does not necessarily foreground lexemes 
which are the most semantically important).  
I delay considerations of sentences' overall meaning, 
according to which some ideas may emerge as more 
central than others; here I want to sketch the 
series of transforms through which the overall meaning 
arises.  In this spirit `i.very` links to `i.were`/, 
as modifier-to-target, because of how the transforms 
are ordered: the kind of transform signified by an 
adjective logically depends on a verb, so that 
`i.were`/, whose ouput supplies that verb, 
precedes `i.very` in the formal transform-order.    
`p`

`p.
Also, `i.very` now has two targets.  We have 
`i.very loud`/, which yields a refined 
(here an `q.augmented`/) adverb, and `i.very` 
(encapsulating `i.very loud`/) adverbially modifying 
`i.were barking`/.  I will differentiate multiple 
targets for one modifier in terms of `q.stages`/.  
Here, `i.very` encompasses two different `i.stages` 
of transformation: first, it modifies an adverb 
to form a new adverb; then, in a second stage, 
the outcome modifies a verb ground.  I adopt the 
notational convention that directed edges in a 
parse graph are labeled with a number designating 
their stage: so in a minimal example the links 
between `i.very`/, `i.loudly`/, and `i.were` could 
be expressed as `WereVeryLoudlyGraph;: the subscripts on the 
edges disinguish the first transform-stage for `i.very` 
from the second.   
`p`

`p.

`p`


`p.

`p`

