`section.Hypegraph Grammar and Conceptual Spaces`
`p.
...
I think the motivation behind Coecke `i.et. al.`'s syntactic 
model based on Hypergraph Categories is comparable to 
my focus on `i.cognitive transforms`/.  In particular, 
their Hypergraph structure invokes the Category Theoretic 
sense of `i.morphisms`/, which are taken as the 
foundation of semantics: each word is construed as a 
morphism in some semantic Category (using this term 
in a mathematical sense, not related to `q.lexical` 
categories).  This applies also to nouns 
(and `i.sentences`/, analogous to what I call 
`i.propositions`/, embodied in finite clauses rather 
than words).  The Coecke `i.et. al.` model 
marshals a feature of hypergraphs wherein we can 
define (direct) edges which are not (on one side 
or another) connected to any nodes.  In conventional 
graphs, each edge connects exactly one node on eiher 
side; for hypergraphs, however, the `i.head set` 
and `i.tail set` of edges' nodes can be of varying sizes 
(including empty).
`p`

`p.
When applied to morphisms in Category Theory, this means 
that hypergraphs can model morphisms with outputs 
but no imputs, and vice versa.  Applied to linguistic 
data, input-less morphisms become nouns, and 
output-less morphisms become propositions 
(qua macrotype, using my terms).  There is a 
good cognitive rationale for this approach, at 
least if we want to treat words not only as 
linguistic artifacts but as, in some sense, 
names or triggers for certain intellectual processes.  
A noun, we might say, enters into the discursive 
space the `q.output` of a cognitive process 
%-- an act of recall and situating whereby we 
consult the word's lexical meaning and figure out 
how it applies in the current context.  
Parallelwise, a proposition can be treated as 
the final integration of linguistic data, 
which are `q.input` into our cerebral faculties.  
The point is that nouns have no `i.linguistically manifest` 
inputs %-- and propositions no manifest outputs in surface 
language, usually (except for cases like `i.I told you so`/, 
in resolving the `i.so` referent) %-- but we can 
still plausibly treat these as cognitive 
processes which generate outputs or take inputs, 
respectively.
`p`

`p.
With this added structure, all linguistic elements 
can be treated as interconnected via 
crossovers between inputs and outputs: when 
nouns are modifier targets, we can envision 
them as mental procedures that `q.output` their 
nominal content; when propositions are 
tranform outcomes, they are `i.inputs` to 
our cognitive machinery for synthesizing and 
contextualizing language artifacts.  
So the output-to-input link becomes common to 
all inter-word relations regardless of their 
lexical category.  The resulting architecture, 
then, is similar to the notion of parse-graphs 
which I have developed under the guise of 
`q.Cognitive Transform Grammar`/.  A sentence 
is a space of linguistic and conceptual 
`q.transforms` or `q.morphisms`/, which 
are interwoven via input/output connections: 
the output of one transform is the input to another.  
The network structure of these connections 
defines an flow of `q.information` or construal 
between different sentence elements.  
An example of `q.flow` in this sense would 
be specification propagation as analyzed in 
the last section: mesotype requirements 
tranferring from `i.dogs` to `i.neighbor's dogs` 
or singular/plural specification carrying over 
from `i.beer(s)` to `i.beer(s) on tap` to 
`i.a lot of beer(s) on tap`/.   
`p`

`p.

`p`
