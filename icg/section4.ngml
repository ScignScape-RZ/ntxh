`section.Hypegraph Grammar and Conceptual Spaces`
`p.
...
I think the motivation behind Coecke `i.et. al.`'s syntactic 
model based on Hypergraph Categories is comparable to 
my focus on `i.cognitive transforms`/.  In particular, 
their Hypergraph structure invokes the Category Theoretic 
sense of `i.morphisms`/, which are taken as the 
foundation of semantics: each word is construed as a 
morphism in some semantic Category (using this term 
in a mathematical sense, not related to `q.lexical` 
categories).  This applies also to nouns 
(and `i.sentences`/, analogous to what I call 
`i.propositions`/, embodied in finite clauses rather 
than words).  The Coecke `i.et. al.` model 
marshals a feature of hypergraphs wherein we can 
define (direct) edges which are not (on one side 
or another) connected to any nodes.  In conventional 
graphs, each edge connects exactly one node on eiher 
side; for hypergraphs, however, the `i.head set` 
and `i.tail set` of edges' nodes can be of varying sizes 
(including empty).
`p`

`p.
When applied to morphisms in Category Theory, this means 
that hypergraphs can model morphisms with outputs 
but no imputs, and vice versa.  Applied to linguistic 
data, input-less morphisms become nouns, and 
output-less morphisms become propositions 
(qua macrotype, using my terms).  There is a 
good cognitive rationale for this approach, at 
least if we want to treat words not only as 
linguistic artifacts but as, in some sense, 
names or triggers for certain intellectual processes.  
A noun, we might say, enters into the discursive 
space the `q.output` of a cognitive process 
%-- an act of recall and situating whereby we 
consult the word's lexical meaning and figure out 
how it applies in the current context.  
Parallelwise, a proposition can be treated as 
the final integration of linguistic data, 
which are `q.input` into our cerebral faculties.  
The point is that nouns have no `i.linguistically manifest` 
inputs %-- and propositions no manifest outputs in surface 
language, usually (except for cases like `i.I told you so`/, 
in resolving the `i.so` referent) %-- but we can 
still plausibly treat these as cognitive 
processes which generate outputs or take inputs, 
respectively.
`p`

`p.
With this added structure, all linguistic elements 
can be treated as interconnected via 
crossovers between inputs and outputs: when 
nouns are modifier targets, we can envision 
them as mental procedures that `q.output` their 
nominal content; when propositions are 
tranform outcomes, they are `i.inputs` to 
our cognitive machinery for synthesizing and 
contextualizing language artifacts.  
So the output-to-input link becomes common to 
all inter-word relations regardless of their 
lexical category.  The resulting architecture, 
then, is similar to the notion of parse-graphs 
which I have developed under the guise of 
`q.Cognitive Transform Grammar`/.  A sentence 
is a space of linguistic and conceptual 
`q.transforms` or `q.morphisms`/, which 
are interwoven via input/output connections: 
the output of one transform is the input to another.  
The network structure of these connections 
defines an flow of `q.information` or construal 
between different sentence elements.  
An example of `q.flow` in this sense would 
be specification propagation as analyzed in 
the last section: mesotype requirements 
tranferring from `i.dogs` to `i.neighbor's dogs` 
or singular/plural specification carrying over 
from `i.beer(s)` to `i.beer(s) on tap` to 
`i.a lot of beer(s) on tap`/.   
`p`

`p.
In contrast to Coecke `i.et. al.`/, my orientation 
here is more explicitly cognitive: I see 
formal models as suggestive, summarial views on 
the structural articulation of mental language-understanding 
processes, rather than as logical distillations of 
semantic machinery.  The linguistic literature on 
Hypegraph Categories (and similar mathematically-inspired 
methodology) has a tendency, in my opinion, to 
gravitate toward relatively context neutral 
semantic constructions, and ones which do not require 
a lot of interpretive finesse.  One of the aforementioned 
authors' more expansive examples, for instance, is the 
phrase `i.yellow bananas`/, which they read in terms 
of (effectively) objective qualities of bananas 
(their taste, color, texture, and classification as a kind 
of fruit) in combination with the predicate 
`i.yellow` (also treated `q.objectively` as a region 
of color space).  They argue that the conjunction of 
the two concepts yields a more specific concept analyzable, 
in some formal sense, as an intersection of the two 
%-- specifically, narrowing the set of bananas to 
those which are yellow (and not green) in color.  
They moreover set out mathematical formulae 
defining this concept-pairing in quantitative 
terms (not to imply that mathematical reasoning 
is part of semantics, I assume, but rather as 
merely a demonstration that certain mathematical 
principles in the superposition of spaces, 
suitably understood, have analogs in the 
scoping of semanticized concepts). 
`p`

`p.
I question how far these quantitative models can go, 
since all but the most trivial concept-combinations 
seem to represent `i.selective` blending which 
is hard to picture mathematically.  For instanse, 
`i.fried green bananas` are not particularly green, 
and I am not sure how to `q.quantify` `i.fried` 
the way that we can assign `i.yellow` to a mathematically 
well-defined color-space (`i.hot`/, `i.sliced`/, and `i.crispy` 
are all concepts I vaguely feel are applicable to 
`i.fried bananas`/; but I would dispute that 
`i.fried` is somehow a conjoined predicate of 
those three %-- instead the adjective seems 
to stand in for a complete idea, a clause, 
like `i.the bananas have been fried`/).  With this 
caveat, however, the Hypergraph model of the 
structural `q.network` at play in a sentence 
does seem to remain a valuable paradigm, 
however we wish to model the concepts 
circulating through the structure.  
Given `i.imported yellow bananas`/, say, 
there is certainly a sense that our 
conceptualization undergoes an accretion which 
blends the predication of `i.yellow` and 
of `i.imported`/, and that output-to-input 
connections %-- the outcome of `i.yellow` 
qua modifier is input to `i.imported` %-- 
serve to suture the conceptualizations 
confirming that one specific ground-concept is 
being declared as `i.yellow` `i.and` as 
`i.imported` (instead of one yellow thing 
and another imported thing).   
`p`

`p.
In hypergraph grammar, sentences' syntactic form 
defines paths which lead (in semantically 
meaningful ways) between words, so that 
paths are formed by output-input connections.  
Along these paths we find some conceptual content 
which gets progressively modified and elaborated.  
Coecke `i.et. al.` turns to Peter `Gardenfors;'s 
Conceptual Space theory for a semantic 
framework adequate for, in effect, how inter-word 
`q.paths` produce aggregate conceptual meanings.  
At the same time %-- perhaps to reiterate 
that this is a structurally organized process 
%-- they endeavor to define the transformations 
along such `q.paths` (in my terms) in mathematical 
terms.  This is probably a good account of 
grammar's role in relation to semantics: 
syntax is constrained by the need, for 
each utterance, to impress on the hearer 
`i.the right path` to combine concepts appropriately.  
For instance, a path must join `i.imported` to 
`i.yellow` so as to fix that the yellow 
things (bananas) are also the imported things.  
So the broad outline of the Hypergraph-plus-Conceptual 
Space `q.project` is to use Conceptual Spaces 
as the conceptual terrain on which progressively 
more determinate significations can be defined, 
such that the need to convey that specific 
accretion of concetual details defines the 
parameters and telos which engenders syntactic 
form.  For each grammatically specified 
path among words on the syntactic side, there 
is a corresponding path within (a suitably 
expansive) set of Conceptual Spaces on 
the semantic side.     
`p`

`p.
Intuitionwise, I believe this perspective aligns with 
(what I call) Cognitive Transforms; my point of 
contention, however, lies with (I believe) 
narrowly quantitative models of Conceptual Spaces.  
To make the syntactic-path-to-conceptual-accretion 
concordance applicable to the broad spectrum 
of linguistic phenomena, we need a more 
general, context-sensitive and nuanced 
picture of Conceptual Spaces, which can perhaps 
draw on the diverse literature varying Conceptual Space 
models as one branch of Cognitive Linguistics.  
I will now consider some potential features of 
such an expanded Conceptual Space Theory. 
`p`

`subsection.Conceptual Spaces and Cognitive Grammar`
`p.
Spaces like color and taste, which have convenient representations 
in terms of axes and quantities, are recurring examples 
in Conceptual Space Theory.  However, this theory has 
been applied to conceptualizations in many domains, 
including verbs and movement-patterns; 
spatial paths and configurations; scientific theories; 
and structural parameters in the context of 
computer modeling, such as markup languages 
(a specific `q.Conceptual Space Markup Language` 
has been formulated which allows conceptual details, 
according to tenets of the theory, to be 
asserted with respect to fields and 
dimensions within digital/computational data structures). 
As such, we should not be discinclined to 
consider possible Conceptual Space formulations 
even if, in some domain of semantic relevance, 
we have trouble finding quantitative 
parameters as clear-cut as, say, `i.hue`/, 
`i.saturation`/, and `i.value` in color space.
`p`

`p.
On the other hand, even in its branching directions 
Conceptual Space Theory does foreground 
the (minimally quantitative) notion of 
`q.convexivity`/, and show a preference  
for concepts which can be paired (or gathered 
into sets more generally) on grounds of 
magnitude in `i.some` sense: tall/short; 
around/across (see Gardenfors, 
Geometry of Preposition Meanings, p. 13 
[http://newprairiepress.org/cgi/viewcontent.cgi?article=1098&context=biyclc]); 
ancestor/descendent (or more generally defining the 
domain of kinship relations by degrees of relation and 
forward or backward in generation; see 
What is a domain? Dimensional structures
versus meronomic relations p. 449).  
The cumulative effect is a system of 
analysis which foregrounds comparative 
boundaries between `q.competing` concepts 
(in the sense that borderline cases may 
be ambiguous between them) and/or concepts 
whose differences can be accounted for 
numerically in some property or domain.
`p`

`p.
I intend to extend this conception of 
Conceptual Spaces to place more emphasis on 
concepts' situational interconnectedness.  
Concepts are often paired up not in the 
sense of competing or dimensionally 
contrastive (e.g. tall/short) prototypes 
within some larger space, but rather in 
terms of `i.functional` connections: 
in general, for any given concept there is a 
family of interrelated concepts linked via 
co-operation in some situation.  The concept 
`i.knife`/, say, is inextricable from both 
a substance cut and a person doing the action; 
the concept `i.fried` inextricable from a foodstuff 
cooked as such and also a pan or pot where 
the food is placed while frying.    
`p`

`p.
When highlighting functional interactions, 
several factors come to the fore.  First, 
concepts are often delimited by their 
relation to `i.verbs`/: functional relations 
are usually established on the basis of 
actions or events in which different 
constituents play different roles: 
the action of `i.carving` links the knife 
to the duck, or the chisel to the wood.  
Second, functional relations such as 
nouns to verbs (and by extension to other nouns) 
are dependent on situational context: 
for example,  
the culinary role of knives comes to 
the fore in kitchens and dining rooms; 
so our most common knife-conceptualization 
becomes properly `q.activated` in these 
settings.  There are other 
contexts where we sometimes encounter knives 
(as weapons; as cutting tools for 
non-kitchen chores), and the 
knife `q.prototype` (to the degree that 
such a thing exists) is different in these 
contexts than the table knife for dining 
or the kitchen knife for cooking.  
So the first step in reviewing the conceptual 
`q.space` in which knives, say 
(continuing the example) are mentally situated, 
is to determine which context is appropriate.    
`p`

`p.
A third observation is that, even when we can 
identify groups of interrelated and 
overlapping concepts, the contextual and 
functionalizing aspects seem more 
esssential to assessing concepts' proper use 
than assessing boundaries between concepts 
which are `q.peers` in a sense.  So 
`i.knife` can be contrasted with 
`i.axe`/, `i.dagger`/, `i.sword`/, `i.saw`/, 
`i.cleaver`/, and `i.reamer`/.  We can find 
certain quantitative measures which 
help define the relevant conceptual 
differences: knives are shorter than 
swords, narrower than cleavers, 
with smooth or serrated rather than
saw-tooth blades, and so on.  Perhaps 
these comparisons are relevant to visual 
cues which condition us to recognize when 
some implement is a knife rather than, say, 
a saw (but context surely also factors in, 
because we `i.expect` to find a knife in a 
kitchen and a saw in a woodshop, not vice-versa).  
But our inclination to construe an object 
as a knife (or, by contrast, a fork, spoon, 
glass, or plate) is surely driven more by 
an anticipatory sense of how situations will 
be functionally organized.  As we sit down 
to dinner, say, our mental map of the 
circumstance encompasses a matrix of 
expected components (the food, our place-settings, 
the different kinds of utencils).  
We are thus primed to anticipate certain 
specific objects, and perceived things 
in our surroundings fill in these preconceived 
slots.  We recognize something as a knife 
because our situational intuitions up to 
that point prompt us to assume that knives 
will be among the objects present.     
`p`

`p.
One question then is to consider how these 
contextualizing effects can be modeled 
in terms of Conceptual Spaces.  
`p`
