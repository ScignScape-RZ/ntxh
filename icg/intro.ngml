
...

`p.
When linguistic methodology is described as 
`i.computational`/, this usually implies that 
the research is not only developing or employing 
some formal representation of linguistic 
elements, but is moreover using some kind 
of computational mechanism to instantiate 
and/or analyze the relevant formal structures, 
concretely filled in with particular 
linguistic content.  In the case of 
constituency trees or dependency graphs, 
for example, there may be a software 
package which implements the trees or 
graphs as digital data structures, 
manifested by assigning words (or other lexemes) 
to the requisite nodes (or `q.leaves`/).  
In that digital form, the data structures may 
then be available for further manipulation, 
such as being visually presented in some 
diagrammatic manner or being paired with 
other data structures, such as tables itemizing 
the lexical interpretation and/or grammatic 
categorization of the component lexemes.
`p`

`p.
The term `i.computational linguistics` probably also 
implies that these digital representations 
are %-- or will potentially be %-- `i.automatically` 
generated by some Natural Language Processing 
system.  For most researchers, that is, the use of 
computers as linguistic tools is bound up with a 
goal to automate various dimensions of linguistic 
processing, which in turn could provide for Artificial 
Intelligent agents that mimic human linguistic 
competence.  Machine translation, for instance, 
could be effectuated by parsing sentences to 
dependency graphs, transforming the resulting graphs 
by translating words between languages, and 
synthesizing sentences in the target language 
via the output graphs.  The machine-translation `q.problem` 
is thereby reduced to three smaller problems whose 
`q.solutions` can be chained together %-- starting 
with the problem of mapping linguistic content to 
formal re-constructions.  
`p`

`p.
For linguists, of course, automating the annotation of 
linguistic components is more convenient than 
doing the same work manually.  In the context of 
Natural Language Processing, automation is more 
than just a convenience: here, theoretical 
paradigms and concrete models are assessed 
specifically on how well they engender 
computational artifacts which manipulate 
linguistic givens in human-like ways without 
human intervention.  A module which transforms 
unadorned sentences into formal dependency 
graphs, say, would be deemed successful if 
most such parses are identical to those 
produced by human annotators.  To be sure, 
the more that reconstruction of linguistic 
content between different surface-level or 
formal representations can be automated, 
the more that Artificially Intelligent 
language engines can be constructed merely 
by interconnecting these automated capabilities.  
However, we should not assume that formal 
representations are `i.only` valuable in 
the context of these automated systems.  
`p`

`p.
Instead, it is possible to employ 
formal or, indeed, computational structures 
as vehicles for linguistic explanation or 
exploration, whether or not the structures 
can be automatically obtained from 
surface-level language (or from other formalisms).  
As a case in point, I will here propose a 
formal model for Cognitive Grammar which, 
in broad outline, takes its structure from 
recent investigations merging Conceptual 
Space Theory, in the sense of Peter `Gardenfors;, 
with a notion of `q.hypergraph` grammar.  
This combination %-- proposed in a schematic 
manner particularly in `cite<BobCocke>;, which 
apparently reflects the experiences of 
several seminars conducted by a group of 
linguists and mathematicians %-- outlines 
an overall linguistic model where hypergraph 
structures are featured in grammatic description 
and Conceptual Spaces are recognized as a 
foundation for semantics.  The proposal, 
which I will examine in more detail below, 
is sufficiently schematic that it may be 
best treated as a metatheoretical template 
%-- open to elaboration both in terms 
of how hypergraph grammars are specified and 
in terms of how Conceptual Space theory 
is understood to anchor semantics 
(here I will consider both dimensions in the 
context of Langacker's Cognitive Grammar, 
e.g. comparing Langacker's notion of 
`i.domains` to that of `Gardenfors;). 
`p`  

`p.
Assuming, in any case, that we have a specific 
`q.Conceptual Hypergraph` model (unifying 
an elaborated Hypegraph Grammar theory with a 
Conceptual Space semantics), this model can be 
considered a `i.computational` model insofar 
as the resulting hypergraph-based data structures 
are amenable to computational treatments: 
for example, being implemented as data structures 
in a programming language, and subject to manipulation 
via these computational representations.  
Indeed, I will argue that a variation of 
the Conceptual Hypergraph model can be adopted as a 
basis for `i.formal` languages, such as 
computer programming languages.  Accompanying this 
essay is demonstrative implementation of a 
programming language which illustrates this explicitly, 
using a `q.Hypergraph Virtual Machine` and 
hypergraph-based Intermediate Representation to 
concretize the compiler and runtime algorithms needed 
to translate source code (conformant to a hypergraph 
grammar) into machine-readable instructions.  
In this sense, I claim that a Conceptual Hypergraph model 
of language in general is interesting in computational 
contexts in part because it has applications to 
both natural and artificial languages. 
`p`

`p.
Nevertheless, I do not claim here that the Conceptual Hypergraph 
model is well-suited to computational `i.automation`/.  
Although I will describe a formal representation for linguistic 
data within this model, I am not concerned with whether 
computers could be programmed to derive these representations 
automatically, in contrast to the process of 
humans manually assembling the hypergraph structures as 
explanatory models for given sentences.  While I think 
Conceptual Hypergraph models are subject to many 
interesting computational treatments (including 
via compiler/runtime pipelines), I do not endorse 
any philosophical speculation that natural language 
is sufficiently formalizable to be emulated 
by machines (even given practically infinite computing 
power and `q.training data`/).
`p`

`p.
Let me, in particular, make a contrast between 
`i.explanatory` formal models as opposed to 
formalisms aspiring to `i.simulate` human linguistic 
performance.  In the former sense, models are used 
by humans to better understand language as a 
structural and/or cognitive phenomenon.  
The compositional requirements and transformative 
options available to formal models can then 
serve as proxies or intuitions for the rules 
and transformations internal to language.  
In this explanatory role, formal models do not need 
to be precise reconstructions of linguistic behavior, 
because we do not need them to serve as our sole 
source of linguistic explanation: formal models are 
simply one vehicle among others for 
investigating linguistic phenomena and communicating 
linguistic theories.  By contrast, when we 
reify formal models to the point of `i.subsuming` 
all linguistic phenomena %-- in the sense that 
models should engender `AI; engines that replicate 
human linguistic competence %-- we impose an 
unreasonably high burden.  Formal (and computational) 
models can be theoretically useful even if they 
are not totalizing enough to reduce all language 
to computationally tractable algorithms.
`p`

`p.
In this spirit I will argue, then, that 
a Conceptual Hypegraph model can be used to 
bridge Cognitive Grammar to formal linguistic 
methodologies %-- not in the sense that 
Hypergraph Grammar and Conceptual Space theory 
jointly embody a complete formal specification 
for language, nor that the model allows us to 
equip `AI; with something like `q.cognition`/, 
but rather that a certain class of 
Hypergraph Grammar and Conceptual Space constructions 
can be simultaneously illustrative specifications 
of Cognitive Grammar constructions and 
also elements of a system with formal properties, 
subject to formal presentations and analysis.  
`p`

`p.
I believe that language is neither completely 
formal nor completely informal; as a result, 
formal models are neither entirely irrelevant 
nor holistically adequate for linguistic explanation.  
It is true that language is a human (communal) 
activity, and surely acquires some of its structure 
from this interpersonal dimension, just as 
social norms or cultural value systems reflect 
certain regularities that can be structurally 
analyzed.  However, structural explanation has a 
deeper warrant in the context of linguistics than 
in, say, sociology or anthropology.  
There are constructive patterns that are directly 
manifest in well-formed sentences, and which 
are in that guise amenable to structural analysis; 
this differs from Structuralism in mythology, for 
instance, where interpretive sophistication is needed 
to reveal patterns that imply compositional `q.laws` 
somehow operative in myth-making.  Even language 
with no formal rule-books, no instructional pedagogy, 
or no written form, evince precise grammatic 
and morphological systems %-- equally or more complex 
than cosmopolitan languages regulated by institutions 
like the `AcademieFrancaise;.      
`p`

`p.
In short, it is possible to find within language precisely 
specifiable patterns that are not imposed by 
any external authority, nor vague enough to be 
biproducts of a certain manifest structurality that 
we might find in many regions of human communal activity.  
This implies that there is something within meaning or 
meaningfulness %-- or our cognition thereof %-- which 
trends toward structural regularity.  It would certainly 
be part of the linguistic purview to explain how this 
trend arises (organically from the mental and intersubjective 
processes which yield language) and to document 
structural regularities at work in specific languages.  
On the other hand, however, such an analysis has no bearing on 
whether the analysis of structural patterns supplies a 
comprehensive method for analyzing language as a whole, 
or how extensively signification relies on structurally 
enumerable patterns rather than gestures, contextual 
cues, communicative conventions among narrowly defined 
dialect communities, and so forth.  To analyze the 
speech of several friends discussing a baseball game, 
it may be that only a relatively small percentage of 
the communicative principles manifest in their activity 
can be explained by structural semantic or syntactic 
models; such models should then be deemed 
necessary but not sufficient for comprehensive 
treatments of language.
`p`

`p.
Structural patterns may emerge from how we cognize 
and then share impressions of empirical situations; 
or in the internal logic of states of affairs that 
are worthy of or conducive to linguistic expression; 
or some combination.  It is reasonable to assume 
that most (or all) linguistic statements have some 
basis in fact %-- that they are associated with 
some `i.propositional content` which the speaker either 
asserts or somehow comments on.  The point is not 
that every sentence has a meaning which can be directly 
reproduced propositionally, but that most or all sentences' 
meanings take their departure from a propositional 
content that the speaker profiles in some orientation 
%-- as asserted, desired, interpreted, explained, 
doubted, and so forth.  Often the communicative burden 
lies not in merely designating the concomitant 
proposition but in expressing a meta-level attitude 
to that content, so that the speech-act signifies and 
then elaborates upon it:

`exs,
`ex; You know that Warren leads Biden in the polls, don't you?
`ex; I hope that Warren can appeal to moderates.
`ex; I'm sure that Warren will pivot to the center and 
will poll well with moderates.
`exs`

The proposition that `i.Warren will poll well with moderates` 
certainly denotes a testable premise %-- it will prove true or 
false as the polls are in fact taken over time %-- but 
the point of (3) is not only to assert that claim, but to 
add interpretive and causative detail: (3) implies 
that Warren will poll well `i.because` she 
`q.pivots to the center`/.  So we cannot `q.reduce` the 
meaning of (3) to the propositional content glossed 
as `i.Warren will poll well with moderates`/; but 
denoting that content certainly seems to be 
a prerequisite to defining (3)'s actual meaning. 
`p`

`p.
Of course, (2) and (3) can be seen as packaging 
one sort of propositional content inside a second, where 
the `q.outer` content reflects states of affairs pertaining 
to propositional attitudes.  Notice that (2) and (3) are 
falsifiable.  Perhaps (2) is spoken by a critic of 
Warren, who is rooting `i.against` her; but (2) is an 
indirect or diplomatic way of calling attention to her problems 
appealing to moderates (to hope for something implies some 
doubt whether that will in fact transpire).  Conversely, 
(3) might be said by a Warren supporter trying to alleviate 
the concerns of (2).  People can consciously 
falsifiable reports of their propositional attitudes, 
for various rhetorical or manneristic reasons.  
But this implies that propositional attitudes are 
themselves falsifiable contents, so it can be 
true or false that someone hopes for or believes 
something.  As a result, we can argue that the 
actual propositional content invoked by some sentences 
can be factored into an `q.extramental` objective part 
and a speaker-relative attitudinal report.  
`p`

`p.
Speech-acts, of course, also signal speakers' desires 
to `i.make` something the case, including via commands 
and requests.  In the above examples, (1) is 
apparently performative in a similar way, since 
(1) implies that the speaker `q.wants` the addressee 
to accept the designated claim as true.  The 
insinuation is that `i.Warren outpolls Biden` 
(as an asserted fact) `i.and moreover` that 
this fact should by collectively agreed upon, 
among the inventory of shared posits that 
undergird any conversation.  So (1), via some 
indirection, expresses a kind of second-order 
`q.attitudinal` content analogous to (2) and 
(3) (the speaker's belief that `i.Warren outpolls Biden` 
is sufficiently extablished as fact to be 
collectively `q.enregistered`/), while (1) also 
implicates a request (that the other 
conversants consent to this epistemic entrenchment).    
`p`

`p.
In effect, language would be impossible if we could not 
express both propositional contents and attitudes 
to those contents (wherein propositional attitudes 
are in turn a form of propositional content).  
In this sense language will reveal some 
structural patterns merely insofar as language's 
signifying resources include, as a proper part, 
exposition of factual beliefs and assertions.  
Here I am not implying that patterns in 
propositional structure are directly translated 
to patterns in language %-- by way of illustration, 
the usages in (3) are, on reflection, highly metaphorical 
and stylized.  The conventionalized trope of 
a politician `q.pivoting to the center` employs a 
spatial construal to reference an empirical state 
of affairs which, logically reconstructed, 
would be very complex and contextual.  
While we can provide a workable predicate 
gloss %-- someone `q.pivots to the center` if 
their political comments or speeches reveal a 
particular pattern `visavis; the projection of 
distinct political topics onto a common centrist/extremist 
scale, such that someone's posturing over time figures 
summarily as `q.movement` in this abstract space 
%-- the semantics of the idiom `i.pivot to the center` 
is not a facile logical construction like 
`i.bachelor` and `i.unmarried man`/, or `i.ring` and 
`i.jewelry worn on one's finger`/.   
`p`

`p.
Merely noting the predicate structure of signified 
propositional contents, that is, may be at best 
tangential to actual linguistic explanation, because 
it cannot account for how metaphorical cases 
like `q.pivot to the center` are cognized and 
conventionalized.  However, the requirement that 
language must signify states of affairs 
%-- via metaphor or conceptual schema or something 
more transparently compositional %-- is a reasonable 
starting point for exploring where language's structural 
sophistication comes from, because it certainly 
seems that propositional contents have structural 
patterns, so that these must eventually fall out 
of linguistic processing.  Of course, we may also 
argue that linguistic structure derives from how 
we conceptualize states of affairs %-- and encode 
these conceptualizations in language %-- 
as much as from the structural logic of states of 
affairs themselves.    
`p`

`p.
As explanatory intermediaries, both cognitive and 
propositional structure can coexist as 
originations for formalisms manifest in language.  
We need not regard analysis of cognitive frames 
exerting a morphosyntactic influence on the shape 
of aggregate linguistic units as denying a parallel 
influence from the situational context with which 
discource must, to some measure, propositionally 
align; or vice-versa.  And yet there do seem 
to be paradigm-grounded pressures to situate 
language morphology (in the informal sense of 
higher-level linguistic `q.shape`/) `i.either` in 
expressed propositions `i.or` in cognitive construals.  
To some degree this may reflect metatheoretic 
divergence in the basic terms of discussion: 
is the `i.meaning` of (4) here to be the 
proposition that Warren will win, or the speaker's 
belief that Warren will win?

`exs.
`ex; Warren will probably win the nomination.
`ex; Biden's support among the base is eroding.
`exs`

Likewise, should we understand the more metaphorical 
phraseology in (5) as a conventionalized idiom that 
has become a kind of entrenched signifier, no longer 
really operating metaphorically; or rather as  
projections of metaphorical devices which the speaker 
uses conceptually to think through the situation, 
anterior to her opinions being verbalized?     
`p`

`p.
Taking a broad view, language involves a (typically iterative) 
process where a speaker has some thought, encodes it via 
language, yielding a spoken or written aggregate which 
an addressee encounters, processes, and then (hopefully) 
understands.  Most of this larger process occurs within 
the minds of the conversants participating; but 
it could reasonably argued that the specifically 
`i.linguistic` (and not psychological or sociocultural) 
concern here is focused on the observable properties of 
the spoken or written content produced.  The mental 
provenance and reception surrounding that content may be 
very real, but (or so one might say) cognitive 
analysis only has bearing on the specific 
disciplinary focus of `i.linguistics` insofar as 
the objective language artifact bears the 
imprint of certain cognitive processes, which are 
irreducible in a comprehensive account of why 
the artifact exists.  To make an analogy, 
suppose a footballer executes a pass in accorded 
with a specific tactic which his team rehearses 
and discusses in practice.  There are surely many 
cerebral and physiological factors contributing to 
the player making that pass, but a football expert 
would be expected to focus on the pass itself 
%-- its tactical rationale and execution technique.  
In this analogy, actual linguistic content 
%-- sentences, say %-- are like the football in a football 
match; they are tangibly observable.  
The ball is played over the course of a match, and its 
spatial position can be described %-- as can player-specific 
factors affecting the ball's location, such as how they 
execute on-ball maneuvers, and their decision-making 
insofar as it fits within the overall scheme of 
football's rules and tactics.  But the analytic 
attention should rest with the ball itself, and 
only expand to include players' thought processes 
insofar as these bring explanatory value to 
observations of the game's objective features.
`p`

`p.
Analogously, language is, objectively, a realm of spoken and 
written productions or perhaps a system for creating new 
ones.  Consequently, someone can reasonably suggest that 
cognitive analyses explaining how a speaker 
came to enunciate `i.this particular` sentence may be 
correct, but also are not in and of themselves 
descriptions of speakers' meanings.  Reprising earlier 
examples, someone might say that political discourse 
(being fairly important and repetitive in a modern 
democracy) has evolved certain entrenched idioms, so 
we speak of `i.pivoting to the center` or `i.eroding support` 
as a shorthand for characteristic, objectively definable 
situations in the realm of politics.  The entrenched 
expressions and their objective content %-- however 
metaphoric the latter may seem, and abstract/contextual 
the former %-- stand in a signifier/signified relation not 
that different from a more mundane/concrete semiosis 
like `i.cat` and one class of animals, or `i.water` 
and `HTwoO;.  We are, in other words, prepared to 
accept that `q.cat` is a word which in some sense 
`i.means` the clade Felidae, and `q.water` means 
`HTwoO;.  We do not take these `q.meanings` to be 
cognitive phenomena, even if we ackowledge that 
cognition is a kind of metaphysical prerequisite 
%-- no word would mean anything if not for 
each language-user having their own mental inventory 
of correct words for different contexts.  
Yet what makes these inventory items `i.words` is 
their stable co-existence in the minds of 
many (or all) speakers of a language.
`p`

`p.
The whole point of agreeing that `i.cat`/, say, 
is a word in English is to recognize a 
substratum in `i.cat` which is at a theoretic 
level non-cognitive even if it `i.is` cognitive 
as a matter of ontological foundation.  
As a `i.word`/, `i.cat` has a theoretical 
status which abstracts from its cognitive manifestation 
in any one English speaker's mind, such that it 
can be defined `q.extramentally` in terms of the 
pairing between the word and the set of mammals which 
it conventionally designates.  To be properly 
linguistic, it might seem, our understanding of 
lexical and syntactic phenomena must perhaps 
acknowledge the cognitive nature of linguistic 
conventions as a matter of physical empiricism 
%-- words don't `q.exist` in some ethereal 
netherworld %-- but then transcend this cognitive 
layer by analyzing language enough that the synergy 
among different speakers' linguistic predilections, 
rather than any intramental immanence, becomes 
the theoretical center-stage.  Linguistics 
proper therefore arises insofar as the 
cognitive dimension %-- siting linguistic 
rules and judgments in people's minds simply 
as a matter of ontological parsimony %-- 
becomes in a sense canceled out by multi-person 
alignments (which are preconditions for 
truly linguistic phenomena).  Linguistic extra-mentality is 
then akin to how political scientists take 
demographic and electoral trends as their scientific 
givens, even though elections only `q.exist` through the 
voting decisions of concrete individuals. 
`p` 

`p.
I will argue in the next section that Cognitive Linguists 
perhaps discount the pervasiveness or rationale behind 
these extra-mental intuitions, and therefore have not 
fully responded to (or potentially bridged to) 
formal methodology which, in turn, are inclined 
to underestimate the cognitive contributions to meaning.  
My own opinion, already intimated, is that language 
actually unifies formal and contextual dimensions, so 
that methodologies which foreground cognitive construal 
and those which prioritize formal substrata are 
both responding to real `q.signals` in linguistic data, 
and therefore both have merit.  However, I also 
believe that Cognitive and (say) Computational linguists 
have not-entirely-aligned intuitions of the basic demarcation 
of the linguistic discipline itself, which renders a 
certain paradigmatic disconnect almost inevitable.  
`p`

`p.
To see this, consider again the minimal example 
wherein, say, English speakers use the phoneme 
`i.cat` to express an idea which (to some approximation) 
matches the biologists' `i.Felidae`/.  I think everyone 
would agree that a certain cognitive construal is thereby 
conventionalized and internalized to the point where, 
from each individual person's point of view, it becomes 
a `i.de facto` fact of the world: everyone knows that 
everybody else expects us to say `i.cat` when we intend 
to talk about Felids, so %-- however much the association 
may lie in people's minds %-- we all accept the useful 
fiction that `i.cat` means `i.Felid` as a matter of 
objective fact; just as Obama's or Trump's victories were  
objective facts notwithstanding `q.victory` meaning 
many individual persons' decisions to vote for them 
(and even more people's decision to recognize the election 
as legitimate, notwithstanding their own vote).  
Against this background, we can say both that 
linguistics concerns cognitive inclinations that 
are synchronized between people to the point 
of conventional entrenchment; `i.and` that 
linguistics concerns regularities conventionalized 
to the point where cognitive processes can be 
excluded from the fields disciplinary scope 
(and when they can't %-- e.g. in the analysis 
of rhetorical persuasion or of literary artistry 
%-- they tap into aspects of language which linguistics 
is not itself about).  But this leaves us with 
two different conceptions of the linguistic enterprise, 
differences which can be seen most clearly not on 
the methodological level but in the `q.philosophical` 
process of identifying the specific analyzands 
which are in fact the substance of linguistic science. 
`p`

`p.
Moreover, I think this divergence is clearest in the 
context of propositional content.  As I argued earlier, 
most or all sentences are specifically aligned 
with (and project propositional attitudes toward) 
specific propositions; such that the meaning of 
a sentence is dependent on (even when it does not 
coincide with) a proposition which the sentence 
signifies.  
`p`

`p.

`p`

