\section{Gatekeeper Code}
\phantomsection\label{sOne}
\p{There are several design principles which can help ensure 
safety in large-scale, native/desktop-style \GUI{}-based 
applications.  These include:
\begin{enumerate}\item{}  Identify operational relationships between types.  
Suppose \calS{} is a data structure modeled via type \caltypeT{}.  
This type can then be associated with a type (say, 
\typeTp{}) of \GUI{} components which visually display 
values of type \caltypeT{}.  A simple data structure 
may have \GUI{} representation via small \q{widgets} 
embedded in other components (consider a thermometer icon 
to display temperature).  Conversely, if \calS{} has many component 
parts, its corresponding \GUI{} type may need to span its 
own application window, with a collection of nested textual 
or graphical elements.  There may also be a type 
(say, \typeTpp{}) representing \caltypeT{}-values in a format 
suitable for database persistence.  Application code should 
explicitly indicate these sorts of inter-type relationships.
\item{}  Identify coding assumptions which determine the validity 
of typed values and of function calls.  For each 
application-specific data type, consider whether every 
computationally possible instance of that type is actually 
meaningful for the real-world domain which the type represents.  
For instance, a type representing blood pressure has a subset 
of values which are biologically meaningful \mdash{} where systolic 
pressure is greater than diastolic and where both numbers are 
in a sensible range.  Likewise, for every procedure defined 
on application-specific data types, consider whether the procedure 
might receive arguments that are computationally feasible but 
empirically nonsensical.  Then, establish a protocol for 
acting upon erroneous data values or procedure parameters.  
How should the error be handled, without disrupting the 
overall application?
\item{}  Identify points in the code base which represent new data 
being introduced into the application, or code which can materially 
affect the \q{outside world}.  Most of the code behind \GUI{} 
software will manage data being transferred between different 
parts of the system, internally.  However, there will be 
specific code sites \mdash{} e.g., specific procedures \mdash{} which 
receive new data from external sources, or respond to 
external signals.  A simple example is, for desktop applications, 
the preliminary code which runs when users click a mouse button.  
In the CyberPhysical context, an example might be code which 
is activated when motion-detector sensors signal something moving 
in their vicinity.  These are the \q{surface} points where data 
\q{enters the system}.
\pseudoIndent{}
Conversely, other code points localize 
the software's capabilities to initiate external effects.  For 
instance, one consequence of users clicking a mouse button might 
be that the on-screen cursor changes shape.  Or, motion detection 
might trigger lights to be turned on.  In these cases the software 
is hooked up to external devices which have tangible capabilities, 
such as activating a light-source or modifying the on-screen cursor.  
The specific code points which leverage such capabilities 
represent data \q{leaving the system}.  
\pseudoIndent{}
In general, it is important to identify points where data 
\q{enters} and \q{leaves} the system, and to distinguish 
these points from sites where data is transferred 
\q{inside} the application.  This helps ensure that 
incoming data and external effects are properly vetted.  
Several mathematical frameworks have been developed 
which codify the intuition of software components as 
\q{systems} with external data sources and effects, 
extending the model of software as self-contained 
information spaces: notably, Functional-Reactive Programming 
(see e.g. \cite{JenniferPaykin}, 
\cite{PaykinKrishnaswami}, 
\cite{WolfgangJeltsch}) and the theory of 
Hypergraph Categories 
(\cite{InteractingConceptualSpaces}, \cite{BrendanFong}, 
\cite{BrendanFongThesis}, \cite{AleksKissinger}). 
\end{enumerate}
Methods I propose in this chapter are applicable to each 
of these concerns, but for purposes of exposition I 
will focus on the second issue: testing 
type instances and procedure parameters for fine-grained 
specifications (more precise than strong typing alone). 
}
\p{Strongly-typed programming language offer some guarantees on 
types and procedures: a function which takes an integer will 
never be called on a value that is \i{not} an integer 
(e.g., the character-string \q{\lclc{46}} instead of the \i{number} 
\lclc{46}).  Likewise, a type where one field is an integer 
(representing someone's age, say), will never be instantiated 
with something \i{other than} an integer in that field.  
Such minimal guarantees, however, are too coarse for 
safety-conscious programming.  Even the smallest 
(8-bit) unsigned integer type would permit someone's age to 
be \lclc{255} years, which is surely an error.  So any 
safety-conscious code dealing with ages needs to check that 
the numbers fall in a range narrower than built-in 
types allow on their own, or to ensure that such checks are 
performed ahead of time.   
}
\p{The central technical challenge of safety-conscious coding 
is therefore to \i{extend} or \i{complement} each programming 
languages' built-in type system so as to represent 
more fine-grained assumptions and specifications.  
While individual tests may seem straightforward on a 
local level, a consistent 
data-verification architecture \mdash{} how this coding dimension 
integrates with the totality of software features and 
responsibility \mdash{} can be much more complicated.  
Developers need to consider several overarching questions, 
such as: 
\begin{itemize}\item{} Should data validation be included in the same 
procedures which operate on (validated) data, or 
should validation be factored into separate procedures?
\item{} Should data validation be implemented at the type 
level or the procedural level?  That is, should specialized 
data types be implemented that are guaranteed only to 
hold valid data?  Or should procedures work with more 
generic data types, and perform validations on a case-by-case 
basis?
\item{} How should incorrect data be handled?  In CyberPhysical software, 
there may be no obvious way to abort an operation in the 
presence of corrupt data.  Terminating the application may not be 
an option; silently canceling the desired operation or trying to substitute 
\q{correct} or \q{default} data may be unwise; and 
presenting technical error messages to human users may be confusing.  
\end{itemize}
These questions do not have simple answers.  As such, we 
should develop a rigorous theoretical framework so as to 
codify the various options involved \mdash{} what architectural 
decisions can be made, and what are the strengths and weaknesses 
of different solutions.
}
\p{This section will sketch an overview of the data-validation 
issues from the broader vantage of planning and stakeholder 
expectations, before addressing narrower programming concerns
in subsequent sections.
} 
\subsection{Gatekeeper Code and Fragile Code}
\p{I will use the term \i{gatekeeper code} for any code which checks 
programming assumptions more fine-grained than strong typing 
alone allows \mdash{} for example, that someone's age is not reported 
as \lclc{255} years, or that systolic pressure is not recorded as 
less than diastolic.  I will use the term \i{fragile code} for
code which \i{makes} programming assumptions \i{without itself} 
verifying that such assumptions are obeyed.  Fragile code is 
especially consequential when incorrect data would cause the 
code to fail significantly \mdash{} to crash the application, 
enter an infinite loop, or any other nonrecoverable scenario.
}
\p{Note that \q{fragile} is not a term of criticism \mdash{} some algorithms 
simply work on a restricted space of values, and it is inevitable 
that code implementing such algorithms will only behave properly 
when provided values with the requisite properties.  It is necessary 
to ensure that such algorithms are \i{only} called with 
correct data.  But insofar as testing of the data lies outside 
the algorithms themselves, the proper validation has to occur 
\i{before} the algorithms commence.  In short, \i{fragile} and
\i{gatekeeper} code often has to be paired off: for each 
segment of fragile code which \i{makes} assumptions, there should 
be a corresponding segment of gatekeeper code which
\i{checks} those assumptions.  
}
\p{In that general outline, however, there is room for a variety 
of coding styles and paradigms.  Perhaps these can be broadly 
classified into three groups: 
\begin{enumerate}\item{} Combine gatekeeper and fragile code in one procedure.
\item{} Separate gatekeeper and fragile code into different procedures.
\item{} Implement narrower types so that gatekeeper code is 
called when types are first instantiated.
\end{enumerate}
Consider a function which calculates the difference between 
systolic and diastolic blood pressure, returning an unsigned
integer.  If this code were called with malformed data wherein 
systolic and diastolic are inverted, the difference would 
be a negative number, which (under binary conversion to an 
unsigned integer) would come out as a potentially 
extremely large positive number (as if the patient had 
blood pressure in, say, the tens-of-thousands).  This nonsensical 
outcome indicates that the basic calculation is fragile.  
We then have three options: test \q{systolic-greater-than 
diastolic} \i{within the procedure}; require that this test 
be performed prior to the procedure being called; 
or use a special data structure configured such that 
systolic-over-diastolic can be confirmed as soon as 
any blood-pressure value is constructed in the system.
}
\p{There are strengths and weaknesses of each option.  
Checking parameters at the start of a procedure makes 
code more complex and harder to maintain, and also 
makes updating the code more difficult.  The 
blood-pressure case is a simple example, but in real 
situations there may be more complex data-validation 
requirements, and separating code which \i{checks} 
data from code which \i{uses} data, into different 
procedures, may simplify subsequent code maintenance.
If the \i{validation} code needs to be modified 
\mdash{} and if it is factored into its its own procedure \mdash{}  
this can be done without modifying the 
code which actually works on the data (reducing the 
risk of new coding errors).  In short, factoring 
\i{gatekeeper} and \i{fragile} code into separate 
procedures exemplifies the programming principle of 
\q{separation of concerns}.  On the other hand, 
such separation creates a new problem of ensuring that 
the gatekeeping procedure is always called.  
Meanwhile, using special-purpose, narrowed data types 
adds complexity to the overall software if these data types
are unique to that one code base, and therefore 
incommensurate with data provided by external sources.  
In these situations the software must transform data between 
more generic and more specific representations before 
sharing it (as sender or receiver), which makes 
the code more complicated.  
} 
\p{In this preliminary discussion I refrain from any concrete 
analysis of the coding or type-theoretic models that 
can shed light on these options; I merely want to 
identify the kinds of questions which need to be 
addressed in preparation for a software project, 
particularly in the CyberPhysical domain.  
Ideally, protocols for pairing up fragile and 
gatekeeper code should be consistent through the code base. 
}
\p{In the specific CyberPhysical context, gatekeeping is especially 
important when working with device data.  Such data is 
almost always constrained by the physical construction of 
devices and the kinds of physical quantities they measure 
(if they are sensors) or their physical capabilities 
(if they are \q{actuators}, devices that cause changes in their 
environments).  For sensors, it is an empirical question what 
range of values can be expected from properly functioning 
devices (and therefore what validations can check that the 
device is working as intended).  For actuators, it should 
be similarly understood what range of values guarantee 
safe, correct behavior.  For any device then we can 
construct a \i{profile} \mdash{} an abstract, mathematical 
picture of the space of \q{normal} values associated with 
proper device performance.  Gatekeeping code can 
then ensure that data received from or sent to devices 
fits within the profile.  Defining device profiles, and 
explicitly notating the corresponding gatekeeping code, 
should therefore be an essential pre-implementation planning 
step for CyberPhysical software hubs.  
}
\subsection{Case Studies}
\p{To motivate the themes I will emphasize going forward, 
this section will examine some concrete data models 
which are used or proposed in various CyberPhysical contexts. 
I hope this discussion will lay out parameters on device 
behavior or shared data to illustrate typical 
modeling patterns and their corresponding safety or 
validation requirements.  As a preliminary overview, here 
are some examples of data profiles that might be 
wedded to deployed CyberPhysical devices 
(my comments here are also summarized in 
Table~\ref{table:profiles} on page \pageref{table:profiles}): 
\begin{description}\item[Heart-Rate Monitor]  A heart-rate sensor generates continuously-sampled integer values
whose understood Dimension of Measurement is in \q{beats per minute}
and whose maximum sensible range (inclusive of both
rest and exercise) corresponds roughly
to the \ftytwoh{} interval.  Interpreting heart-rate data depends on 
whether the person is resting or exercising.  Therefore, a 
usable data structure will join a beats-per-minute dimension 
with field indicating (or measuring) exertion, either a 
two-valued discrimination between \q{rest} and \q{exercise} 
or a more granular sampling of a person's movement cotemporous 
with the heart-rate calculations.
\item[Accelerometers]  These devices measure people's rate of 
movement, and therefore can be paired with heart-rate sensors 
to quantify how heart rate is affected by exercise 
(likewise for other biometric sensors, such as those 
calculating breathing rate).  An accelerometer
presents data as voltage changes in two or three directional
axes, data which may only produce signals when a change occurs
(and therefore is not continuously varying), and which is
mathematically converted to yield information about physical
objects' (including a person's) movement and incline.  
Physically, that is, accelerometers actually 
measure \i{voltage}, from which quantitative measures 
of movement and incline can be derived.Accelerometers are 
classified as \i{biaxial} or \i{triaxial} depending on 
whether they sample forces in two or three spatial 
dimensions.  
The pairwise combination of heart-rate and acceleration data
(common in wearable devices) is then a mixture of these
two measurement profiles \mdash{} partly continuous and
partly discrete sampling, with variegated axes and
inter-dimensional relationships.  
\item[Speech Sampling]  Audio sensors can be used to 
isolate different people's speech episodes (see Raju Alluri 
and Anil Kumar Vuppala, this volume, and 
Ravi Kumar Vuddagiri \textit{et. al.}, this volume).  Feature 
extraction cancels background noise and partitions the foreground 
audio into different segments, individuated (potentially) by 
differences between speakers as well as different episodes 
where one speaker is talking or silent.  Such data can 
then be employed in several ways.  Ant\'onio Teixera's chapter 
(\textit{et. al.}, this volume) discusses 
speech-activated User Interfaces for 
software, while the prior two chapters present methodology for 
estimating speakers' emotional states and 
identifying samples' spoken language or dialect, 
respectively.  The data generated by 
an audio processor will be determined by the system's 
overarching goals.  For example, \cite{JongyoonChoi} describes 
techniques for measuring emotional stress via heart-rate signals.  
Combined with speech-derived data, a system might be designed 
around emotional profiles, merging linguistic and 
biometric evidence.  Given that kind of system's 
use-cases, the programming would emphasize 
isolating speakers' locations, keep track of time, and 
and look for signs (reinforced by both metrics) of 
emotional changes.  On the other hand, a voice-based 
User Interface might similarly model speaker's identity 
and location, but perform Natural Language Processing 
to translate speech patterns into models of user's 
requests (and the optimal software responses).       
Conversely, the use-case in Vuddagiri \textit{et. al.} 
(this volume), where speech data is parsed for language 
classification (viz., matching voices to the language 
or dialect spoken) as part of a \q{smart city} network, 
calls for different features.    
The priority here is not necessarily identifying individual 
speakers, but, potentially geotagging samples to obtain a geospatial model of 
language-use in a given urban area. 
\item[Bioacoustic Sampling]  Similar to speech sampling, 
audio samples can be used to track and identify 
species (Todor Ganchev, this volume, and Boulmaiz, \textit{et. al.}, 
this volume).  Here again feature 
extraction foregrounds certain noise patterns, but the 
main analytic objective is to map audio samples to 
the species of the animal that produced them.  
Sensor networks can then build a geospatial/temporal 
model of species' distribution in the area covered by 
the network: which species are identified, their prevalence, 
their concentration in different smaller areas, and so forth.  
These measurements can be employed in the study of 
species populations and behavioral patterns, and can 
also add data to urban-planning or ecological models.  
For example, precipitous decline of a species in some 
location can signal environmental degradation in that vicinity.   
\item[Remote Medical Diagnosis]  An emerging application of 
CyberPhysical technology involves medical equipment 
deployed outside conventional clinical settings 
\mdash{} in remote areas with little electricity, refugee 
camps, temporary ad-hoc medical units (established 
to contain potential epidemics, for instance), and 
so forth.  These settings have limited diagnostic 
capabilities, so data is often transmitted to distant 
locations in lieu of on-site laboratories.  A 
good case-study derives from \q{medical whole slide imaging} 
(\mWSI{}) \cite{Auguste}, where a mobile 
phone attached to an ordinary microscope, 
by subtle modifications of camera position and microscope 
resolution, allows many views to be made on one slide.  
Positional data (the configuration of the phone and microscope) 
then merges with image segmentation 
computations characteristic of 
conventional whole slide imaging (see, e.g. \cite{Farahani}), 
and diagnostic pathology 
in general, which 
is concerned with isolating medically significant image 
features and identifying diagnostically significant 
anomalies (such as cell shapes 
suggesting cancer).  Segmentation, in turn, 
generates multiple forms of geometric data; in 
\cite{KaleAksoy}, for instance, segments are 
identified as approximations to ellipse shapes, 
and features are tracked across scales of resolution, 
so geometric data merges ellipse dimensions with 
positional data (in the image) and a metric 
of feature persistence across scales.  
(Features which are detectable at many scales of 
resolution are more likely to be empirically 
significant rather than visual \q{noise}; calculating 
cross-scale \q{persistence} is an applied 
methodology within Statistical Topology \mdash{} 
see e.g. \cite{HaneyMaxwell}, \cite{EdelsbrunnerHarer}).  
Merged with \mWSI{} configuration info and patient data, the 
whole data package integrates geometric, CyberPhysical, 
and health-record dimensions. 
  
\item[Facial Recognition]  Given a frontal view, software can 
rather reliably match faces to a preexisting database or 
track faces across different locales.  The methodology depends 
on normalizing each foreground image segment (corresponding to 
one face) into a rectangle, whose axes then establish vector 
components for any features inside the segment.  Feature extraction 
then isolates anatomical features like eyes, nose, mouth, and 
chin, quantifying their position and distances, yielding a 
collection of numeric values which can statistically 
identify a person with relatively small error rates.  
Given privacy concerns, enterprise or government use 
of this data is controversial: should analyses be performed 
on every person, or only on exceptional circumstances 
(crime investigation, say)?  Can facial-recognition outcomes 
be anonymized so that faces can be tracked across locations 
but not tied to specific persons without extra (normally 
inaccessible) data?  When and by whom should face data be 
obtainable, and under what legal or commercial circumstances?  
Should stores be allowed to use these methods to 
prevent shoplifting, for example?  What about searching 
for a missing or kidnapped child, or keeping tabs on an 
elderly patient?  When does surveilance cross the line 
from benevolent (protecting personal or public safety) 
to privacy-invasive and authoritarian?
\end{description}
}
\p{Of course, there are many other examples of CyberPhysical 
devices and capabilities that could be enumerated.  But 
these cases illustrate certain noteworthy 
themes.  One obsevervations is that a gap often exists between how 
devices physically operate and how they are conceptualized: 
accelerometers, for instance, mechanically measure voltage, not 
acceleration or incline; but their data exposed to 
client software is constructed to be used as vectors indicating 
persons' or objects' movement.  Moreover, multiple processing 
steps may be needed between raw physical inputs and usable  
software-facing data structures.  Such processing may generate 
a large amount of intermediate data; for instance, feature extraction 
from audio or image samples can yield numeric aggregates with 
tens or hundreds of different fields.  Further processing usually 
reduces these structures to narrower summaries: an audio sample 
might be consolidated to a spatial location and temporal timestamp, 
along with a mapping to an individual person speaking 
(perhaps along with a text transcription), or 
human language spoken, or animal species.  Engineers then 
have to decide what level of detail to expose across a 
software network.  Another issue is integrating data from 
multiple sources: most of the more futuristic scenarios 
envision multi-modal Ubiquitous Computing spaces 
where, e.g., speech and biometric inputs are cross-referenced.
}
\p{Different levels of data resolution also 
intersect with privacy concerns: simpler data structures 
are more likely to employ private or sensitive 
information as an organizing device, heighting security 
and surveilance concerns.  For example, a simple 
facial-recognition system would match faces against known 
residents of or visitors to the relevant municipalities.  
This is less technologically challenging than anonymized 
systems which would persist more mid-processing data in order 
to complete the algorithmic cycle \mdash{} matching faces to 
concrete individuals \mdash{} only under exceptional circumstances.
}
\p{Analogously, syncing speech technology with personal health 
data would be simplified by directly matching speaker identifications 
to biosensor devices wearers.  Again, though, using 
personal identities as an anchor for disparate data points makes 
the overall system more vulnerable to intrusive or inappropriate use.  
In total, security concerns might call for more complex data structures 
wherein shared data excludes the more condensed summaries 
wherever they may expose private details, and rely more 
on multipart, mid-level processing structures.  Rather than 
organize face-recognition around a database of persons, for example, 
the basic units might be numeric profiles paired with probabilistic 
links notating that a face detected at one time and place matches 
a face analyzed elsewhere, but without that similarity being 
anchored in a personal identifier.           
}
\p{Other broad issues raised by these CybePhysical case-studies 
include (1) testing and quality assurance and (2) data 
interoperability.  In the case of testing, many of the scenarios 
outlined above (and throughough this volume) require complex 
computational transformations to convert raw physical data into 
usable software artifacts.  In \cite{FadelAdib}, for example, 
the authors present technology to measure heart rate from a 
distance, based on subtle analysis of physical motions 
associated with blood circulation and breathing.  The analytic 
protocols leverage feature extraction from wireless signal 
patterns.  As with feature extraction in audio and 
image-analysis (e.g. face recognition) settings, algorithms need 
to be rigorously tested to guard against false inferences 
or erroneous generated data.  This implies that analytic 
code needs to be developed in a software ecosystem which is 
rigorously structured in documenting algorithmic inputs, outputs, 
and parameters.  In \cite{FadelAdib} the ultimate goal is to 
introduce heart and breathing monitors within a Smart Home environment,  
with computations performed on embedded Operating Systems.  
However, testing and prototyping of the technology should be 
conducted in a desktop \OS{} environment so as to 
generate or leverage test data, document algorithmic revisions, 
and in general prove the system's trustworthiness in a 
controlled setting (including a software environment which 
transparently windows onto computational processes) before 
this kind of network is physically deployed.
}
\p{With respect to data integration, notice how projects mentioned 
here often anticipate pooled or overlapping information.  For 
instance, Smart Homes are envisioned to embed sensors analyzing 
speech, biomedical data-points like heart rate, atmosopheric 
measurements like current temperature, and appliance or architectural 
states like windows, doors, or refrigerator doors being open, 
ovens or stove burner being turned on, or heaters/coolers being active.  
In some cases this data would be cross-referenced, so that 
e.g. a voice command could close a window or turn off a stove. 
Analogously, \cite{MohamedSoltane} (one of whose co-authors, 
Nouredine Doghmane, is also a coauthor of this volume's 
chapter on bird species) describes a combination of 
face-recognition and speech analysis for \q{multi-modal biometric 
authentication}; here again a component supplying 
image-processing data and one supplying speech metrics 
will need to transmit data to a hub where the two inputs 
can be pooled.  Or, as I pointed out in the case of 
Mobile Whole Slide Imaging, image-segmentation, 
CyberPhysical, and personal-health information fields 
may all be integrated into a diagnostic platform.
}
\p{Overall, future CyberPhysical systems may be integrated 
not only with respect to their empirical domain but in term 
of the environs where they are deployed \mdash{} smart homes, 
smart cities, factories or industrial plants, hospitals and 
medical offices, schools and children's activities centers, 
refugee or displaced-persons camps/campuses, and so forth.  
I'll take smart homes as a case in point: 
we can imagine future \mbox{homes/apartments} provisioned with a panoply 
of devices evincing a broad spectrum of scientific backgrounds, 
from biology and medicine to ecology and industrial manufacturing.      
}
\p{In this eventuality, individual devices are no longer isolated in 
proprietary circuits linking device signals to databases and 
phone apps, for example, where one company builds each component as 
part of a commercialized network.  Devices are not only being 
connected to their in-house product suite \mdash{} technology inside 
the home is charged with pooling data from many kinds of devices 
into a comprehensive smart home platform, where users can see a 
broad overview, access disparate device data from a central location, 
and where cross-device data can be merged into aggregate models: 
e.g., cross-referencing speech and biometric inputs.  
In this scenario, devices must be designed to broadcast 
data to third-party software platforms.  Smart home \q{hub} 
applications will likely be often upgraded; likewise, home 
owners/renters will likely buy or replace devices fairly 
often, so the precise configuration data senders and 
receivers will dynamically evolve.  These givens call for 
a modular, flexible architecture where a central software 
hub is poised to receive data from different devices as 
they come \q{online}, i.e., exposed on the smart home 
internal network.  Hub applications should seamlessly 
adjust to devices joining as well as exiting the network.       
}
\p{All of this calls for carefully designed protocols, where 
devices do not only expose data but do so in a manner 
conducive to centralized aggregation.  The role of a 
software hub is not only to receive data, but to 
transform multi-domain signals into a common 
graphical presentation, and also to wrangle received data into 
a common format permitting integration algorithms \mdash{} 
e.g. syncing speech and biometrics \mdash{} to operate.  
Received device data must therefore be systematically 
mapped to appropriate transform procedures and 
\GUI{} components.  This is an important point, because 
we are no longer considering data models from the viewpoint 
of devices' own capabilities \mdash{} their specific physical 
measurements and parameters.  Our data models, in short, 
are no longer device-centric.  Instead, data models are 
now assessed in a software-centric milieu: how do we 
route device data to proper interpretive functions?  
How do we consolidate device data into \GUI{} presentations?     
}
\p{Given a \Cpp{} hub application, we might assign a distinct 
\Cpp{} class to each distinct kind of device, so that methods 
on that class are responsible for wrangling device inputs into 
application-specific common representations.  These device-specific 
classes could then be mapped to device-specific \GUI{} components 
displaying device state for human users.  Meanwhile, overarching 
\GUI{} components could be designed to bundle visual components 
from multiple device classes, perhaps using a common base 
class representing \q{graphical device presentations} in the abstract.  
Device data is then routed to distinct \Cpp{} classes and 
then \GUI{} classes.  These Object-Oriented are foregrounded in 
the hub software, but the point is that such software architecture 
retroactively should influence device constructions themselves.  
For devices to be used with an integrated smart home platform, 
the information they broadcast will have to be amenable 
to being processed within a software ecosystem structured 
around Object-Oriented paradigms. 
}
\p{Current literature on CyberPhysical data sharing has focused 
primarily on establishing common formats and Ontologies for 
CyberPhysical information: our energies have been 
invested in stadardized representational paradigms.  But 
common representational formats is only a minimal foundation 
for robust software ecosystems.  I would argue that 
engineers have overemphasized the virtues of standardized 
representations in general, driven perhaps by the press 
surrounding mechanisms like \XML{} or \RDF{}.  There is 
nothing wrong with widely-adopted formats, but how 
data is \i{encoded} is tangential to the primary goal 
of networked software, which is to route shared data 
to the proper procedures and \HCI{} protocols.  In 
a smart home case, once we commit to aggregating devices 
via a software hub, the key organizing principle is 
a mesh of procedures implemented in the hub application 
that can pool all relevant devices into one information 
space.  What needs to be standardized then are not 
so much data \i{formats}, but data-handling 
\i{procedures}.  
}
\p{To put it differently, device manufacturers would now be dealing 
with an ecosystem in which hub applications receive and 
aggregate their data, providing users access points to and 
overviews of device data and state.  Hub applications may be 
provided by different companies and iterations, 
their inner workings opaque to devices themselves.  What 
can be standardized, however, are the procedures 
implemented within hub software to receive and properly 
act upon device data.  Software might guarantee, for example, 
that so long as devices are supplying signals in 
their documented formats, the software has capabilities 
to receive the signals, unpack the data, and interally 
represent the data in a manner suitable for device particulars.  
Devices can then specify what kind of internal representations 
are appropriate for their specific data, essentially 
specifying conditions on software procedures and data types.  
}
\p{In short, the key units of mutual trust and verification among 
and between CyberPhysical devices and CyberPhysical software 
is not, in theory, data structures themselves, but instead 
\i{procedures} for processing relevant data structures.  
Robust CyberPhysical ecosystems can be developed by 
reinforcing procedural alignment wherever possible, 
including by curating substantial collections of 
reusable software libraries, either for direct 
application or as prototypes and testing tools.  
Suppose many CyberPhysical sensors were paired 
with open-source code libraries which illustrate how 
to process the data each device broadcasts.  
Commercial products could use those libraries directly, or, 
if they want to substitute closed-source alternatives, 
could be required to document that their data management 
emulates the open-source prototypes.  Test suites and 
testing technology can then be implemented against the 
open-source libraries and reused for stress-testing 
analogous proprietary components.  This appears to be 
the most likely path to ensuring interoperable, 
high-quality CyberPhysical technology that 
serves the ultimate goal of integrated Smart Home 
(and Smart City, etc.) solutions. 
}
\p{Nevertheless, engineers have not apparently adopted 
such a procedure-focused and software-centric paradigm 
to this point.  There are a lot more academic papers on 
CyberPhysical Ontologies or common signal/message formats, 
like \CoAP{} and \MQTT{}, than there are open-source libraries 
which prototype device data, its validation, parameters, 
and proper transformations.\footnote{The rationale for emphasizing standard data formats is 
probably these formats constrain any procedure 
which operates on the data, so standardization in 
data representation indirectly leads to standardization 
in data-management procedures, or what I am calling 
\q{procedural alignment}.  This accommodates the fact 
that shared data may be used in many different 
software environments \mdash{} components implemented in 
different programming languages and coding styles.   
However, more detailed 
guarantees can be engineered by grounding standardization 
on procedures rather than data representations themselves.  
To accommodate multiple programming languages and paradigms, 
data models can be supplemented with a 
\q{reference implementation} which prototypes 
proper behavior \visavis{} conformant data; components 
in different languages can then emulate the prototype, 
serving both as an implementation guide and a 
criterion for other developers to accept a new 
implementation as trustworthy.  There are several examples 
of a reference implementation used as a standardizing 
tool, analogous to an Ontology, such as the \librets{} 
(Real Estate Transaction Standard) library, 
servers and clients for \FHIR{} (Fast Healthcare Interoperability 
Resources), and clients for \DICOM{} (Digital Imaging and 
Communications in Medicine), e.g. for Whole Slide Imaging 
(see \url{https://www.orthanc-server.com/static.php?page=wsi}).
}  In lieu of \q{data-centric} 
Ontologies, whose mission is to standardize data 
\i{representation}, here I will consider \q{Procedural} 
Ontologies which specify procedural capabilities and 
requirements that indicate whether software components 
are properly managing CyberPhysical data.  The 
idea is that proving procedural conformance should be a 
central step, and an organizing groundwork, for 
showing that software due for production deployment 
is trustworthy and complies to technical and legal specifications.
}
\p{To make this discussion more concrete, I will examine in 
more detail the specific case of speech and language 
data structures.
}
\subsubsection{Linguistic Case-Study}
\p{Establishing data models for deployed technology 
is certainly part of the Research and Development 
cycle, which means that data profiles tend 
to emerge within the scientific process of 
formulating and refining technical and algorithmic 
designs.  This is particularly true for complex, 
computational nuanced processes like image segmentation 
or (to cite one above example) measuring heartbeats 
and breathing patterns via subtle waveform analyses.  
We can assume that each \RnD{} phase will itself leave 
behind an ecosystem of testing data and code which 
can be decisive for consolidating data models, directly 
or indirectly influencing production code for systems 
(even if their deployment and commercialization is 
well after the \RnD{} period).       
}
\p{In the case of speech and language technology, 
a research-oriented data infrastructure has been 
systematically curated, in several subdisciplines, 
by academic or industry collaborations.  The 
Conference on Natural Language Learning (\CoNLL{}), for 
example, invites participants to develop 
Natural Language Processing techniques targeted at 
a common \q{challenge} dataset, updated each year.  These 
data sets, along with the data formats and code libraries 
which allow software to use the data, thereby serve 
as a reference-point for Computational Linguistics 
researchers in general.  Similarly, this volume's 
chapter on Language Identification describes research 
targeting a multilingual data set curated for an 
annual \q{Oriental Language Challenge} conference 
dedicated to language/dialect classification for 
languages spoken around East Asia (from  
East Asian language families and also Russian).  
}
\p{Technically, curated and publicly accessible data 
sets are a different genre of information space than 
real-time data generated by CyberPhysical technology 
(e.g. voices picked up by microphones in a Smart Home).  
That is to say, sofware developed to access speech 
and language data sets like the \CoNLL{}'s or the 
Oriental Language Challenge has different requirements 
than software responding to voice requests in real time 
\mdash{} or other deployment use-cases, such as medical 
transcription, or identifying dialects spoken in an 
urban community.  However, data models derived from 
publicly shared test corpora \i{do} translate over 
to realtime data: we can assume that \RnD{} 
data sets are collections of signals or information 
granules which are structurally similar to those 
produced by operating CyberPhysical devices.  
As a result, portions of the software targeting 
\RnD{} data sets \mdash{} specifically, the procedures 
for acquiring, transforming, validating, and 
interactively displaying individual samples \mdash{} 
remain useful as component or prototypes for 
deployed product.  Code libraries used in 
\RnD{} cycles should typically be the basis 
for data models guiding the implementation of 
production software. 
}
\p{To make this discussion more concrete, I will use 
the example of \CoNLL{} data sets.  This chapter's 
demo includes samples from the most recent 
collection of \CoNLL{} files and 
conference challenge tasks (at the time of writing) 
as well as demo code which operates on this 
data via techniques described in this chapter.  
The \CoNLL{} format is representative of the 
kinds of linguistic parsing requisite for 
using Natural Language content (such as 
speech input) in CyberPhysical settings.
}
\p{The chapter by Ant\'onio Teixera  and others on 
\q{Natural (Language) Interaction} with 
Smart Homes (this volume) considers voice-activated 
CyberPhysical interfaces, where Natural Language 
segments become the core elements in translating 
user queries to actionable software responses.  The proposed 
systems analyze speech patterns to build textual 
reconstructions of speakers' communications, then 
parses the text as Natural Language content, before 
eventually (if all goes well) interpreting the 
parsed and analyzed text as an instruction the 
software can follow.  Text data can then be 
supplemented with metrics measuring vocal 
patterns, syntactic and semantic information, speaker's 
spatial location, and other information that can 
help interpret speakers' wishes insofar as software 
can respond to them.  
}
\p{The authors discuss, for instance, the possibility 
of annotating language samples (after speech-to-text 
translation) with Dependency Grammar parses.\footnote{Adequately describing Dependency Grammar
is outside the scope of this chapter, but, in a nutshell, 
Dependency Grammar models syntax in terms of 
word-to-word relations rather than via phrase 
hierarchies; as such, Dependency parses yields 
directed, labeled graphs (node labels are words and 
edge labels are drawn from an inventory of 
syntactic inter-word connections),  
which are structurally similar to Semantic Web 
graphs.  See also \cite{DebusmannThesis}, 
\cite{KongRushSmith}, \cite{Nivre},
\cite{OsborneMaxwell}, \cite{SivaReddy}, 
\cite{Schneider}, \cite{Schneider}, \cite{XiaPalmer}, etc. 
}  Textual content can also be annotated with 
models of intonation, stress patterns, and other 
acoustic features (because the original inputs are 
audio-based) that can help an \NLP{} engine to 
properly parse sentences (for instance by 
noting which words or syllables are stressed).  
So, in the context of integrated Smart Home 
hub software (continuing the above discussion), 
this is the kind of data which would be transmitted to 
a hub application.  We can assume that audio processing 
as well as \NLP{} technology would supply intermediary 
processing somewhere between acoustic devices and 
the centralized application. 
}
\p{Different kinds of linguistic details require different 
data models.  Dependency parses, for instance, 
are often notated via some version of a specialized 
\CoNLL{} format, which textually serializes parse and 
lexical data, usually one word per line.  
The most recent standard (dubbed CoNNL-U) recognizes 
ten fiels for each word, identifying, in particular, 
Parts of Speech and syntactic connections with other 
words.  As a cutom format, CoNNL-U requires its 
own parser, such as the \UDPipe{} library for 
\Cpp{} (a slightly modified version of this library 
is published with this chapter's data set).  
For hub applications, then, a reasonable assumption 
is that they can compile in the \UDPipe{} library 
or an alternative with similar capabilities, 
in order for them to handle parsed \NLP{} data.
}
\p{Suppose, then, that a Smart Home speech-technology 
product suite bundled audio-capture devices with 
software that can perform dependency parsing, perhaps 
after training against users' speech and 
language patterns.\footnote{Users in this context meaning homeowners or 
other people expected often to be in the 
home: renters, children, health aides, etc.
}  The \NLP{} components can be 
bundled with the acoustic devices, so that complex 
\NLP{} code is isolated in its own software environment.  
Smart Home applications would not then compile 
\NLP{} capabilities directly; instead, \NLP{} features would 
be provisioned by a distinct program receiving audio input and 
generating text and parse transcriptions, which would 
subsequently be sent 
to hub applications, in lieu of raw device data.  
Let us assume that this architecture is in effect.
}
\p{A hub application will, then, periodically receive a 
data package comprising an audio sample along with 
text transcriptions and \NLP{}-generated, e.g., 
\CoNLLU{} data.  To make sense of linguistic content, 
the software would presumably pair the \NLP{}-specific 
information with extra details, such as, 
the identity of the speaker (if a Smart Home system 
knows of specific users), where and when each sentence 
or request was formulated, and perhaps the original 
audio input (allowing functionality such as users playing 
back instructions they utterred in the past).  A relevant 
data model might thereby comprise: (1) \CoNLLU{} data 
itself; (2) location info, such as spatial position and 
which room a speaker is found in; (3) timestamps; 
(4) speaker info, if available; (5) audio files; 
and maybe (6) extra acoustical or intonation data. 
Extra data could include annotations based on how 
conversation analysts notate speech patterns, or 
might be acoustic features derived from initial 
processing of speech samples.   
}
\p{This data model would presumably translate to muliple 
data types: we can envision (1) a class for \UDPipe{} 
sentences obtained from \CoNLLU{} (2) a class for audio 
samples; (3) speaker and time/location information; plus 
versions of these classes appropriate for 
\GUI{}s and database persistence.  And, in addition, 
these data requirements for speech and text samples only 
considers obtaining a valid parse for the text; 
to actually react to speech input, an application would 
need to map lexical data to terms and actions the software 
itself can recognize.  For instance, \i{close the window} 
would map to an identifier of which window is intended 
(inferred prehaps from speaker location) and a 
\i{close} operation, which could be available via 
actuators embedded in the window area.  All of the 
objects which users might semantically reference 
in voice commands therefore need their own data 
models, which are interoperable with linguistic 
parses.  So along with data types specific to 
linguistic elements we can consider \q{bridge} types 
which connect linguistic data (e.g., lexemes) to 
data types modeling physical objects themselves.  
}
\p{Likewise, we 
can anticipate the procedures which speech and/or language 
data types need to implement: correctly decoding 
\CoNLLU{} files; mapping time/location data points to a 
spatial model of the Smart Home (which room is 
targeted by the location and also if that point is 
close to a door, window, appliance, and so forth; 
and perhaps matching the location to a 3D graphics 
model for user visualization); audio-playback procedures, 
along with interactive protocols for this process such 
as users pausing and restarting playback; procedures 
to map identified speakers to user profiles known to 
the Smart Home system.  The audio devise makers and 
\NLP{} providers \mdash{} assuming those products are 
delivered as one suite separate and apart from the 
Smart Home hub \mdash{} can mandate that hub applications 
demonstrate procedural implementations that satisfy 
these requirements as a precondition for accessing 
their broadcast data.  Conversely, hub applications can 
stipulate the procedural mandates they are prepared 
to honor as a guide to how devices and their drivers 
and middleware components should be configured for 
an integrated Smart Home ecosystem. 
}
\p{The essential point here is that procedural requirements 
and validation becomes the essential glue 
that unifies the diverse Smart Home components, and 
allows products designed by different companies, 
with different goals, to become interoperable.  
Once again, procedural alignment and predictability is 
more important than standardized data formats.   
}
\p{We can also consider representative criteria for 
testing procedures; for instance, preconditions which 
procedures need to recognize.  In \CoNLLU{}, 
individual words can be extracted from a parse-model, 
but the numeric index for the word must fall within a 
fixed range (based on the word count for the 
relevant sentence).  In audio playback, time intervals 
are only meaningful in the context of the length of 
the audio sample in (e.g.) seconds or milliseconds.  
Similarly, features extracted from an audio 
sample (of human speech or, say, a bird song) are localized 
by time points which have to fit within a sample window; 
and image features are localized in rectangular coordinates 
that need to fit within the surrounding image.  
Therefore, procedures engaged with these data 
structures should be checked to ensure that they 
honor these ranges and properly respond to 
faulty data outside them.  This is an example 
of the kind of localized procedural testing which, 
cumulatively, establishes software as trustworthy.
}
\p{I will discuss similar procedural-validity issues 
for the remainder of this section before 
developing more abstract or theoretical models 
of procedures, as formal constructions, subsequently 
in the chapter. 
}
\input{profiles}
%\spsubsection{Proactive Design}
\subsection{Proactive Design}
\p{I have thus far argued that applications
which process CyberPhysical data need to rigorously organize their functionality
around specific devices' data profiles.  The procedures that directly interact
with devices \mdash{} receiving data from and perhaps sending instructions
to each one \mdash{} will in many instances be \q{fragile} in the sense
I invoke in this chapter.  Each of these procedures may make assumptions
legislated by the relevant device's
specifications, to the extent that using any one procedure too broadly
constitutes a system error.  Furthermore, CyberPhysical devices may
exhibit errors due to mechanical malfunction, hostile attacks,
or one-off errors in electrical-computing operations, causing
performance anomalies which look like software mistakes even if the code is
entirely correct (see \cite{MichaelEngel} and
\cite{LavanyaRamapantulu}, for example).  As a
consequence, \i{error classification} is especially
important \mdash{} distinguishing kinds of software errors
and even which problems are software errors to begin with.
}
\p{Summarizing the case studies from earlier in this section, 
Table~\ref{table:profiles} identifies several details 
about dimensions, parameters of operation, data fields, 
and other pieces of information relevant to 
implementing procedures and data types capturing 
CyberPhysical data.  These types may derive from 
CyberPhysical input directly or may model artifacts 
constructed from CyberPhysical input midstream, such 
as audio or image files, or text transcriptions 
representing speech input.  The summaries are not 
rigorous data models, but are just suggestive cues 
about what sort of details engineers should consider 
when formalizing data models.  In general, detailed 
models should be defined for any input source 
(including those transformed by middleware 
components, such as \NLP{} engines), thereby 
profiling both CyberPhysical devices and also 
\q{midstream} artifacts such as audio or image files 
\mdash{} i.e., aggregates, derived from CyberPhysical 
input, that can be shared between software components 
(within hub applications and/or between hubs and 
middleware).
}
\p{These data profiles need to be integrated with CyberPhysical code from a
perspective that cuts across multiple dimensions of project scale and
lifetime.  Do we design for biaxial or triaxial accelerometers, or both,
and may this change?  Is heart rate to be sampled in a context where
the range considered normal is based on \q{resting} rate or is it
expanded to factor in subjects who are exercising?  These kinds
of questions point to the multitude of subtle and project-specific
specifications that have to be established when implementing and then
deploying software systems in a domain like Ubiquitous Computing.
It is unreasonable to expect that all relevant standards will be
settled \i{a priori} by sufficiently monolithic and comprehensive
data models.  Instead,
developers and end-users need to acquire trust in a development process
which is ordered to make standardization questions become apparent
and capable of being followed-up in system-wide ways.
}
\p{For instance, the hypothetical questions I pondered in
the last paragraph \mdash{} about biaxial vs.
triaxial accelerometers and about at-rest vs. exercise
heart-rate ranges \mdash{} would not
necessarily be evident to software engineers or project architects when the
system is first conceived.  These are the kind of modeling questions that tend
to emerge as individual procedures and datatypes are
implemented.  For this reason, code development serves a role beyond just
concretizing a system's deployment software.
The code at fine-grained scales also reveals questions that need to be
asked at larger scales, and then the larger answers reflected back in the
fine-grained coding assumptions, plus annotations
and documentation.  The overall
project community needs to recognize software implementation as a crucial
source for insights into the specifications that have to be established
to make the deployed system correct and resilient.
}
\p{For these reasons, code-writing \mdash{} especially at the smallest scales \mdash{}
should proceed via paradigms disposed to maximize
the \q{discovery of questions} effect
(see also, as a case study, \cite[pages 6-10]{Arantes}).  Systems in operation will be
more trustworthy when and insofar as their software bears witness to a project
evolution that has been well-poised to unearth questions
that could otherwise diminish the system's trustworthiness.
Lest this seem like common sense and unworthy of being emphasized
so lengthily, I'd comment that literature on Ubiquitous Sensing for 
Healthcare (\USH{}), for
example, appears to place much greater emphasis on Ontologies or Modeling
Languages whose goal is to predetermine software design at such
detail that the actual code merely enacts a preformulated schema,
rather than incorporate subjects (like type Theory and
Software Language Engineering) whose insights can
help ensure that code development plays a more proactive role.
}
\p{\q{Proactiveness}, like transparency and trustworthiness, has been
identified as a core \USH{} principle, referring (again in
the series intro, as above)
to \q{data transmission to healthcare providers
... \i{to enable necessary interventions}} (my emphasis).  In
other words \mdash{} or so this language implies, as an
unstated axiom \mdash{}
patients need to be confident in deployed \USH{} products
to such degree that they are comfortable with clinical/logistical
procedures \mdash{} the functional design of medical spaces; decisions about
course of treatment \mdash{} being grounded in part on data generated from
a \USH{} ecosystem.  This level of trust, or so I would argue,
is only warranted if patients feel
that the preconceived notions of a \USH{} project have been vetted against
operational reality \mdash{} which can happen through the interplay between
the domain experts who germinally envision a project and the programmers
(software and software-language engineers) who, in the end, produce its
digital substratum.
}
\p{\q{Transparency} in this environment means that \USH{} code needs
to explicitly declare its operational assumptions, on the
zoomed-in procedure-by-procedure scale, and also exhibit its
Quality Assurance strategies, on the zoomed-out system-wide scale.  It
needs to demonstrate, for example, that the code base has sufficiently
strong typing and thorough testing that devices are always matched to
the proper processing and/or management functions: e.g., that there are no
coding errors or version-control mismatches which might cause situations
where functions are assigned to the wrong devices, or the wrong
versions of correct devices.  Furthermore, insofar as most \USH{} data
qualifies as patient-centered information that may be personal and
sensitive, there needs to be well-structured transparency concerning
how sensitive data is allowed to \q{leak} across the system.  Because
functions handling \USH{} devices are inherently fragile,
the overall system needs extensive and openly documented
gatekeeping code that both validates their input/output and controls
access to potentially sensitive patient data.
}
\thindecoline{}
\p{Fragile code is not necessarily a sign of poor design.  Sometimes
implementations can be optimized for special circumstances, and
optimizations are valuable and should be used wherever possible.  Consider an
optimized algorithm that works with two lists that must be the same size.
Such an algorithm should be preferred over a less efficient
one whenever possible \mdash{} which is to say, whenever dealing with two
lists which are indeed the same size.  Suppose this algorithm is
included in an open-source library intended to be shared among many different
projects.  The library's engineer might, quite reasonably, deliberately
choose not to check that the algorithm is invoked on same-sized lists
\mdash{} checks that would complicate the code, and sometimes slow the
algorithm unnecessarily.  It is then the responsibility of code that
\i{calls} whatever procedure implements the algorithm to ensure that it
is being employed correctly \mdash{} specifically, that this
\q{client} code does \i{not} try
to use the algorithm with \i{different-sized} lists.  Here \q{fragility} is
probably well-motivated: accepting that algorithms are sometimes
implemented in fragile code can make the code cleaner, its intentions
clearer, and permits their being optimized for speed.
}
\p{The opposite of fragile code is sometimes called \q{robust} code.
While robustness is desirable in principle, code which simplistically
avoids fragility may be harder to maintain than deliberately fragile but
carefully documented code.  Robust code often has to check for many
conditions to ensure that it is being used properly, which can make
the code harder to maintain and understand.  The hypothetical
algorithm that I contemplated last paragraph
could be made robust by \i{checking}
(rather than just \i{assuming}) that it is invoked with same-sized lists.
But if it has other requirements \mdash{} that the lists are non-empty,
and so forth \mdash{} the implementation can get padded with a chain of
preliminary \q{gatekeeper} code.  In such cases the gatekeeper
code may be better factored into a different procedure, or expressed
as a specification which engineers must study before attempting to
use the implementation itself.
}
\p{Such transparent declaration of coding assumptions and specifications can
inspire developers using the code to proceed attentively,
which can be safer in the long run than trying to avoid fragile code
through engineering alone.  The takeaway is that while \q{robust} is
contrasted with \q{fragile} at the smallest scales (such as
a single procedure), the overall goal is systems and components that are robust at the
largest scale \mdash{} which often means accepting \i{locally} fragile
code.  Architecturally, the ideal design may combine
individual, \i{locally fragile} units with rigorous documentation and gatekeeping.
So defining and declaring specifications is
an intrinsic part of implementing code bases which are both robust
and maintainable.
}
\p{Unfortunately, specifications are often created
only as human-readable documents, which might have a semi-formal
structure but are not actually machine-readable.
There is then a disconnect between features \i{in the code itself} that
promote robustness, and specifications intended for \i{human} readers
\mdash{} developers and engineers.  The code-level and
human-level features promoting robustness will tend to overlap partially
but not completely, demanding a complex evaluation of where gatekeeping
code is needed and how to double-check via
unit tests and other post-implementation examinations.  This is the
kind of situation \mdash{} an impasse, or partial but incomplete overlap,
between formal and semi-formal specifications \mdash{} which many programmers
hope to avoid via strong type systems.
}
\p{Most programming language will provide some basic (typically relatively
coarse-grained) specification semantics, usually
through type systems and straightforward code observations
(like compiler warnings about unused or uninitialized variables).
For sake of discussion, assume that all languages have distinct
compile-time and run-time stages (though these may be opaque to
the codewriter).  We can therefore distinguish compile-time
tests/errors from run-time tests and errors/exceptions.
Via Software Language Engineering, we can study
questions like: how
should code requirements be expressed?  How and to
what extent should requirements be tested by the language
engine itself \mdash{} and beyond that how can the language help coders implement
more sophisticated gatekeepers than the language natively offers?
What checks can and should be compile-time or run-time?  How
does \q{gatekeeping} integrate with the overall semantics and
syntax of a language?
}
\p{Given the maxim that
procedures should have single and narrow roles \mdash{} \q{separation 
of concerns} \mdash{} note that \i{validating} input
is actually a different role than \i{doing} calculations.  This is 
why procedures with fine requirements might be split into two: a
gatekeeper that validates input before a fragile procedure is called,
separate and apart from that procedure's own implementation.
A related idea is overloading fragile procedures: for example,
a function which takes one value can be overloaded in terms
of whether the value fits in some prespecified range.  These two
can be combined: gatekeepers can test inputs and call one of several
overloaded functions, based on which overload's specifications are
satisfied by the input.
}
\p{But despite their potential elegance, mainstream programming languages
do not supply much language-level support for expressing
groups of fine-grained functions along these lines.  Advanced 
type-theoretic constructs \mdash{} including Dependent Types,
typestate, and effect-systems \mdash{} model requirements with more precision
than can be achieved via conventional type systems alone.  Integrating these
paradigms into core-language type systems permits data validation 
to be integrated with general-purpose type checking, without the need for
static analyzers or other \q{third party} tools (that is, projects maintained
orthogonally to the actual language engineering; i.e., to
compiler and runtime implementations).  Unfortunately, 
these advanced type systems are also more complex to implement.  
If software language engineers aspire to make Dependent Types and 
similar advanced constructs part of their core language, 
creating compilers and runtime engines for these languages 
becomes proportionately more difficult.
} 
\p{If these observations are correct, I maintain that it is a worthwhile
endeavor to return to the theoretical drawing board, with the goal 
of improving programming language technology itself.  
Programming languages are, at one level, artificial 
\i{languages} \mdash{} they allow humans to communicate 
algorithms and procedures to computer processors, and 
to one another.  But programming languages are also 
themselves engineering artifacts.  It is a complex
project to transform textual source-code \mdash{} which is 
human-readable and looks a little bit like natural 
language \mdash{} into binary instructions that computers 
can execute.  For each language, there is a stack 
of tools \mdash{} parsers, compilers, and/or runtime libraries 
\mdash{} which enable source code to be executed 
according to the language specifications.  
Language design is therefore constrained by 
what is technically feasible for these supporting tools.  
Practical language design, then, is an interdisciplinary 
process which needs to consider both the dimension of 
programming languages as communicative media and 
as digital artifacts with their own engineering challenges 
and limitations.
}
%\spsubsection{Core Language vs. External Tools}
\subsection{Core Language vs. External Tools}
\p{Because of programming languages' engineering limitations, 
such as I just outlined, software projects should not 
necessarily rely on core-language features for 
responsible, safety-conscious programming.
Academic and experimental languages tend to have 
more advanced features, and to embody more 
cutting-edge language engineering, compared to mainstream 
programming languages.  However, it is not always feasible 
or desirable to implement important software with 
experimental, non-mainstream languages.  By their nature, 
such projects tend to produce code that must be understood by 
many different developers and must remain usable years into 
the future.  These requirements point toward 
well-established, mainstream languages \mdash{} and 
mainstream development techniques overall \mdash{} as opposed to 
unfamiliar and experimental methodologies, even if those 
methodologies have potential for safer, more productive 
coding in the future.   
}
\p{In short, methodologies for safety-conscious coding can be 
split between those which depend on core-language features, 
and those which rely on external, retroactive analysis 
of sensitive code.  On the one hand, some languages and projects
prioritize specifications that are intrinsic to the language and integrate
seamlessly and operationally into the language's foundational
compile-and-run sequence.  Improper code (relative to specifications)
should not compile, or, as a last resort, should fail gracefully at run-time.
Moreover, in terms of programmers' thought processes, the
description of specifications should be intellectually continuous
with other cognitive processes involved in composing code, such
as designing types or implementing algorithms.  For sake of 
discussion, I will call this paradigm \q{internalism}.  
}
\p{The \q{internalist} mindset seeks to integrate data 
validation seamlessly with other language features.  
Malformed data should be flagged via similar mechanisms 
as code which fails to type-check; and errors should 
be detected as early in the development process as possible.   
Such a mindset is evident in passages like this (describing
the Ivory programming language):
\begin{dquote}Ivory's type system is shallowly embedded within Haskell's
type system, taking advantage of the extensions provided by [the
Glasgow Haskell Compiler].  Thus, well-typed Ivory programs
are guaranteed to produce memory safe executables, \i{all without
writing a stand-alone type-checker} [my emphasis].  In contrast, 
the Ivory syntax is \i{deeply} embedded within Haskell.
This novel combination of shallowly-embedded types and 
deeply-embedded syntax permits ease of development without sacrificing
the ability to develop various back-ends and verification tools [such as]  
a theorem-prover back-end.  All these back-ends share the
same AST [Abstract Syntax Tree]: Ivory verifies what it compiles.
\cite[p. 1]{ivory}.
\end{dquote}   In other words, the creators of Ivory are promoting the
fact that their language buttresses via its type system 
\mdash{} and via a mathematical precision suitable for 
proof engines \mdash{} 
code guarantees that for most languages require external
analysis tools.
}
\p{Contrary to this \q{internalist} philosophy, other approaches
(perhaps I can call them \q{externalist}) favor a neater separation
of specification, declaration and testing from the core language,
and from basic-level coding activity.  In particular \mdash{} according to 
the \q{externalist} mind-set \mdash{} most of the more important or complex
safety-checking does not natively integrate with the
underlying language, but instead requires
either an external source code analyzer, or 
regulatory runtime libraries, or some combination of the two.  
Moreover, it is unrealistic
to expect all programming errors to be avoided with enough proactive planning,
expressive typing, and safety-focused paradigms: any complex
code base requires some retroactive design, some combination
of unit-testing and mechanisms (including those
third-party to both the language and the projects whose code is
implemented in the language) for externally
analyzing, observing, and higher-scale testing for the code,
plus post-deployment monitoring.
}
\p{As a counterpoint to the features cited as benefits to the
Ivory language, which I identified as representing the 
\q{internalist} paradigm, consider Santanu Paul's Source Code Algebra (\SCA{})
system described in \cite{SantanuPaul} and
\cite{GiladMishne}, \cite{TillyEtAl}:
\begin{dquote}Source code files are processed using
tools such as parsers, static analyzers, etc. and the necessary information
(according to the SCA data model) is stored in a repository.  A user interacts
with the system, in principle, through a variety of high-level languages, or
by specifying SCA expressions directly.  Queries are mapped to SCA expressions,
the SCA optimizer tries to simplify the expressions, and finally, the SCA
evaluator evaluates the expression and returns the results to the user.\nl{}
We expect that many source code queries will be expressed using high-level
query languages or invoked through graphical user interfaces.  High-level queries
in the appropriate form (e.g., graphical, command-line, relational, or
pattern-based) will be translated into equivalent SCA expressions.  An SCA
expression can then be evaluated using a standard SCA evaluator, which
will serve as a common query processing engine.  The analogy from
relational database systems is the translation of SQL to expressions based on
relational algebra. \cite[p. 15]{SantanuPaul}
\end{dquote}
So the \i{algebraic} representation of source code is favored
here because it makes computer code available
as a data structure that can be processed via \i{external}
technologies, like \q{high-level languages}, query languages, and
graphical tools.  The vision of an optimal development environment
guiding this kind of project is opposite, or at least
complementary, to a project like Ivory: the whole point
of Source Code Algebra is to pull code verification \mdash{} the
analysis of code to build trust in its safety and robustness
\mdash{} \i{outside} the language itself and into the surrounding
Development Environment ecosystem.
}
\p{These philosophical differences (what I dub \q{internalist} vs. \q{externalist}) 
are normative as well as descriptive:
they influence programming language design, and how languages in turn influence
coding practices.  One goal of language design is to produce languages 
which offer rigorous guarantees \mdash{} fine-tuning the languages' 
type system and compilation model to maximize the level of detail 
guaranteed for any code which type-checks and compiles.  
Another goal of language design is to define syntax and 
semantics permitting valid source code to be analyzed 
as a data structure in its own right.  Ideally, 
languages can aspire to both goals.  In practice, however, 
achieving both equally can be technically difficult.  
The internal representations conducive to strong type and 
compiler guarantees are not necessarily amenable to 
convenient source-level analysis, and vice-versa.    
}
\p{Language engineers, then, have to work with
two rather different constituencies.  One community of
programmers tends to prefer that specification and validation be
integral to/integrated with the language's type system and
compile-run cycle (and standard runtime environment); whereas
a different community prefers to treat code evaluation
as a distinct part of the development process, something logically, operationally,
and cognitively separate from hand-to-screen codewriting
(and may chafe at languages restricting certain code constructs
because they can theoretically produce coding errors, even when
the anomalies involved are trivial enough to be tractable for
even barely adequate code review).  One challenge for language engineers is
accordingly to serve both communities.  We can, for example, aspire to
implement type systems which are sufficiently
expressive to model many specification, validation, and
gatekeeping scenarios, while also anticipating that language code
should be syntactically and semantic designed to be
useful in the context of external tools (like
static analyzers) and models (like Source Code
Algebras and Source Code Ontologies).
}
\p{The techniques I discuss here work toward these goals on two levels.  First, I
propose a general-purpose representation of computer code in terms
of Directed Hypergraphs, sufficiently rigorous to codify a
theory of functional types as types whose values are (potentially) initialized from
formal representations of source code \mdash{} which is to say, in the present
context, code graphs.  Next, I
analyze different kinds of \q{lambda abstraction} \mdash{} the idea of
converting closed expressions to open-ended formulae by asserting that
some symbols are \q{input parameters} rather than fixed values, as in
Lambda Calculus \mdash{} from the perspective of
axioms regulating
how inputs and outputs may be passed to and obtained from
computational procedures.  I bridge these topics \mdash{} Hypergraphs
and Generalized Lambda Calculi \mdash{} by taking abstraction as a
feature of code graphs wherein some hypernodes are singled out
as procedural
\q{inputs} or \q{outputs}.  The basic form of this model
\mdash{} combining what are essentially two otherwise unrelated
mathematical formations, Directed Hypergraphs and
(typed) Lambda Calculus \mdash{} is laid out in
Sections \sectsym{}\hyperref[sTwo]{\ref{sTwo}}
and \sectsym{}\hyperref[sThree]{\ref{sThree}}.
}
\p{Following that sketch-out, I engage a more rigorous study of
code-graph hypernodes as \q{carriers} of runtime values, some of
which collectively form \q{channels} concerning values which
vary at runtime between different executions of a function body.
Carriers and channels piece together to form
\q{Channel Groups} that describe structures with meaning both
within source code as an organized system (at \q{compile time}
and during static code analysis) and at runtime.  Channel Groups
have four different semantic interpretations, varying via the
distinctions between runtime and compile-time and between
\i{expressions} and (function) \i{signatures}.
I use the framework of Channel Groups to identify
design patterns that achieve many goals of
\q{expressive} type systems while being implementationally
feasible given the constraints of mainstream programming
languages and compilers (with an emphasis on \Cpp{}).
}
\p{After this mostly theoretical prelude, I conclude this
chapter with a discussion of code annotation, particularly
in the context of CyberPhysical Systems.  Because CyberPhysical applications
directly manage physical devices, it is especially important that they be
vetted to ensure that they do not convey erroneous instructions
to devices, do not fail in ways that leave devices uncontrolled, and
do not incorrectly process the data obtained from devices.
Moreover, CyberPhysical devices are intrinsically \i{networked},
enlarging the \q{surface area} for vulnerability, and often worn by people
or used in a domestic setting, so they tend carry personal (e.g., location)
information, making network security protocols especially important
(\cite{RonaldAshri}, \cite{LalanaKagal}, \cite{AbhishekDwivedi},
\cite{TakeshiTakahashi}, \cite{BhavaniThuraisingham},
\cite{MozhganTavakolifard}).  The dangers
of coding errors and software vulnerabilities, in CyberPhysical
Systems like the Internet of Things (\IoT{}), are even more pronounced
than in other application domains.  While it is
unfortunate if a software crash causes someone to lose data,
for example, it is even more serious if a CyberPhysical \q{dashboard}
application were to malfunction and leave physical, networked
devices in a dangerous state.
}
\p{To put it differently, computer code which directly interacts with
CyberPhysical Systems will typically have many fragile pieces, which
means that applications providing user portals to maintain and control
CyberPhysical Systems need a lot of gatekeeping code.  Consequently,
code verification is an important part of preparing CyberPhysical Systems
for deployment.  The \q{Channelized Hypergraph} framework I develop here
can be practically expressed in terms of code annotations that benefit
code-validation pipelines.  This use case is shown in demo code
published as a data set alongside this chapter (available for
download at \url{https://github.com/scignscape/ntxh}).
These techniques are not designed to substitute for Test Suites or
Test-Driven Development,
though they can help to clarify the breadth of coverage of
a test suite \mdash{} in other
words, to justify claims about tests being thorough enough that
the code base passing all tests actually does argue for the code
being safe and reliable.  Nor are code annotations intended to
automatically verify that code is safe or
standards-compliant, or to substitute for
more purely mathematical code analysis using proof-assistants.
But the constructions presented here,
I claim, can be used as part of a
code-review process that will enhance stakeholders' trust
in safety-critical computer code, in cost-effective, practically
effective ways.
}
