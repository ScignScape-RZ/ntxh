`section.Conclusion`

`p.
Both type theory and Semantic Web style Ontologies pose 
fundamental questions about data modeling %-- about 
how digitized data structures can capture the nuances 
and detail of human and scientific concepts.  Ideally, 
data models are expressive enough to represent 
concepts and artifacts, drawn from our cultural and 
scientific domains, without any sense of conceptual 
mismatch or simplification; but at the same time 
work in a software ecosystem, where data structures have 
sufficient predictability and classification that they 
are amenable to algorithms and mutations to accommodate 
different software roles, such as database and 
`GUI; presentations.  Software users should not feel as 
if they have to wrangle real-world data into awkward 
formats, in order to introduce infomation they deem 
worthy of being shared and studied into a digital 
ecosystem.  On the other hand, digital resources 
should have enough structure and planning to be 
accessible to high-quality software, with optimized 
User Interface design and responsiveness, as well 
as trustworthy safety and privacy features.    
`p`

`p.
Achieving all of these goals involves a certain balancing 
act, where data repositories are modeled via expressive, 
fine-grained prototypes without becoming too unstructured, 
or too heterogeneous, for rigorous software implementations.  
The technical terrain of Ontology-based or type-theoretic 
modeling can therefore be seen as a drive to 
expand models' expressiveness as far as possible, but without 
losing models' underlying formal rigor and tractability.  
In terms of data models, this can be reflected in the 
evolution from fixed-field structures (like spreadsheets and relational 
databases) to labeled-graph Ontologies to Hypergraphs and 
other multi-scale graph paradigms.  Parallel to the emergence 
of Semantic Web technology there is also a body of research in 
Scientific Computing, where expressiveness translates to 
modeling strategy which encapsulate scientific theories and 
workflows %-- cf. Object-Oriented simulations 
(`cite<Telea>;, `cite<TeleaWijk>; being a good case-study) and 
such formats as Conceptual Space Markup Language 
(`cite<RaubalAdams>;, `cite<RaubalAdamsCSML>;, 
`cite<Strle>;, `cite<Zenker>;).
Meanwhile, in type theory, a similar 
impetus leads from the simple type systems of Typed Lambda 
Calaculus through to Dependent Types, typestate, effect systems, 
Object-Orientation, and other features of modern programming 
environments.
`p`

`p.
Whatever their features, data models are ultimately 
only as usable as the software that receives them.  
Applications may be receiving CyberPhysical measurements 
`q.in real time` or affording access to archived 
research data sets, but in each case the structured 
formats of shared and/or persisted information must 
be transformed into interactive, usually `GUI;-based  
presentations for applications to qualify as 
productive viewers onto the relevant information space.  
This is how we should understand 
the criterion of expressiveness: expressiveness at the 
modeling level is a means to an end; the ultimate 
goal is `q.expressive` software, i.e., software whose 
layout, visual presentations, and interactive features/responsiveness 
render applications effective vehicles for interfacing 
with complex, nuanced digital content.
Ultimately, then, data models are effective to the 
extent that they promote effective software engineering 
for the applications that transform modeled data into 
user-facing digital content.  
`p`

`p.
On the other hand, this leaves room for differences in 
what is prioritized: data models can be targeted at 
a narrow, specialized set of software end-points, or 
can be designed flexibly to work with a diversity of 
software products, in the present and going forward.  
Broader application-scope is desirable in theory, 
but practically speaking a data model which is open-ended 
enough to work with a range of software components 
is potentially too provisional, or insufficiently 
detailed, to promote the highest-quality software. 
`p`


`p.
Information Technology in the last one or two decades 
seem to have favored general-purpose data models 
%-- or at least serialization techniques %-- which 
exist in isolation from applications that work 
with them.  Canonical examples would be `JSON;, `XML;, 
and `RDF;.  Conceptually, however, data models' 
most important manifestation are in the software 
components where they are shared %-- sent (perhaps 
indirectly via a generated archive) and received. 
To the degree that multi-purpose formats like 
`XML; are beneficial, there merits are in part that 
developers can anticipate the code that generated 
and/or will receive the data: while programmers do not 
necessarily just write code off of an `XML; sample 
(or corresponding Document Type Declaration), any 
`XML; document or `DTD; gives us a rough idea of 
what its client code would look like. 
`p`

`p.
Nevertheless, for robust software engineering we should 
aspire to something more rigorous than that.  In effect, 
we should consider documentation of components which 
send and/or receive data structures to be an intrinsic 
aspect of rigorous data modeling itself: description of 
the procedures which construct, serialize/deserialize, 
validate, and transform data structures, particularly 
those procedures supplying functionality determinative 
of their components' ability to be part of a 
conformant data-sharing network.  In this
sense data and code modeling coincide.  In 
particular, characterization of individual 
procedures %-- their types, assumptions, and 
requirements %-- is an essential building-block 
of data models generally.  Data structures 
can be indirectly systematized in terms of 
the procedures which act upon them.  
`p`

`p.
With this background, the demo proposes a notion of 
`q.Procedural Hypergraph Ontology` which 
extends (or diverges from) conventional 
Semantic Web Ontology partly by orienting toward 
Hypergraphs, but more substantially by centering 
on this procedutal dimension: the role of an 
Ontology being to desribe components' procedural 
interface as well as their targeted data structures.  
`p`

`p.
In particular, the demo presents both a hypergraph serialization 
format and methodology for generating interface descriptions, 
based on channel complexes.  The demo code shows a compilation process 
which works with channel groups, branching off into a runtime 
engine which actually evaluates channel packages and, separately, 
algorithms to compile information about procedure signatures and 
function calls.  This last capability can be a point for embedding 
more detailed Interface Definition metadata, including via the non-standard 
channel protocols I have discussed in this chapter.  
Both static data structures and compiled channel groups 
translate to a Hypergraph format, which thereby serves as 
a common denominator between code and data.  
`p`

`p.
Rigorous procedural documentation, or Interface Definition, can 
then serve several different roles in application development, 
including testing, Requirements Engineering, and `GUI; design.  
In particular, a formal review of important procedures exposed 
by a software component allows front-end development to identify 
which operational features need to covered by interactive 
User Interface components; for example, what units of 
functionality should be linked to buttons, context menus, 
and other responsive design elements.  
`p`

`p.
Extending Interface Definition outward to front-end `GUI; 
layers then presents the challenge of integrating 
rigorous data-modeling paradigms with quality 
front-end software affordances.  
Whether or not by technical necessity or just entrenched culture, 
`GUI; frameworks are still predominantly built in procedural 
or `OO; languages like `CLang;, `Java;, and `Cpp;.  It seems likely 
that a truly integrated type system, covering User Interface as 
well as data management logic, will need a hybrid functional/procedural 
paradigm at some stratum %-- either the underlying `GUI; framework or 
at application-level code.  So long as `GUI; frameworks remain 
committedly procedural, the most likely site for such hybrid 
paradigms to emerge is at the application level, and in the 
context of application-development `SLE; tools.
`p`

`p.
Implementing `GUI; layers in a Functional environment is usually 
approached from the perspective of `i.functional reactive` 
programming, which emphasizes how the interface between visual 
components and controller logic can be structured in terms 
of `i.event-driven` programming.  In this paradigm, there is 
no `i.immediate` linkage between `GUI; events and the 
functions called in response %-- for example, no 
single function that automatically gets called when the user 
clicks a mouse button.  Instead, events are entered into a pool 
wherein each event may have a varying number of handlers 
(including being ignored entirely).  This style of 
programming accords well with paradigms that try to minimize 
the number of functions with side effects.  Event-handlers are free 
to post new signals (these are interpreted as events) which 
may in turn be handled by other functions %-- so that 
signals may be routed between multiple functions entirely 
without side-effects.  That said, most events `i.should` cause 
side effects eventually %-- for instance, after all, 
a user does not typically 
initiate an action (triggering an event) without intending 
to change something in the application data or display.  But 
events can be routed between pure functions until an eventful 
handler is called, so side-effects can be localized in a 
proportionately small group of functions.
`p`

`p.
Moreover, certain qualities of `GUI; design can be expressed as 
logical constraints rather than as application-level state enforced 
by procedural code.  For example, optimal design may stipulate that some 
graphical component must automatically be resized and repositioned to remain 
centered in its parent window, sustaining that geometry even when 
the window itself is resized.  
Functional-Reactive frameworks allow many of these constraints to be 
declared as logical axioms on the overall visual layout and properties 
of an application, constructing the procedures to maintain this 
state behind-the-scenes %-- which minimizes the extent of 
procedural code needing to be explicitly maintained by application developers.
`p`

`p.
But while Functional Reactive Programming is a strategy for 
providing `GUI; layers on a Functional code base, it can equally 
be treated as an Event-Driven enhancement to Object-Oriented 
programming.  In an `OO; context, events and signals constitute an 
alternative form of non-deterministic method-call, where signal-emitting 
objects send messages to receiver functions %-- except indirectly, 
passing through event-pools.  Indeed, as documented 
by the demo code, events and signals 
have a natural expression in terms of Channel Algebra, where both signal 
emitters and receivers are represented via special `q.Sigma` channels.  
On this evidence, Functional Reactive Programming should be assessed not 
just as a `GUI; strategy for Functional languages but as a 
hybrid methodology where Functional and Object-Oriented 
methodologies can be fused, and integrated.
`p`

`p.
Contemporary software engineering still seems caught in a paradigm 
split, with Functional and Object-Oriented styles seen as 
competitors rather than candidates for admixture, and with a 
profound divergence between code libraries targeting native, 
desktop applications and those designed for the web ecosystem.  
I believe that further research in programming language design 
and software engineering methodology will however reveals 
these divisions to be preliminary, and a new generation of 
web/desktop and Object/Functional hybrid paradigms can emerge.  
Given the unique requirements of the CyberPhysical domain, 
perhaps CyberPhysical and Ubiquitous Sensing technology will 
be a momentum boost for this kind of behind-the-scenes research.     
`p`

