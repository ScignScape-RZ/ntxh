
`section.Types as Conceptual Structures`
`p.
The types encountered in a type system `TyS; are 
technical artifacts, but in many cases they also 
are designed to model, or track information about, 
concepts in the everyday world: fragments 
of natural language (i.e., text); people; 
places; colors; events; medical procedures and 
diagnoses; demographic and government data; 
scientific data and research findings; and so forth.  
We can accordingly consider types (implemented 
in software components) as `i.conceptual` artifacts, 
but with the caveat that their conceptual 
details have to be modeled within the constraints 
of software environments and computational processes.
`p`

`p.
Most real-world-based types are syntheses of 
multiple dimensions, or `q.fields`/: a type 
representing a person, say, might include 
their name, date of birth, gender, marital 
status, address, phone numbers and other 
contact info, etc.  Or, consider how we 
might lay out data for restaurant listings: 
`GPS; coordinates to locate restaurants 
on a map; street address; restaurants' 
names, hours of operation, and phone numbers; 
perhaps an indication of their relative priciness; 
perhaps a categorization of their style of 
cuising (French, Italian, Chinese, Japanese ...).  
The type might also have certain `q.flags` 
with pieces of information in a yes/no format: 
do they take reservations; are they wheelchair 
accessible; do they accept credit cards; 
do they serve alchohol.  An overall 
`q.restaurant` type (say, `tyR;) would then 
be a `q.product` of these various fields.  
`p`

`p.
The idea of `i.concepts` as multi-field complexes 
is also present in Conceptual Space Theory; 
many concepts are defined by a crossing of 
multiple dimensions, which collectively 
define the space of variations which their 
instances can occupy.  The concept `i.juice`/, 
for instance, would seem to be constituted by 
dimensions of color, taste, and perhaps liquidity 
(some juices are more viscous than others).  
Conceptual analysis can then proceed by 
isolating individual dimensions of various before 
investigating how they unify to create our 
impressions of conceptual similarity and 
dissimilarity, how concepts are refined or 
generalized, etc.  
`p`

`p.
In formal types, fields have different structural or 
extensional properties which influence their conceptual 
status.  One obvious criteria derives from 
the statistical distinction of `i.nominal`/, `i.ordinal`/, 
`i.interval`/, and `i.ratio`/.  In describing a restaurant, 
say, the field representing `q.kind of cuisine` is 
presumably nominal, in that we identify certain terms 
like Italian, Japanese, etc., and assign the restaurant 
to one or another.  This field probably has no 
`q.scale` or `q.metric` %-- French is not `i.more` or 
`i.less` than Chinese.  A field representing `i.cost` 
may similarly be broken down into discrete 
options (inexpensive, moderate ...) but here there 
`i.is` a notion of scale (we can rank `i.moderate` 
as between `i.inexpensive` and `i.expensive`/, say; 
and order restaurants from cheapest to costliest, 
or vice-versa).  On the other hand, cost may also be 
figured by a metric such as the average price of a 
typical meal, which would be a straightforward, 
increasing, whole-number scale; prices can be 
ordered and degrees of difference calculated 
%-- two restaurants are similarly expensive 
if their average meal costs roughly the same amount.  
Meanwhile, geospatial coordinates represent a 
two-dimensional and (up to approximation) continous 
space which permits distances but not ordering, unless 
we are taking distance from one central point 
(e.g., someone looking for restaurantes close to their 
home).  Hours of operation, for their part, cannot 
obviously be `q.ordered`/, though we can determine if 
a restaurant is open at a particular time; we 
can also rank establishments by how late they close, or 
how early they open %-- in principle, hours are cyclic, but 
when defining closing times we just need to consider the 
window of hours during the evening and night 
(respectively morning and afternoon for opening times).   
`p`


`p.
For a hypothetical restaurant `tyR; type, then, we 
can analyze their various fields in terms of their 
propensity for `i.ordering` and/or `i.distance`/.  
We can say, that is, that some fields allow 
restaurants to be ranked in increasing or decreasing 
measures for some fields (e.g. average cost of a meal); 
and some fields permit the `q.difference` between 
restaurants, within the dimension of the field, 
to be quantified (average cost of a meal again, 
or location).  Other fields allow for no 
particular comparison except for matching 
against single nominal values %-- unless we impose some 
metric whereby, say, Chinese and Japanese restaurants 
are deemed more similar to each other than to French or 
Italian, the most we can say is that two Chinese 
(etc.) restaurants are likely to be considered similar 
by virtue of both serving Chinese cuisine.  Still 
other fields allow for different kind of 
comparison if someone is looking for a restaurant 
meeting some criteria %-- that it accepts reservations, 
say, or is open at 10p.m. on a weekday.
`p`

`p.
The statistical or qualitative structure of fields, along 
these lines, become implementationally significant if 
we seek to derive algorithms to match `ty;-values 
to `i.queries` (say, to find a restaurant matching 
some customer's preferences), or to estimate whether 
`ty;-values will be deemed similar or dissimilar 
(if someone likes one restaurant, an engine might look 
for `q.similar` restaurants to recommend).  
These requirements, then, translate to 
`ty;-values as a whole: can we quantify (perhaps by a single 
distance metric) the degree of difference, or 
similarity, between two values?  Can we order any collection 
of `ty;s and, if so, ranked by which dimension?  
Can we search a collection of `ty;s and find a list 
of values that meet some search criteria?  To put 
this last question differently, how can we define 
parameters for searching `ty; collections?  Do we 
create a hypothetical `ty; %-- say, an Italian 
restaurant open at 10p.m. that serves wine %-- and 
use that as a template for matching concrete values 
in the collection?  Or do we create a different 
data structure, with a different type, aggregating 
search criteria against sets of `ty;s?  Should 
we, correlated with `ty;, define a distinct type 
for searches yielding `ty;s?  In the case of 
restaurants, such a `q.search` type might 
specify a range of price-points, maximum geospatial 
distance from a central location, one or more 
kinds of cuisine, and so forth %-- replacing 
single fields in `tyR; with ranges and bounds. 
`p`


`p.
In some ways, these operations of ordering, querying, 
and measuring difference/similarity are consistent 
with Conceptual Space Theory: they capture how 
conceptual reasoning can be bound to 
quantitative possibilities, using 
quantitative relations %-- distances, orderings 
%-- to reason through concepts' extensions.  
On the other hand, such quantitative reasoning 
does not constitute a full-fledged reduction of 
these concepts to quasi-mathematical spaces 
%-- it is not, say, that quantitative 
fields in a restaurant `tyR; type reveal 
how all conceptual details about restaurants 
can be reduced to numeric patterns.  Instead, 
quantitative structures fall out as the result 
of `i.operationcs` on this type %-- operations 
like ordering, querying, or measuing similarity.  
I would argue that, as cognitive processes, 
sorting and comparing are more fundamental 
than construing relations numerically, 
although numeric patterns may arise organically 
in the manifestation of sorting/comparing 
cognitions.  In short, the quantitative 
picture of (in this example) restaurants 
(or analogously I would say for many 
concepts) is derivative upon rational 
operations we perform `i.on` sets of 
concept-instances, more than latent 
mathematizations of an underlying 
conceptual space.  
`p`

`p.
Analogously for formal types, many 
numeric structures come into play, not internally 
within those types own fields or 
structures, but in terms of operations 
performed `i.on` types %-- and particular 
on `i.collections` of `ty;-instances, 
collections that can be ordered, queried, 
clustered by degrees of similarity, and so forth.
`p`

`p.
Suppose we have a restaurant database which tracks favorble 
reviews, assigning each restaurant a `q.grade` from, say, 
`Zero; to `OneHundred;.  Our `tyR; type thereby has another 
scalar field, which can be combined, say, with an average-cost-of-meal, 
yielding a two-dimensional space which restaurants can mapped 
into.  From the distribution of the resulting points, we 
could identify `q.good values` which are unusually highly-reviewed 
for their price-point, or outliers in the opposite direction where 
the review scores are lower than price would suggest.
`p`

`p.
In other words, a sample-space of restaurants mapped into the 
price/review space gives us a quantitative distribution, and 
our ability to compare restaurants in this way is doubtless 
a facet of restaurants' concept.  But these quantitative 
details are only really salient when it comes to 
`i.comparing` restaurants, and stistically reviewing 
restaurant collections.  The numeric structures are less 
conceptually foregrounded in our cognitive appraisals 
of any `i.single` restaurant.  It is true that the 
quantified comparisons are possible via 
aspects which all restaurants have because of the 
their conceptual `q.package`/, so to speak; 
so mathematizable comparisons are latent in 
restaurants' internal conceptualizations.  
But these aspects only really become `i.quantitative` 
in the context of comparisons, whether these are 
explicit (e.g. analyses of a database) or more 
mental and informal.  My judgment that a certain 
establishment is pricey, say, or cheap, inevitably 
results from a comparison (maybe subconscious) with 
other restaurants I have visited, or at least heard about.
`p`

`p.
The acknowledegment that quantitative structures thereby 
arise in the course of conceptualizing `i.restaurants` 
(in this case-study) does not accordingly demonstrate 
that our conceptual activity representing restaurants 
as cognitive acquaintences %-- as a feature of the 
world we roughly understand, for which the concept serves 
as an orchestrative tool %-- is at some fundamental 
level essentially mathematical.  Instead, numeric 
spaces and axes emerge from mental operations layered 
on top of our basic restaurant-cognitions, particularly 
insofar as our reasoning turns from thinking about 
individual restaurants to their comparisons.  
The analogous phenomenon in formal type theory 
would be that quantitative models of types' 
distributions come to the fore in conjunction 
with operations for sorting and comparing type-instances.  
I will now examine these kinds of operations in more detail. 
`p`


`subsection.Dimensional Analysis and Axiations`
`p.
Let us assume we direct attention to types in a 
`TyS; which are characterized by numerous distinct 
fields, and also that we are interested in 
procedures for ranking and comparing `ty;-instances.  
We can then analyze fields on the basis of how 
they might contribute to such comparative operations.  
To have a sufficiently unspecific term, I will use 
the phrase `i.intratype comparisons` to refer 
collectively to various ordering, comparing, 
querying, clusters, and measuring-dissimilarity 
operations. 
`p`

`p.
A `ty;s fields can then be classified according to 
how they may contribute to intratype comparisons.  
In the abstract, such an analysis would be provisional, 
because certain type-implications may have their own 
peculiarities.  For instance, it is reasonable to 
say that a restaurants' `i.name` is not a factor in 
estimating similarity: two restaurants with similar names 
are not especially likely to be similar in other respects.  
However, we can envision scenarios where textual 
similarity `i.would` be taken into account 
(e.g., a search engine trying to accommodate 
spelling errors).  Or, consider the question I mentioned 
earlier as to whether factors like Chinese/Japanese 
or French/Italian similarities (as styles of cuisine) 
should be modeled so as, in effect, to yield a 
distance metric on a nominal `q.kind of cuisine` 
dimension.  These examples show that anticipating 
exactly how clustering or distance algorithms would 
be designed, in any concrete case, is rather 
speculative.  Nevertheless, I think it is possible to 
make some broad claims about how data fields 
`i.usually` work in the intertype-comparison context. 
`p`

`p.
On that basis, then, I propose to distinguish five rough 
sorts of data fields, as follows:

`enumerate,
`item; Digital/Internal fields: Computational artifacts, such 
as globally unique identifiers, which are employed by 
code managing type-instances behind the scence but do 
not typically embody real-world concepts related to 
the type, and are not typically salient in intertype comparisons.

`item; Textual fields: Natural Language artifacts such as 
names and descriptions, which would not normally be 
used directly for intertype comparisons.  Ordering or 
measuring similarity on collections of textual contents 
tends to be difficult, or to have little actually 
conceptual resonance, unless some sort of Natural Language Processing 
is used to extract structured data.  For example, there 
is probably little conceptual significance attached to (dis)similarity 
or ordering among restaurant names, except perhaps if it is desired 
to list restaurants alphabetically (which in any case is a 
presentational matter more than a comparative one).

`item; `q.Axiatropic` fields: I use this term to represent any fields 
which have nominal, statistical, scalar, or in any sense 
quantitative qualities that can be leveraged for comparisons.  
I include nominal fields (enumerations) because these are 
relevant to similarity %-- consider two restaurants both labeled 
`q.Chinese` %-- and also nominal dimensions can sometimes 
have extra comparative structure (e.g., an inexpensive-moderate-expensive 
scale is ordered by increasing cost).  Other than enumerations, 
I define axiatropic fields as any dimension with numeric 
values where numeric properties are consequential for 
ordering or for measuring similarity %-- excluding, say, numeric 
id's where numbering has not structural meaning other than uniqueness, 
but including `q.locally` ordered scales like time points, as well as 
`q.globally` ordered dimensions such as integer magnitudes 
(e.g., prices), and spaces which have no particular ordering but 
which can be ordered via distances (like geospatial locations).  
Axiatropic fields can be seen as `q.axes` to which 
type-instances can be project, and the union of `ty;s 
axiatropic fields, which I propose to call `ty;s 
`i.axiatropic structure`/, defines a multi-dimensional 
space wherein `ty;s are mapped to individual points.  
The tuple of values obtained from these fields 
I call an `i.axiatrope`/, and the points to which a 
given `ty;-axiatrope projects I call an `i.axiatropic image`/.  
If desired, axes can be annotated with details such as 
valid ranges and units of measurement (consistent 
with, for instance, Conceptual Space Markup Language).

`item; Flags: I separate out boolean values 
%-- along the lines of whether a restaurant is wheelchair 
accessible, or takes reservations %-- because these pieces 
of information are more likely to be used to match 
candidate values against criteria than for comparisons 
between values.  This does not preclude some similarity 
algorithm from ranking, say, two restaurants that serve 
liquor, or take reservations, as a little more alike  
%-- i.e., these data points may add to a metric 
of similarity (or inversely subtract from a metric of 
difference) when they agree between instances, or 
contrariwise when they disagree.  However, I would 
argue that conceptually these kinds of factors are 
more pertinent to building a sufficiently detailed 
picture of an instance than to intratype comparisons; 
on that premise I distinguish `q.flag fields` as a 
separate field grouping.    

`item; `q.Mereotropic` fields: These fields represent 
collections of values (lists, tuples, and so forth) rather than 
single values.  In general, tuples are less conducive 
to direct comparison, without further analysis 
%-- say, comparing two students' grades by comparing 
their average; or comparing two lists by counting 
the elements they have in common.  Similarity 
metrics, then, can employ collections fields, 
but the calculations are more involved than just projecting 
type instances onto points in a mathematical space.  
I leave open the possibility that collections may 
have elements that are themselves collections, so that 
mereotropic fields can give rise to `q.mereological`/, 
or part-whole, hierarchical structures.
`enumerate` 

These different genre of fields are reflection in 
different compartments to a type's interface; 
to this list I would then add the 
portion of the interface related to 
constructing `ty;-instances: constructors, 
co-consructors, and related functionality for 
testing the validity of a data structure and/or 
`q.deconstructing` values into a construction 
pattern.  Collectively I will refer to 
this last aspect %-- which does not involve 
fields `i.per se` %-- `ty;s `i.constructive interface` 
or (with apologies if my neologisms are getting a 
little heavy-handed) its `i.nomotropic interface`/.  
The fields (textual, flags, binary) which are 
neither axiatropic nor collections-based I will 
call `i.endotropic`/.  I will then call 
procedures related to restructuring or re-presenting 
`ty;-values for analysis, `GUI; or visual 
rendering, serialization and deserialization, 
or database persistence, as collectively 
a `i.morphotropic` interface.  We then have a partition 
of types' interface into five facets: 
nomotropic, axiatopic, morphotropic, endotrophic, mereotropic.
`p`

`p.
With respect to the portions of an interface 
which `i.do` involve data fields,  
certain elements of this overall model 
are consistent with and informed 
by Conceptual Space theory, but its practical 
applications are oriented more 
toward computational operations on multiple 
values.  Specifically, this aspect of types' implementations 
is focused on operations wherein `ty;-instances 
are compared to one another.   
The idea is not to use a `q.theory of 
fields` to uncover underlying conceptual patterns 
linking formal types `ty; to real-world phenomena.  
The concept `i.field` itself may imply an 
operational manipulation of type-instances: 
a computation may be described or utilized as a field 
even if it is not technically provided 
among the tuple of values via which a `vVar; 
is registered in memory.  For instance, 
the average value of a collections field 
can be considered a data point on `ty; 
even if it has to be dynamically computed.
`p`

`p.
In effect, then, `ty; data points which 
are relevant to comparing or querying 
`ty;s are facets of `ty; that need to 
be deliberately engineered.  Implementers 
choose which `ty; details to make accessible 
to external code; they may also choose to 
provide dynamic calculations that provide 
information about  `ty; values 
(sometimes called `i.views`/), such as 
the average of a student's grades, even if 
this data is not internally tracked by 
the `ty;s binary layout.  Implementers, 
in short, have to anticipate and provide 
procedures for client code %-- sofware using 
the type implementation %-- to interact 
with `ty;s fields: iterating over collections, 
dynamically calculating views, 
querying against a prototype, ordering 
on one or another field, etc.  
These considerations inform the process of 
designing an `i.interface` for `ty;  
Overall, a `i.ty` interface covers various 
tasks, such as constructing `ty; instances 
in the first place, or itegrating 
`ty;s with other kinds of software components 
(serializing and deserializing `ty;s for 
data sharing; sending `ty;s to a database; 
showing `ty;s in a `GUI;; etc.).  
I will use the term `i.paratropic interface` 
to that portion of a `ty;s interface 
which concerns sorting, comparing, and 
querying `ty;s (or sets of `ty;s). 
`p`

`p.
In practice, the field-classification I proposed 
earlier would most likely be applicable to a 
type's `q.paratropic interface`/: it would 
help implementers reason about what data points to 
expose for a `ty; and how precisely to 
set up the logistics for obtaining this data 
from `ty; values.  Taken in conjunction with 
the principle that formal types are (often) 
modeling artifacts which approximate real-world 
concepts, this discussion then suggests that 
such conceptual goals are mostly operative in the 
interface `i.to` types.  That is, when considering 
how to use formal artifacts to proxy real-world, 
human concepts, the point is not only to 
assemble a list of particular details to keep 
track of (for a restaurant: name, location, cost, 
etc.).  A type's effectiveness in representing 
human concepts is equally dependent on a programming 
interface which allows digital operations to be 
performed; operations which reflect how we conceptualize 
real-world phenomena, including by actions of 
ranking and comparing instances of the same concept. 
`p`

`p.
To the degree that Conceptual Space Theory has 
an intuitive role to play type semantics, then, I 
would argue that this role is chiefly manifest 
in the realm of types' `i.interface design`/, in 
the procedures defined `i.for` types, rather than 
in the internal mathematical/computational structure 
of type-instances qua digital constructions. 
`p`

`p.
I think this perspective is also consistent with 
my earlier analysis from a natural-language 
perspective: a conceptual account of `i.nouns`/, 
in particular, should be oriented in the 
pragmatic employments of concepts as 
cognitive tools: the idea that we mentally 
`i.do things with` concepts.  Perhaps there is a 
certain correlation between the idea that formal 
types' are conceptually defined by their 
`i.interface` and that, cognitively, concepts (noun concepts in 
particular) are constituted essentially by their 
intellectual-functional roles (or, at least, 
so I will now consider).
`p`

`subsection.Concepts as Functional Tools`
`p.
Earlier I argued that judgments of conceptual 
appropriateness are typically driven by pragmatic 
concerns %-- deeming rain as `i.pouring`/, for instance, 
is a useful characterization if it implies a response 
consistent with how most people would behave in pouring rain 
(as opposed to, say, drizzling rain).  In some cases 
%-- as with drizzle/pour %-- we might find a quantitative 
dimension which models the peaks and nadirs of 
concepts' applicability, but the quantitative picture is 
most relevant when it implies a `i.qualitative` 
difference.  Along the scale of rain intensity, 
conceptualizing rain as `i.pouring` or `i.drizzling` 
will have qualitatively different implications. 
`p`

`p.
I centered my earlier discussion on verbs and prepositions: 
using the verb `i.pour` or `i.drizzle`/, or say 
`i.on` vs. `i.all over` for `i.dishes on the table`/, 
reflect different situational consturals which are 
appropriate to the degree that they are useful, 
in organizing our response to situations or anticipating 
that of others.  Choosing the phrase, e.g., 
`i.dishes all over the table` signals the speaker's 
framing the circumstance, a semantic cue which will 
be deemed appropriate if it engenders valid 
follow-up reasoning; e.g. if it implies that it will 
take some effort to clear the dishes, and such is 
indeed the case.  Word-choices like `i.pour`//`i.drizzle` 
or `i.on`//`i.all over` imply a conceptualization 
which people will tend to judge `i.appropriate` if 
they are practically `i.useful` glosses on states of affairs.  
`p`

`p.
Presenting this thesis in the context of verb and prepositions, 
I would argue that a similar analysis can be given for nouns: 
subsuming an entity under a given noun-concept is admissable 
if doing so gleans useful hints as to the entity's 
properties or dispositions.  Calling something a restaurant 
is useful if the practical implications %-- that 
someone can go there to order and consume food, etc. 
%-- are appropriate.  By choosing the word 
`i.restaurant`/, we hear the speaker as implying certain 
practical details appertaning to the thereby categorized 
establishment, and we are wont to deem the concept-attribution 
accurate or not depending on how well those practical 
expectations are borne out.  Calling a food court stall 
`i.restaurant` is dubious because it raises 
expectations that are unwarranted. 
`p`

`p.
In the case of verbs, concept-attributions are canonically 
assessed in the context of specific situations, because verbs 
typically profile discrete events.  If I say 
`i.we ran for the bus`/, the word-choice `i.ran` packages 
several components expected to be part of the relevant 
circumstance, e.g. that some thing or person/people 
were `i.doing` the running, as a conscious choice, 
were running `i.to` some destination, and were moving 
with non-ordinary speed.  The characterization invites 
various addenda: why were the rushing?  Where were they 
running to?  Who was running?  If these questions are 
not sensible in the described situations' context, 
the `i.run` choice of words was probably ill-conceived 
(and the corresponding conceptualizing of the 
situation would be deemed flawed).  Or, citing 
`i.pouring` rain would imply that people would or should 
seriously try to avoid being exposed to that weather, that it 
could potentially be dangerous, and so forth: if in 
fact the rain is not strong enough to elicit that 
response from other people, then `i.pour` is not a 
useful concept to apply to the situation; it does 
not have realistic predicative implications.  
`p`

`p.
In the case of nouns, estimations of applicability may 
have a similar overall structure but are less oriented to 
individual events, and more toward `i.potential` or 
`i.typical` events that we can anticipate `visavis; 
the target of a conceptualization.  Announcing that 
a restaurant is conceptualized as such commits us to 
predicating the pattern of various kinds of events 
that would commonly occur in conjunction with the 
restaurant: people being seated at tables, ordering 
food, having the food served to them, etc.  
If these are not the events evidenced in how 
typical situations unfold %-- e.g. a food court stall 
would instead characteristically have people find 
their own tables and carry their orders themselves 
%-- then the proposed concept is of limited applicability.   
`p`

`p.
The worthiness of a concept-attribution, in short, can 
be measured by how accurately the concept leads us on to 
further strategies for learning about, or responding to, 
the things, events, or situations covered by the concept.  
If we are conceptualizing an event %-- that someone ran, 
that it rained %-- the concept we apply (signaled by 
the verb we choose if talking about it) unlocks a passage to 
uncovering more details.  In our `q.inner`/, or `q.subconscius` 
thoughts, conceptualization mentally gear ourselves 
for where to then turn our attention, if we seek to 
glean more information, or how to respond pragmatically.  
If concepts map to words, and we hear or speak of 
conceptualizations interpersonally, recognizing the 
concepts others apply cues us to how `i.they` are 
disposed to act, and also opens a shared information space 
where both parties anticipate how to query each others' 
conceptualizations.  
`p`

`p.
For natural language semantics, concepts' intersubjective dimension 
should be foregrounded, even though it is based on each 
persons' internal, probably partly subconscious 
ideations.  Accordingly, then, we should emphasize 
how conceptualizations, signfified by language, 
`q.open up` a dialogic space.  When conceptual 
attitudes profile an event, their linguistic expression 
puts conversants on the threshold of co-responsive 
process where parties tease out details of other's 
situational framings.  The dynamics of this process 
depend on context.  If one person knows the most 
about a situation %-- maybe they are telling a story %-- 
the interpersonal epistemics would be oriented to 
those learning of the situation at one remove 
steering the former party to provide information the 
other find relevant.  If multiple parties were all 
equally proximate to a situation, the dialog 
could be more about syncing their respective 
interpretations %-- the how, why, and 
so-what which profile scenarios toward 
finding optimal pragmatic responses.
`p`

`p.
Noun-concepts add the complication that 
they are not necessarily employed in 
conjunction with framings of specific 
situations.  Of course, negotiations can 
apply to nouns also: 

`sentenceList,
`sentenceItem; We were delayed in traffic because we got stuck behind a truck.
`sentenceItem; Well, it wasn't really a truck, it was more like a van, 
but it kept slowing down at the intersections.
`sentenceItem; Well, we didn't get stuck for long, but we pulled off 
onto a side street so it took longer to get here.
`sentenceList`

In this case the speakers are debating with each other over 
which conceptual presentation best captures the situational details; 
a dialectic that can be evidenced in negotiations over noun-acceptability 
(as in (2)) as well as verb-acceptability (3).  
Analogous collaborative framing-choices could play out in 
immediate, pending situations where conversants have to 
act in consort: 

`sentenceList,
`sentenceItem; Should we stop at that restaurant?  The kids are starting to act up.
`sentenceItem; That's not really a restaurant %-- I'd rather wait until 
we can sit down for lunch.
`sentenceItem; They're not really acting up %-- I think they'll be OK 
for another hour or so.
`sentenceList`
`p`

`p.
Counter to these `q.in the moment` examples, though, 
noun-concepts can be negotiated outside any immediate 
situation: 

`sentenceList,
`sentenceItem; The best place to eat in this neighborhood 
isn't actually a restaurant; it's a health-food store where 
they have a seating area in the basement.
`sentenceItem; Experts are predicting a recession next year, which 
could affect the elections.
`sentenceItem; They're not really predicting a recession, just a slowdown.
`sentenceList`

These hypothetical discourses could likely 
occur in situations where the speakers do not have an immediate 
need to act, but are sharing `q.background knowlege`/.  
We use language not only to coordinate responses to 
pressing circumstances, but to asssess and enrich our 
collective knowledge by sharing beliefs and ideas 
%-- it is useful if friends, neighbors, coworkers, 
etc., share a robust stock of propositional attitudes 
so as to facilitate collective action when it `i.does` 
become necessary.  
`p`

`p.
But whether oriented to immediate or to generic, hypothetical, 
or future situations, conceptualizations are 
linguistically and cognitively salient largely because 
of how they open the space for new rational activity.  
In language, my grasping how another language-user 
conceptually frames a given event or entity 
provides a data point from which I can then respond, 
pursuing ever refined mutual understanding and 
co-ordination: do their conceptualizations agree with 
mine?  Do they have knowledge that I should access?  
Do their conceptualizations spur a propensity to 
act in ways I can anticipate and/or reciprocate?  
Are there differences in beliefs or opinions that 
we can reconcile?   
`p`

`p.
Conversation, of course, cycles through a large number of 
conceptualizations over the course of several turns 
of dialog, or even of one sentence.  
Each concept may play a role in the subsequent course 
of that dynamic, because how we conceptualize something 
is a window, for others, onto our thought-processes 
and dispositions.  Concepts as cognitive tools, 
in short, play out largely in this intersubjective, 
co-intellectual dynamic, marked by our `q.theory of 
other minds`/.  Consequently, we should view 
theories of concepts which are too `q.solipsistic`/, 
or treat concepts only in the confines of one person's 
cognition, as at best incomplete.  This would include theories 
that model concepts around prototypes or exemplars 
%-- a paragon `q.restaurant idea` that grounds the concept 
by offering a point of comparison for actual restaurants. 
It would also cover theories that treat concepts 
as instramental logical conjuncts, a semantic frame of qualities 
to check off: $x$ is a restaurant if it serves food, 
as a commercial activity, with table service, 
with a menu, with a fixed location, etc.  
In particular, uncomplicated encodings of 
conceptual structures within logicomathematical 
frameworks have to be taken with a grain of salt.
`p`

`p.
Assuming the gist of this analysis, how then `i.should` 
an `q.intersubjective` theory of concepts affect 
our appraisal of formal types and digital artifacts 
being, in part, technical representations of 
concepts as they operate in human affairs and natural language? 
`p`

`subsectiontwoline.Hypergraph Grammar for Natural and Formal Languages`
`p.
I have argued that conceptualizations in (and expressed via) 
natural language are cognitively significant because they 
spur subsequent discourse and reasoning: mentally 
deciding (or hearing from someone else) that something 
or some situations falls under some conceptual rubrick 
is a stepping stone to further mental or practical 
activity.  This may be purposeful behaviors enacted 
as the situation unfolds, or less tangible actions 
in the discursive space of a dialog, or intramental 
operations wherein new beliefs are formed, or 
choices are made about where to apply attentional 
focus.  But whether in behavior, language, or cognition, 
concept-attributions acquire their full meaning only 
through subsequent operations which become available 
or reasonable by virtue of it being accepted, 
or posited, that some event or entity is attributable 
by some corresponding concept.
`p`

`p.
The formal analog to this theory is clearly that 
types' are semantically operationalized, as I 
argued earlier in this section, by procedures 
defined on them rather than by their precise 
layout of data fields.  The purpose of a type, 
qua digital artifact, is not just to collate 
a tuple of values.  It is rather to package 
multiple values into an operationally beneficial 
whole.  Each `ty;'s utility derives from procedural 
resources that come into play via `ty;'s inplementation, 
not merely from the values `ty;-instances carries with them.  
`p`

`p.
This means that types' formal semantics derives in part from 
procedures defined on them %-- i.e., procedures which  
input and/or output `ty; values.  Such a proposition may 
appear to make types' semantics open-ended, because most 
programming environments do not restrict where and when 
typical procedures involve each `ty; may be defined.  
It would not make much sense to say that types' 
semantics are so closely bound to any and all 
procedures such that simply defining any function that
takes or returns a `ty; converts `ty; to a different 
type; that would render notions of type-identity almost 
meaningless.    
`p`

`p.
However, it `i.is` reasonable to isolate a group of 
procedures as an `i.interface` to `ty;, which are 
then constitutive of `ty;'s semantics: modifying 
the interface %-- by adding, or changing the 
signature of, an interface procedure %-- 
`i.does` make `ty; into a new type.  
Types' identities are therefore bound to 
the specific collection of procedures which are 
declared to be operationally `q.internal` to 
`ty;; which provide the basic functionality 
through which other sorts of actions involving 
`ty; can be defined.
`p`

`p.
Some coding styles offer a construction to transparently 
demarcate these core, interface procedures from the 
others.  The obvious example is Object-Orientation: 
in most Object-Oriented languages, the `i.interface` to 
a `i.class` (a `i.type` whose values are Objects) 
is constituted by those procedures (`q.methods`/) 
which take an instance as a privileged `q.`this;` 
value, symantically separated out from and 
treatedly differently than ordinary 
function parameters %-- in the context of `q.channels` 
I say that methods have a separate channel for 
`q.message receiver` values (using Object-Oriented 
terminology), or a `q.sigma` channel (using terms 
from `q.Sigma Calculus`/, an Object-Oriented extension 
to `q.Lambda Calculus`/).  
`p`

`p.
For a general model of type-systems, however, we cannot say 
for sure that some semantic or channel-based 
mechanism will force procedures to be classified as 
within a type interface or not; at most we can stipulate 
that interface procedures may be explicitly declared 
as such.  Accordingly, assume that any `TyS; can denote, 
for each `ty; it covers, that a procedure which 
inputs and/or outputs a `ty; is somehow `q.core` and 
part of `ty;s interface.  Conceptually, such 
procedures provide the foundation for digitally or 
computationally manipulating `ty;-instances, so they 
reveal a conceptual model of `ty;'s purpose 
%-- the modeling agenda behind `ty; being implemented 
as such can be revealed through the choice of 
procedures included in `ty;'s interface.
`p`

`p.
The semantics of types, then, is essentially revealed 
through the semantics of their interface procedures.  
This points to the larger observation that the 
semantics of computer code in general is predominantly 
the semantics of procedures: almost all code, if not 
literally all code, in mainstream programming languages, 
is code describing (or effectuating) the implementation of 
procedures.  Moreover, almost all code in the body 
of a procedure describes call to `i.other` procedures.  
As such, the most important criteria for defining 
programming languages' syntax and semantics is to 
describe procedure bodies (to demarcate sequences of 
statements or operations enacting each procedure) 
and procedure calls (how one procedure achieves 
its rationale by calling to other procedures in some 
structured fashion).
`p`

`p.
Modeling inter-procedure calls is especially amenable to a 
graph-based representation because individual nodes 
can represent external procedures to invoke, during the 
execution of the current procedure.  If we process 
code via graphs %-- i.e., `q.code graphs` %-- then 
each procedure's implementation can encompass its 
own code graph; some nodes in that graph then represent 
other procedures, with their own graphs.  There is not 
necessarily a one-to-one relationship between nodes in one 
procedure and other procedures the first one calls: 
sometimes a source-code token, for instance, 
provides a name for a function or method which may actually 
be mapped to several different procedures at runtime.  
Subclasses can override the methods of the class they inherit 
from; also generic code can be reused across multiple types.  
As a result, a single code-graph node may not embody 
one specific procedure, but rather be a placeholder for a 
variety of possible procedures for which the proper 
target is dynamically selected when the enclosing procedure 
executes, given a specific set of input parameters.   
`p`

`p.
What `i.is` fixed within a code-graph, however, is how the 
nodes standing for external procedures connect to surrounding 
nodes which supply onputs to those procedures, or hold 
their outputs.  Almost all the structure of code-graphs 
derives from their marking how some nodes carry values which 
become inputs to inter-procedure calls, with edges linking 
those nodes to the central node representing that 
external procedure itself.  In the `q.Channel Algebra` terms 
I proposed in `cite<MeHBTT>;, code-graphs modeling procedure 
implementations comprise (in general) multiple 
`i.channel package`/, each package being a graph structure 
centered on one `q.ground` node which designates an 
external procedure, and other nodes linked to it which 
represent how the channels carrying data between the 
two procedures are to be populated with values from 
the original node's current environment.  Code-graphs on 
this approach are hypergraphs because channels group 
multiple edges (specifically those linking `q.carriers` 
to ground nodes in a channel package), representing an extra 
layer of graph structure (as well as the fact that 
graph-nodes represent typed values which may internally 
contain a tuple of further values; i.e. they are hypernodes 
spanning a tuple of hyponodes).    
`p`

`p.
Programming language grammars, in turn, can then be analyzed 
as systems for mapping surface-level computer code to 
`q.channelized`/, hypergraph-based code-graph representations.  
Grammars need to document how source code indicates which 
tokens represent `q.ground` nodes in a channel package 
(i.e., represent procedures to call); which tokens represent 
input or output values and their corresponding ground node; 
which channels the carrier-to-ground links belong to; 
and how carriers in the context of one procedure-call 
pass values to carriers in the context of subsequent 
procedure calls.  In short, an adequate grammar for a 
programming language has to engender compilers that 
transform source code to code-graphs comprised 
of `i.channel packages`/.  Insofar as the underlying 
`q.channelized` graph model is based on hypergraphs, 
this criteria effectively states that programming 
languages need hypergraph grammars.`footnote.
The point is not that all languages' formal specifications 
are hypergraph-based %-- or that all compilers 
should or do use hypergraph code representations %-- but 
rather that all languages's code `i.can` be modeled via 
Channelized Hypergraphs and that all languages thereby
`i.can` be modeled via hypergraph grammars. 
`footnote`  
`p`

`p.
Note that my form for hypegraph structures in this analysis 
is divergent from Hypergraph Categories as 
originally proposed for integration with 
Conceptual Space semantics.  Instead of hyperedges 
linking two procedures, I situate graphs primarily 
within the implementation of each single procedure.  
Hyperedges then, most commonly, represent 
links between `i.carriers` and `i.ground` nodes; 
they represent the relation whereby a procedure 
call draws from a surrounding value to pass as a 
parameter to an external procedure.  In practice, 
however, the two notations can coincide insofar as 
(according to a common pattern) one procedure call 
is provisional to a second; so the output acquired 
from a former call becomes mapped %-- internally in 
the calling procedure's environment %-- to 
input for the subsequent call.  
`p`

`thindecoline;

`p.
The mashup of hypergraphs and Conceptual Spaces 
%-- what I earlier dubbed `q.`HCS;` %-- 
aims to web a hypergraph-based grammar to a 
Conceptual Space semantics.  Here I have 
provided at least a provisional outline 
of a Hypergraph Grammar framework for 
programming languages; also ideas on formal 
type semantics `visavis; programming language 
type systems, noting some similarities and 
contrasts with Conceptual Space theory.  
Plus I outlined my case for which directions best 
capture the theoretical strengths of Conceptual Space 
semantics in natural language.    
`p`

`p.
Against this backdrop, I have largely neglected what would 
be the fourth leg of a double-hybrid framework 
combining hypegraph grammars and conceptual space 
within both formal and natural language %-- I have 
not discussed whether my just-outlined 
`q.Channelized Hypegraph` model, formulated in the 
programming-language context, has any applicability 
to natural language.  I will offer some comments on 
that subject next.
`p`

`subsubsection.Hypergraph Grammars and Natural Language`
`p.
So far I have argued that programming language grammars 
should be rooted in the foundationally procedural 
nature of computer code %-- the building-blocks 
of any self-contained stretch of source code 
are constructions of calls to one procedure from 
the context of the second.  In graph representations, 
once a node is marked as identifying the procedure 
to call, the essential structure of the surrounding 
code is how other nodes supply values to pass 
to the marked procedure as inputs.  Inter-procedure 
calls are therefore the prime semantic constituents of 
program behaviors, so that programming languages' 
syntactic rules are (directly or indirectly) governed by 
the need for inter-procedure calls to be unambiguously 
constructed.  The syntactic forms which these languages 
exhibit, that is, can be explained by our need to use 
these languages to implement procedures by calling other 
procedures, identifying the procedures to call and 
the symbols carrying their inputs and outputs, 
in a transparent fashion which computers can 
mechanically react to.
`p`

`p.
So the basic building blocks of programming languages' 
`q.discourse` are `q.statements` which involve one 
procedure calling another, possibly using results 
obtain from calling prior procedures as intermediate 
values.  Statements are structural analogs in this 
context to sentences in natural language, although 
the analogy may not seem very substantive 
%-- after all, it is hard to see in what sense a 
`i.sentence` could be deemed a `q.procedure call`/.  
Still, I `i.will` pursue this analogy to some extent, 
not necessarily on the actual sentence level but 
on the level of sentences' smaller constituents. 
`p`

`p.
Here I have endorsed a generally cognitive-linguistis 
methodology more than a reductively `q.logical` model of 
language, one which discounts (natural) language nuance 
and contextuality.  I disavowed what might be called 
`q.logistical` models of concepts, where concepts could 
be treated as artifacts of logic, and entertaining 
concepts %-- or deeming concepts attributable to 
their tokens %-- could proceed as an essentially 
logical operation: how close is a candidate 
bearer to a conceptual prototype; how many 
`q.boxes` does the candidate `q.check off`/.  
I believe instead that conceptualization is 
more `i.interpretive`/: to ascribe a concept to a bearer 
is to offer (internally in one's mind, or externally 
through language) an interpretation of that 
(object, situation, or event) which is conceptualized.
`p` 

`p.
On this perspective, when we communicate conceptualizations 
via language we are also signifying `i.interpretations`/; 
and the syntactic and semantic strata of language 
can be studied through the lens of how elements of 
language contribute to this process.  Each word in a sentence, 
for example, adds some detail to interpretation 
conveyed by the whole.  Such a principle has 
ramifications for both semantics and grammar.  Semantically, 
the meanings of words can be understood in the context 
of the interpretive additions or alterations they 
contibute to other words in their context.  Syntactically, 
the structures of language can be seen as vehicles to 
mark which words are, most directly, interpretively 
linked to other words.
`p`

`p.
When we say, for instance: 

`sentenceList,
`sentenceItem; A friendly dog was behind the gate barking and wagging her tail.
`sentenceItem; My neighbor walks that dog every morning.
`sentenceItem; The store around the corner is closed, so we have to walk 
a few blocks out of the way.
`sentenceItem; We drove along the scenic drive by the lakeshore.
`sentenceItem; We drove the scenic lakeshore drive.
`sentenceItem; We drove to the lake shore.
`sentenceList`

we are using implicitly connected pairs or groups of words to paint 
various interpretive pictures, with one word adding interpretive 
shading to the other(s).  The obvious case is in (1), interpreting 
the dog as `i.friendly`/.  But describing the dog's action (and the 
tail's movement) as `i.wagging` is also an interpretation.  
Likewise characterizing a store as `i.closed` (which could 
mean either closed for the night or shut down permanently) is 
an interpretation of its current state.
`p`

`p.
Those hearing sentences like (say) (1)-(5) likewise have to 
glean the speaker's interpretative attitudes (I'm deliberately 
playing off the philosophical `i.proposiontal attitudes`/) 
%-- which is itself an interpretation.  Moreover, the 
addressee's interpretations are driven by a combination of 
linguistic and extra-linguistic, or situational, 
givens.  The verb `i.walk`/, e.g., calls for a different 
interpretation in (2) than in (3): in the former the walk is 
not a fixed path to a destination, whereas the latter profiles 
the endpoint of the activity more than the process.  
A similar dynamic defines the contrast between (4)-(5) 
and (6).  
`p`

`p.
However, these interpretive variations are not 
fully specified by the language itself; 
we cannot very well account for the different kinds 
of situations represented in these sentences by 
examining words by themselves.  A lot of 
this interpretation instead comes from background 
knowledge: that people routinely walk dogs 
(typically with no set destination) for the dogs 
to get exercise and relieve themseves; that 
people like to drive on scenic roads so as 
to appreciate the scenery, separate and apart 
from their desire to get to destination; 
that people in urban areas use the term 
`q.block` as an indicator of distance 
(as well as for profiling a spatial region 
as in `i.around the block` or `i.down the block`/).  
These `i.usages` are not really explicated 
by considering just word-senses, or discrete 
lexical entries; it is more that words 
become entrenched in specific roles 
for discussing specific kinds of 
situations.
`p`

`p.
So the point is not that `i.walk` as in `i.walk the dog` 
is a different (or, indeed, the same) lexified 
concept as in `i.walk to the store`/.  Instead, 
conventional usage primes us to certain discursive 
habits which transcend notions of sameness or 
difference among word-senses.  Both `i.walk the dog` 
and `i.walk to the store` are conceptual packages 
which involve more than the `i.walk` verb alone.  
The enunciation of `q.walk` `i.in its specific context` 
calls forth the entirety of those conceptual 
tableux: so when we hear `i.walk` in `i.walk the dog` 
we instinctively intuit a pre-formed cognitive framing; 
a kind of mental `q.script` that spells out the 
prototypical situation of someone walking a dog, 
why they do so and their typical behaviors in the process.  
Likewise for `i.walk to the store`/.  Likewise also, 
the concept of driving `i.to a destination` brings 
up a cognitive script somewhat different than 
driving to observe the scenery.  
`p`

`p.
Words, on this theory, do not usually signify 
logical constructs; rather, they are proxies 
for situational gestalts, selected to 
bring forth in the addressee's mind an 
interpretive framing consistent with the 
speakers' own.  As a competent participant 
in dialog %-- and a rational member of a 
relevant language community %-- our fellow 
conversants assume that we have a stock 
of many situational `q.scripts` and that 
they can trigger the appropriate construal 
in our minds by their choice of words, 
and of phrase-structures.  Such scripts, 
moreover, are (at least to some degree) 
extralinguistic %-- they emerge from our 
background knowledge and overall ability 
to craft purposeful behavior in the 
face of a present situation, rather 
(in general) than from purely linguistic 
competence with syntax or semantics.  
`p`

`p.
Properly understanding language is therefore 
a matter of parsing sentences so that, in response 
to both semantic (e.g., word-choice) and 
syntactic (e.g., phrase-structural) data, we 
can call forth the proper interpretive 
script for each received linguistic performances 
(the scripts which best recapitulate speakers' 
own interpretations).  I believe that this theory 
does, now, actually begin to resonate with 
my earlier gloss on programming-language 
grammars.  In that context I argued that the 
key responsibilities of grammars were to 
build packages of values (or `q.carriers`/) 
and `q.ground nodes` marking the call to a procedure, 
in the context of a different procedure.  
In the current, natural language setting, 
I would say that the role of `i.natural` language 
grammar is to package words %-- together with 
their interword links and larger phrasal units, 
if applicable %-- so as to map from linguistic 
givens to interpretive `q.scripts`/, selected from 
a vast warehouse of situational prototypes and 
interpretive dispositions we are assumed to 
command, as competent language-users.  Interpretive 
scripts here play a role analogous to computer 
languages' procedures. 
`p`

`p.
Interpretations are called forth on a smaller scale, not 
just a whole sentence %-- the predication `i.friendly dog`/, 
say, does more than just attribute the quality of 
friendliness to a canine.  It is more even than a 
prototypical `q.friendly dog` which anchors the script.  
Instead it is something like a concept that is narrativized, 
that is cognitively entrenched not just as a mental picture 
or observed pattern but as a recipe for pragmatic 
action: conceptualizing the dog as `i.friendly` primes 
me to behave toward her in certain ways, including 
bearing certain expectations of her own behavior.
`p`

`p.
Moreover, language does not have sufficient logical transparency 
to allow an independently meaningful semantic substratum 
independent of cognitive and pragmatic concerns 
(`q.pragmatics` here in the linguistic sense).  
The fact that `i.friendly dog` is not really just a predicate 
structure is one example.  A related example comes from 
one of the introductory presentations for `CSML;, where 
the notions of a `q.contrast class` is demonstrated through 
the case of `i.large chihuahua`/, which would still be a
`i.small` dog \cite[page 3]{RaubalAdamsCSML}.  Contrast classes
are introduced as a semantic device to redress the 
supposed anomaly wherein `i.chihuahuas are dogs` does not
entail to `i.large chihuahuas are large dogs` (to be sure, there
is much literature on these `q.contrastive semantics` 
problems in linguistics).  But this is only a problem 
if we expect that people receive language segments 
as if they were propositional forms, where rules of 
logical deduction or substitution apply.        
`p`

`p.
In truth, language is much more illogical than that.  
Even simple adjective-noun pairings are received as 
condensates of some conventualized subconcept or 
situational template more than a straightforward 
logical predication.  We have a `q.large dog` script 
no less than a `q.friendly dog` script.  Dogs we 
would conceptualize as `i.large` are not even, necessarily, 
larger than the median or mean; the conventionalized 
usage has `i.large` essentially meaning any breed other 
than truly diminutive pets bred over generations for 
that tiny size.  There is probably also a social dynamic 
in play; dog owners appear to classify themselves 
(and one another) as `q.small dog` or `q.large dog` 
lovers.  A 40 pound pit bull and 120 pound mastif would both 
be called `i.large` in part because the pit bull's owner 
is more likely to think of herself as the kind of 
person that is drawn to `q.large` dogs.
`p`

`p.
Obviously, linguists (much less logicians) do not do 
sociological surveys of dog owners, so these 
usage pattern do not really manifest as logical 
rationales for linguistic predication.  That's the point; 
our instinct as languages users is to search for 
entrenched conventions rather than to parse 
phrases as novel logical combinations, so we 
decode `i.large dog` through whatever mental script 
best captures the entrenched usage, whether or not 
these are actually logically accurate.  Dog owners 
would not start calling pit bulls `q.small` even if 
it were statistically demonstrated that the average 
pit bull were actually on the small end of the 
dog-size scale. 
`p`

`p.
Nor is this actually being stubbornly irrational: the 
reason why we are interested in a dog's size is usually 
because it indirectly implies how the dog might behave.  
A pit bull should be deemed large insofar as it is 
more likely to act similarly to an actual large 
dog than to a chihuahua.  Likewise a large chihuahua
will still act like a small dog, which is more important 
in most practical context than the fact that it 
`i.is` small in a statistical sense.  
This is the same pattern as I analyzed `visavis; 
`i.friendly dog`/: we do not intrinsically read 
this as a logical unit, but as a compound signification 
targeting some script most people have sketching out 
how friendly dogs behave and how most people then respond.
`p`


`p.  
These anticipations are not intrinsic to the notion of 
friendliness, so they are not simply etched out in the 
word's lexical resonances.  We can, for example, 
expect that a dog's friendliness will be manifest by 
their propensity to lick our ankles or sniff around 
our waist.  Those are not behaviors we would 
associate with friendliness in other environments 
%-- say, when we describe a shopkeeper as 
`q.friendly`/, or a politician as friendly to 
advocates of a political cause.  Each use of 
`q.friendly` bears a distinct implication as to 
what the predicate implies for the bearer's 
behavior `i.in that context`/.   
`p`

`p.
The upshot is that interpretive scripts need to be 
selected for %-- and are solicited by %-- even small-scale 
linguistic units, like a word-pair (e.g., adjective-to-noun).  
These smaller interpretive stages get absorbed into larger 
interpretive processes; see how `i.friendly dog` becomes 
a unit in a spatial and descriptive scene in (1).  
So %-- insofar as natural language grammar describes 
how syntax (and word morphology) connects words 
together (as adjective to noun, verb to subject, 
subject to object, and so forth) %-- we can see 
grammar as stating the rules by which we 
identify some words as interpretive modifiers to 
others, their fact of being thereby linked spurring a 
cognitive sifting for the optimal interpretive 
procedure reciprocating speakers' intent. 
`p`

`p.
This is still a rudimentary analysis, and 
elsewhere (in a paper accompanying this one 
in the software demo) I have developed these 
ideas more substantially, through a theory I 
described as `q.Cognitive Transform Grammar`/.  
The minimal intuition of that theory is that 
grammar describes how words should be paired 
up (or more preceisely grammar describes 
syntactic formations that convey intended pairings), 
and that any pairing profiles a cognitive, or 
interpretive, process; whereby one word 
(a `q.modifier`/) somehow alters our constural of 
(the signified or referent of) its partner 
(a `q.ground`/).  Moreover, indirectly, the modifier 
transforms our overall appraisal of a situation by 
staging a focused modification, or revised 
interpretation, of the ground %-- passing from 
`i.dog` to `i.friendly dog` both refines our 
conceptualization of the dog in question `i.and` 
adds an important shading to our understanding 
of the overall circumstance spanned by our interaction 
with the dog.   
`p`

`p.
I also think that grammar formalisms based on hypergraphs 
%-- and something like a theory of channels 
%-- can apply in the natural language context.  
One reason is that interpretive juxtapositions 
between words can draw in further removed 
syntactic contexts, refining the `q.local` interpretive 
process.  Consider the difference between: 

`sentenceList,
`sentenceItem; A few times we passed friendly dogs who barked at us.
`sentenceItem; We passed friendly dogs who barked at us a few times.
`sentenceList`

Here (1) is more likely heard as identifying `i.different` groups of 
dogs, with a `q.friendly dogs barking` scenario repeated for several 
occasions; while (2) more likely reads as only naming `i.one` such 
occasion.  The phrase `i.friendly dogs` is interpreted differently 
depending on how we read the scope of the `i.a few times` quantifier, 
and whether we the scenario is understood as specific or as 
generic and replicated.  I would argue that this constitutes 
contextual data that gets pulled in alongside the 
interpretive effect of `i.friendly` on `i.dogs`/, as if 
following a distinct channel for interpretive nuance 
alongside the `q.local channel` of co-modifying word pairs.  
These extra `q.non-local` channels can be modeled as an 
overlay structure, i.e. a hypergraph, above word-pair 
graphs which would be the primary syntactic representatums 
for graph-based grammars, e.g., Dependency Grammar.    
`p`

`p.
But the key point of my analysis is that linguistic formations 
have to trigger interpretive processes which are, at least 
in part, extralinguistic.  This influences how we should 
see the role of natural language grammar, because the point of 
syntax is not to describe phrase constructions which, 
by their internal logical pattern %-- e.g. by appending 
the property `i.friendly` to `i.a dog` %-- fully 
explicate an intended signification.  Rather, syntax 
is ordered to guide addressees, within their response to 
linguistic content, toward extralinguistic cognitions %-- the 
`i.friendly dogs` phrase being semantically complete not 
by logically predicating `q.friendliness` but by 
being received as in instruction to load up the 
mental frame, the cognitive anticipation-package, 
of `i.friendly dogs`/, within suitably typical ambient situations.
`p`

`p.
In short, this theory builds off the idea that semantis is, 
at root, cognitively procedural; and that syntax thereby is a 
process of forming aggregates whose pattern triggers 
the right procedure, rather than a logically self-contained 
signification.  This, I would argue, is quite consistent 
with formal grammars organized around the idea that 
formal languages %-- or at least prorgramming languages 
%-- are, themselves, at a semantic and pragmatic level, 
intrinsically procedural; although the notion of 
`q.procedure` in a computational context is obviously 
different from cognitive or interpretive procedures.  
Modulo the diversity in senses of `q.procedure`/, 
we can still say that procedure-oriented semantics 
retroactively compels procedure-oriented grammar, 
and that this trend cuts across both 
formal and natural language grammars. 
`p`

`p.
Here then is a case for unifying hypergraph grammars 
across the formal/natural delineation, to 
complement the projection of a Conceptual Space Theory 
similarly straddling that line.  I have addressed, 
then, some points of similarity and difference 
between grammar and semantics in both 
formal and natural-language contexts, from the 
perspective of a Hypergraph/Conceptual Space 
unification.  To what degree do these individual 
comparisons add up to a plausible framework 
for combining Hypergraphs and Conceptual Space Theory 
in both formal and natural contexts? 
`p`

`subsubsection.Limitations of Conceptual Space Semantics`
`p.
Because a lot of the substance of natural language 
semantics, I would argue, is `i.extra`/-linguistic, 
any semantic formalism has only limited value 
against languages' complexities and nuances.  
I think this applies to the core `Gardenfors; Conceptual Space Theory, 
but also to any other `i.a priori` semantic methodology.  
The only non-reductive option for truly unpack 
semantic structures is to take each sentence, each 
language artifact, on a case-by-case basis, explaining 
the interpretive fiats and contextual idiosyncracies 
of every usage.  General theories can still be 
guiding framework for such case-by-case analysis, 
however, and Conceptual Spaces are certainly one 
of the devices which belong in the linguists' 
and philosopher of langage's toolkit.
`p`

`p.
For formal types, I have argued that Conceptual Spaces are 
most relevant when it comes to interface design, and 
in particular to that portion of a types' interface 
I called `i.axiatropic`/, where intra-type comparisons 
can be ordered and/or measured.  The notion of 
partioning a types' `q.space` of potential instances is 
also relevant, I argued, to reason about types' 
constructors and construction patterns %-- to 
their `q.constructive interface`/.  More generally, 
the notion of types' `i.interface` being internally 
organized, with different parts exposing different 
facets of types' operational and conceptual rationales, 
is itself perhaps an embodiment of 
types being understood in some sense as 
`q.conceptual spaces`/.  Not every interface 
facet may involve quantitative dimensions, but 
interface design does reveal the workings 
of human conceptualization crafting the 
type as an organized space, a technical artifact 
which encompasses and renders computationally 
tractable a spectrum of values.
`p`

`p.
I would argue, then, that a systematic theory of 
interface design would be a reasonable formal 
application of Conceptual Space theory.  
To some degree this is already hinted at in 
existing sofware-oriented tools based on 
Conceptual Spaces, such as `CSML;; my 
`q.`NEMAM;` model of type-interface facets 
can legitimately be presented as a superset of 
the `CSML; semantics. 
`p`

`p.
So I think we have strong theoretical grounds 
for pursuing a combined grammar/semantics 
paradigm in the `i.formal` context which 
covers (channelized) hypegraphs on the 
syntactic end with `NCFour; type systems 
and the `q.`NEMAM;` type-interface model.  
How this might look in practice is 
suggested perhaps by the code base accompanying 
this paper (and others distributed with the 
data set), which essentially operationalizes 
a Channelized Hypergraph/`NCFour; combination.  
I also believe a strong case can be made for 
connecting Channelized Hypergraph grammars in the 
programming language context to natural language 
syntax, e.g. what I have analyzed as 
`q.Cognitive Transform Grammar`/.  
`p`

`p.
Conceptual Space semantics for natural language 
remains rather an odd man out for this unification, 
perhaps.  I have presented reasons why this may be 
the case, concerning the extra-linguistic 
reaches of any comprehensive natural semantics, 
which would seem to inhibit `i.any` substative 
analogy between formal languages (or formal type 
theories, formal semantics, etc.) and human 
language.  The question though is whether this 
would `i.practically` limit the applicability 
of a Hypergraph/Conceptual Space unification for 
linguistics (in a non-reductive vein) and humanities 
in general.  This in turn depends on the 
proposed ends to which a `HCS; model might be 
applied in a humanities context, which I'll 
comment on in conclusion. 
`p`

