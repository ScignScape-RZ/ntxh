
\documentclass{statsoc}

\usepackage[a4paper, outer=1cm, inner=1cm,top=1.8cm,bottom=.8cm]{geometry}

\usepackage[T1]{fontenc}

\usepackage{tgbonum}

\usepackage{graphicx}
\usepackage[textwidth=8em,textsize=small]{todonotes}
\usepackage{amsmath}
\usepackage{natbib}

\newcommand{\p}[1]{

\vspace{.75em}#1}

\setlength{\parindent}{60pt}

\colorlet{orr}{orange!60!red}
\newcommand{\textscc}[1]{{\color{orr!35!black}{{%
						\fontfamily{Cabin-TLF}\fontseries{b}\selectfont{\textsc{\scriptsize{#1}}}}}}}
\newcommand{\AcronymText}[1]{{\textscc{#1}}}


\newcommand{\q}[1]{{\fontfamily{qcr}\selectfont ``}#1{\fontfamily{qcr}\selectfont ''}} 
\newcommand{\API}{\resizebox{!}{7pt}{\AcronymText{API}}}

\newcommand{\PACS}{\resizebox{!}{7pt}{\AcronymText{PACS}}}
\newcommand{\EMR}{\resizebox{!}{7pt}{\AcronymText{EMR}}}

\newcommand{\HCI}{\resizebox{!}{7pt}{\AcronymText{HCI}}}

\newcommand{\TDM}{\resizebox{!}{7pt}{\AcronymText{TDM}}}


\newcommand{\visavis}{vis-\`a-vis}

\usepackage{enumitem}
\usepackage{setspace}

\setlist[itemize]{topsep=20pt,before=\leavevmode\vspace{-1.5em}}


\colorlet{dsl}{purple!20!brown}
\colorlet{dslr}{dsl!50!blue}

\setlist[description]{%
  topsep=10pt,
  labelsep=12pt,
  itemsep=12pt,               % space between items
  font=\normalfont\bfseries\color{dslr!50!black}, % if colour is needed
  style=nextline 
}

\colorlet{rgrey}{red!30!grey}


\newif\ifSummaryInText
\SummaryInTexttrue

\makeatletter
\long\def\grabsummary#1#2\end{%
  \applydraftsummary{#2}
  \end}

\long\def\applydraftsummary#1{%
\vspace{-2pt}
  \noindent\hfil\textcolor{rgrey!50!purple}{\rule{0.5\textwidth}{.4pt}}\hfil 

  \vspace{-2pt}{%
{\fontfamily{phv}\fontsize{9}{11}\selectfont #1}
}\\\vspace{-20pt}
  {\begin{center}\textcolor{rgrey!50}{\rule{0.5\textwidth}{.4pt}}\end{center}}}

\newenvironment{summaryx}[1][0]{\let\BEGIN\begin\let\END\end\grabsummary{#1}}{}%
\makeatother


\newenvironment{summary}{\\\vspace{-4pt}%
%
%
\noindent\hfil\textcolor{rgrey!50!purple}{\rule{0.5\textwidth}{.4pt}}\hfil

\hspace{-2cm}\begin{minipage}{1.02\textwidth}\fontfamily{phv}\fontsize{9}{11}\selectfont}%
{\\\vspace{-1em}\end{minipage}
%\vspace*{-2em}
{\begin{center}\textcolor{rgrey!50!yellow}{\rule{0.5\textwidth}{.4pt}}\end{center}}
\vspace{2em}}%


\title[Data Modeling and Text Mining for Covid-19]{Advances in Data Modeling and Text Mining for Covid-19 Research}
\author[Amy Neustein]{Amy Neustein}
\author[Amy Neustein]{Nathaniel Christen}

\begin{document}

\vspace{1em}
\noindent{}Approx 250 pages, 15 chapters\\
Manuscript Submission Date: August 31, 2020\\

{\fontsize{10}{14}\selectfont
\section{Introduction}

\p{The Covid-19 pandemic has inspired an unprecedented 
convergence of scientific research, driven in part 
by publishers choosing to allow open public access 
to many research papers and published data 
relevant to Covid-19 (the disease) and SARS-CoV-2 
(the viral agent).  The sheer volume of this data 
presents both a practical challenge --- how should 
scientists find the information most relevant to them? 
--- and a valuable case-study in the difficulties and 
possibilities for curating multi-disiplinary 
information spaces unified around a common 
scientific theme (in this case, 
determining how best to treat Covid-19 and to 
mitigate the global SARS-CoV-2 pandemic).}

\p{In a matter of mere weeks, the scientific and publication 
community has essentially engineered the origination of 
an entirely new data ecosystem, unified around the 
challenges of studying and predicting properties 
of Covid-19 and SARS-CoV-2 at the molecular, genomic, 
epidemiological, clinical, and public-health levels.  
In many respects, this ecosystem has emerged 
haphazardly, with the rush to publicly share 
text and data taking priority over rigorous 
curation and accuracy.  In this context, 
scientists and policymakers may benefit from a 
volume which provides a critical and analytic 
overview of the Covid-19 data ecosystem: the 
different genres of data which are marshalled 
toward scientific investigation of Covid-19 
and SARS-CoV-2; how this data is obtained, 
consumed, analyzed, and interpreted; how 
research data supports scientific claims 
pertaining to Covid-19's biological and 
epidemiological mechanisms and trajectory; 
and how these scientific claims should 
translate into public policy, taking into 
consideration both the importance of 
basing government actions on empirical data and 
the gaps in scientific knowledge which make 
such data inexact and provisional.} 

\subsection{Audience}
\p{Against this background, our proposed volume 
will be directed at two distinct audiences.  
At one level, we intend to examine the 
operational logistics of Covid-19 data: its 
structures, protocols, analytic methodology, 
and empirical significance.  That is, we 
intend to identify the distinct scientific 
disciplines which each concern one facet 
of Covid-19 research --- molecular biology, 
genomics, radiology, epidemiology, clinical 
informatics, and their variatious subfields 
--- and, for each of these disciplines, 
review their distinct paradigms for 
data acquisition, analysis, and modeling.  
The point of these expositions is to bring 
the reader from conceiving \q{data} as something 
abstract and amorphous, to understanding 
data as the building-blocks of scientific 
research and biomedical claims.  One way to supply this backstory 
is to examine Covid-19 data from the viewpoint 
of software engineering: to demonstrate 
the methodology for data acquisition and 
management from the perspective of programmers 
implementing software which manipulates Covid-19 data.  
This explication would therefore examine the 
data structures, file formats, \API{} protocols, 
and other technical details intrinsic to writing 
code which works with Covid-19 data as a 
digital artifact.  Such an exposition might be 
of interest to programmers who actually are 
writing Covid-19-related code, but the primary 
goal of these discussions will be to help 
scientists (who may be well-versed in data 
structures relevant to their specialization 
but less so \visavis{} other disciplines), 
policymakers, and the general public understand 
the technical chains which transform 
Covid-19 information from the realm of 
laboratories and experiments to the realm of 
public health and policy.}

\p{Apart from that topical focus --- the analysis 
of Covid-19 data as a concrete ecosystem 
manifest in standardized formats, global 
identifiers, and other concrete information 
artifacts --- we also intend to write for a 
more theoretical audience for whom the 
pandemic is a unique case-study in data 
curation and integration.  From this perspective, 
the Covid-19 data ecosystem is a concrete example 
through which theories of data integration can 
be presented and exercised.}

\subsection{Data Integration in the Covid-19 Context}
\p{There are two distinct phenomena which render 
inter-disciplinary data integration significant for 
Covid-19 in particular, and clinical/biomedical practices 
in general.  First, certain forms of analysis explicitly 
combine information or statistical parameters from 
distinct subject areas.  For example, in addition 
to epidemiological models of SARS-Cov-2 within an 
entire population, it is important to study the 
present or projected spread of the disease among 
different social groups, identified by age, gender, 
race, economic status, and so forth.  This form 
of analysis will therefore merge epidemiological 
and sociodemographic data and methods; as such it is an 
instance of analyses wherein it is explicitly necessary 
to pool data that is typically represented 
via different schemas, and accessed via different 
protocols, into a single algorithmic or computational 
environment.  This volume will therefore 
examine cross-disciplinary analysis along these 
lines as case studies of data integration on a procedural 
level: how computer code can obtain and marshal heterogeneous 
data into a common form suitable for qualitative and quantitative analysis.} 
   
\p{The second context where multi-disciplinary integration 
becomes relevant operates at a higher level: the development 
of heterogenous information spaces which can absorb 
data from many environments, evincing a variety of 
disciplinary orientations.  The rationale for such heterogeneous 
repositories is often practical and logistical; institutions 
have operational reasons for curating a single, comprehensive data ecosystem shared 
by multiple information producers and consumers, such as 
a \q{Semantic Data Lake}.  In these situations, one 
large central repository will take the place of numerous 
narrower, domain-specific databases.  A central repository 
may be subdivided into smaller components implementing 
narrower protocols --- a clinical software network may provide 
diagnostic images via a \PACS{} (Picture Archiving and Communication 
System) service, and treatment/outcome data via an 
\EMR{} (Electronic Medical Record) architecture.  The 
structure and use of data in these two environments 
(\PACS{} and \EMR{}) is very different.  Nevertheless, 
institutions will often unify these systems into 
a single data platform, for logistical reasons: it 
is more convenient for doctors and researchers to 
have a single access point, a single login account, 
a single query framework, etc., which accesses the 
totality of information used across the organization's 
activities.}

\p{These institutional repositories present 
challenges which are different from granular 
syntheses of heterogeneous data into a single 
procedural/algorithmic context.  Disparate data structures 
in a heterogenous archive, such as a \q{Data Lake}, may 
never be directly combined in a single computation.  
Nevertheless, Data Lakes and their kin seek to 
provide a single software, query, and accession infrastructure 
which can accommodate a diversity of data models, and 
this diversity presents technological challenges.  
It appears that Covid-19 demonstrates the problems 
engendered by these complexities in a tangible way, 
insofar as health and governmental officials have criticised 
the lack of integrated data across disciplinary and 
jurisdictional boundaries --- poor coordination between 
city, state, and federal governments in the US, for 
example, as well as between medical and governmental institutions.  
Covid-19 therefore offers a case-study in the challenges 
of implementing large-scale heterogeneous data repositories, 
and we intend to offer theoretical analyses and practical 
recommendations which could potentially improve such 
technology in the future.} 

\p{The volume will therefore be organized in a pattern 
which progresses from domain-specific to integrative 
styles of analysis: the first part will focus on data 
models and protocols within individual disiplines 
relevant to Covid-19, while the second part will discuss 
cross-disiplinary integration at both a theoretical and 
practical level.  The third part will then delve deeper 
into integrative paradigms in several areas, particularly 
text mining and software development.}  

\section{Table of Contents}

\begin{description}

\item[Foreword (Invited)]

\item[Authors' Introduction]

\item[Part I: Architecting Data Models for Scientific Disciplines Associated with Covid-19]

\begin{itemize}
\item Chapter 1: Data structures for molecular biology and virology

\begin{summary}
This chapter will consider data pertaining to scientists' 
investigation of SARS-Cov-2's viral mechanisms.  It will 
consider how data modeling the pathogen's proteins, 
physical structure, and interactions with human cells 
is collected and utilized.  Emphasis will be placed 
on cheminformatic pipelines and protocols for 
working with molecular-biological information.   
\end{summary}

\item Chapter 2: How Genomic Data is Stored and Analyzed in the Coronavirus Context

\begin{summary}
This chapter will examine the genomic data ecosystem with SARS-Cov-2 
as a case-study.  It will focus on the challenges presented 
by the large scale of genomic data, and how these challenges 
are addressed through data acquisition protocols and 
distributed software networks. 
\end{summary}

\item Chapter 3: Structuring of Radiographic and Diagnostic Data in the Context of Covid-19

\begin{summary}
This chapter will focus on information germane to identifying Covid-19 infections.  
It will be centered on diagnostic imaging, but will also consider 
the structure of data generated by Covid-19 tests or 
biometric indicators of possible SARS-Cov-2 infection.
\end{summary}

\item Chapter 4: Reviewing Epidemiological Structures and Methodology for SARS-Cov-2 Research

\begin{summary}
This chapter will consider epidemiological modeling both \textit{a posteriori}, 
via empirical clinical data, and simulations, analyses which try to 
predict different possible trajectories for the Covid-19 pandemic.  
The chapter will review fundamental epidemiologic measures such 
as infection and transmission rate, mortality, and R$_O$, documenting 
how these magnitudes are assessed both empirically and theoretically. 
\end{summary}

\item Chapter 5: Modeling Clinical Data in the Covid-19 Patient Population  

\begin{summary}
This chapter will examing medical records, in the context of 
Covid-19 as a case-study: formats for representing 
diagnostic, treatment, and outcome data in a clinical setting, 
and the construction of patient cohorts.  Focus will be 
placed on one specific technology --- the 
Clinical Looking Glass software application and Object Model --- 
as a representative example of how patient-centered 
data is structured and queried.
\end{summary}

\end{itemize}

\item[Part II: Creating a Cross-Disciplinary Ecosystem for Covid-19]

\begin{itemize}

\item Chapter 6: Approaches for Merging Heterogeneous Data Sets: Ontologies and Hypergraphs

\begin{summary}
This chapter will review the use of ontologies to model 
specific information domains and, via ontology integration, 
to construct unified knowledge systems that encompass 
multiple domains.  The chapter will present varieties 
of hypergraphs as generic data containers applicable 
to heterogenous domains, with the goal of identifying 
sufficiently generic data structures appropriate 
for cross-disciplinary integrated ontologies.   
\end{summary}

\item Chapter 7: Scientific Workflows and Inter-Application Networking: Reviewing data pipelines commonly used in Covid-19 research

\begin{summary}
This chapter will focus on \API{}s, command-line interfaces, 
and related technologies through which software components 
communicate with one another.  The purpose of this review is 
to direct attention not at specific procedures which are 
implemented within a given software component, but rather 
how components expose functionality to the 
\q{outside world}.  This review can then set the 
stage for a more internal focus in the following chapter. 
\end{summary}

\item Chapter 8: Formal Procedural Models: Representing computational procedures applicable 
to Covid-19

\begin{summary}
This chapter will concentrate more rigorously on individual 
procedures implemented within software component.  
The goal is to use external interface models (discussed 
in the previous chapter) as entry-points to analyzing 
components at a procedural level: a good method for 
analyzing software functionality is to examine how 
internal procedures provide the capabilities exposed 
to an external interface.  On this basis we will 
develop a general model of procedural properties 
which adequately represents computational logic 
across different programming languages and 
software-development methodologies.   
\end{summary}

\item Chapter 9: Integrating Procedural and Data Models

\begin{summary}
This chapter will unify the discussion within the prior 
three chapters, which will have yielded first a 
general and cross-domain model of data structures, and 
second a paradigm-agnostic model of procedural logic.  
The current chapter will merge both models into an 
overarching paradigm, exploiting the fact that 
procedural types and requirements are representable 
as data structures in their own right. 
\end{summary}

\item Chapter 10: Type Theories for Procedural Data Modeling

\begin{summary}
This chapter will provide a formal structure supporting 
the analysis developed in Chapter 9.  The priority 
in this chapter is codifying a hypergraph-based 
type theory, one which exploits the hypergraph context 
to concretely anchor a type system: each inhabited 
type is by definition an attribute of one or more 
hypernodes.  This starting-point, along with the 
specification of a particular genre of 
\q{channelized} hypergraphs (introduced preliminarily in 
Nathaniel Christen's chapter in Amy Neustein's 
just-published \textit{Advances in Ubiquitous Computing} 
volume), permits the construction of a general-purpose 
type theory applicable to most programming environments.  
\end{summary}

\end{itemize}

\item[Part III: Text and Data Mining for Covid-19]

\begin{itemize}

\item Chapter 11: Applying Data Mining Techniques to Covid-19 Research Corpora

\begin{summary}
This chapter will take a concrete look at research data published 
in conjunction with Covid-19 data-sharing initiatives.  
The machinery of the prior chapters will be leveraged 
to examine this existing data in a rigorous fashion, 
using formally described data types and hypergraph 
meta-models as methodologically tools in surveying the 
Covid-19 data ecosystem.
\end{summary}

\item Chapter 12: Text Mining of Covid-19 Publication Archives

\begin{summary}
This chapter will examine Covid-19 corpora with a concrete 
focus that overlaps with Chapter 11's, but with an emphasis 
on text rather than data mining.  The goal of the 
present chapter is to review publishers' efforts to 
provide open-access document corpora to support 
Covid-19 research, and to demonstrate how text 
mining tools can make these corpora more valuable.
\end{summary}

\item Chapter 13: Human-Computer Interaction Approaches for Covid-19 Software 

\begin{summary}
This chapter will apply hypergraph-based type theory 
(as developed in Part II) to \HCI{} methodology, 
with Covid-19 Text and Data Mining (\TDM{}) as a case-study.  
The goal of this chapter is to consider \TDM{} software 
targeting Covid-19 corpora as concrete examples illustrating 
how \HCI{} concerns may be analyzed through the lens 
of the data models and type systems developed earlier in 
the volume. 
\end{summary}

\item Chapter 14: Using Text-Mining Tools to Extract Medical History 
from Clinical Narratives  

\begin{summary}
This chapter will examine how text-mining technology considered 
in Chapter 12 may be applied to clinical and patient narratives ...  
\end{summary}


\item Chapter 15: Annotating Patient Narratives for Emerging 
Covid-19 Symptomatology 

\begin{summary}
This chapter will introduce techniques for encoding 
rhetorical structures identified within patient 
narratives ...  
\end{summary}


\end{itemize}

\end{description}

%\end{spacing}
%}

\end{document}


