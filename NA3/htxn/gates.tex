\documentclass[11pt,letterpaper]{article}

\usepackage[T1]{fontenc}
\usepackage{tgtermes}

%\usepackage{mathptmx}

\usepackage{eso-pic}

%\setlength\parindent{0pt}

\AddToShipoutPictureBG{%

\ifnum\value{page}>1{
\AtTextUpperLeft{
\makebox[18.5cm][r]{
\raisebox{-2.3cm}{%
{\transparent{0.3}{\includegraphics[width=0.29\textwidth]{e-logo.png}}	}} } }
}\fi
}

\AddToShipoutPicture{%
{
 {\color{blGreen!70!red}\transparent{0.9}{\put(0,0){\rule{3pt}{\paperheight}}}}%
 {\color{darkRed!70!purple}\transparent{1}\put(3,0){{\rule{4pt}{\paperheight}}}}
% {\color{logoPeach!80!cyan}\transparent{0.5}{\put(0,700){\rule{1cm}{.6cm}}}}%
% {\color{darkRed!60!cyan}\transparent{0.7}\put(0,706){{\rule{1cm}{.6cm}}}}
% \put(18,726){\thepage}
% \transparent{0.8}
}
}



\AddToShipoutPicture{%

\ifnum\value{page}>0


{\color{blGreen!70!red}\transparent{0.9}{\put(300,8){\rule{0.5\paperwidth}{.3cm}}}}%
{\color{inOne}\transparent{0.8}{\put(300,10){\rule{0.5\paperwidth}{.3cm}}}}%
{\color{inTwo}\transparent{0.3}\put(300,13){{\rule{0.5\paperwidth}{.3cm}}}}

\put(301,16){%
\transparent{0.7}{
\includegraphics[width=0.2\textwidth]{logo.png}} }

{\color{blGreen!70!red}\transparent{0.9}{\put(5.6,5){\rule{0.5\paperwidth}{.4cm}}}}%
{\color{inOne}\transparent{1}{\put(5.6,10){\rule{0.5\paperwidth}{.4cm}}}}%
{\color{inTwo}\transparent{0.3}\put(5.6,15){{\rule{0.5\paperwidth}{.4cm}}}}

\fi
}

%\pagestyle{empty} % no page number
%\parskip 7.2pt    % space between paragraphs
%\parindent 12pt   % indent for new paragraph
%\textwidth 4.5in  % width of text
%\columnsep 0.8in  % separation between columns

\setlength{\footskip}{23pt}

\usepackage[paperheight=14in,paperwidth=8.5in]{geometry}
\geometry{left=1.3in,top=1.1in,right=.62in,bottom=1.8in} %margins

%\linespread{1.3}

\newcommand{\sectsp}{\vspace{12pt}}

\usepackage{graphicx}
\usepackage{color,framed}

\usepackage{textcomp}

\usepackage{float}

\usepackage{mdframed}


\usepackage{setspace}
\newcommand{\rpdfNotice}[1]{\begin{onehalfspacing}{

\Large #1

}\end{onehalfspacing}}

\usepackage{xcolor}

\usepackage[hyphenbreaks]{breakurl}
\usepackage[hyphens]{url}

\usepackage{hyperref}
\newcommand{\rpdfLink}[1]{\href{#1}{\small{#1}}}
\newcommand{\dblHref}[1]{\href{#1}{\small{\burl{#1}}}}
\newcommand{\browseHref}[2]{\href{#1}{\Large #2}}

\colorlet{blCyan}{cyan!50!blue}

\definecolor{darkRed}{rgb}{.2,.0,.1}

\hypersetup{
    colorlinks=true,
    linkcolor=blCyan!80!darkRed,
    filecolor=magenta,
    urlcolor=blue,
}

\urlstyle{same}

\definecolor{blGreen}{rgb}{.2,.7,.3}

\definecolor{darkBlGreen}{rgb}{.1,.3,.2}

\definecolor{oldBlColor}{rgb}{.2,.7,.3}

\definecolor{blColor}{rgb}{.1,.3,.2}

\definecolor{elColor}{rgb}{.2,.1,0}
\definecolor{flColor}{rgb}{0.7,0.3,0.3}

\definecolor{logoOrange}{RGB}{108, 18, 30}
\definecolor{logoGreen}{RGB}{85, 153, 89}
\definecolor{logoPurple}{RGB}{200, 208, 30}

\definecolor{logoBlue}{RGB}{4, 2, 25}
\definecolor{logoPeach}{RGB}{255, 159, 102}
\definecolor{logoCyan}{RGB}{66, 206, 244}
\definecolor{logoRed}{rgb}{.3,0,0}

\definecolor{inOne}{rgb}{0.122, 0.435, 0.698}% Rule colour
\definecolor{inTwo}{rgb}{0.122, 0.698, 0.435}% Rule colour

\definecolor{outOne}{rgb}{0.435, 0.698, 0.122}% Rule colour
\definecolor{outTwo}{rgb}{0.698, 0.435, 0.122}% Rule colour

\usepackage[many]{tcolorbox}% http://ctan.org/pkg/tcolorbox

\usepackage{transparent}

\newenvironment{cframed}{\begin{mdframed}[linecolor=logoPeach,linewidth=0.4mm]}{\end{mdframed}}

\newenvironment{ccframed}{\begin{mdframed}[backgroundcolor=logoGreen!5,linecolor=logoCyan!50!black,linewidth=0.4mm]}{\end{mdframed}}

\usepackage{aurical}
\usepackage[T1]{fontenc}

\usepackage{relsize}

\newcommand{\bref}[1]{\hspace*{1pt}\textbf{\ref{#1}}}

\newcommand{\pseudoIndent}{

\vspace{10pt}\hspace*{12pt}}

\newcommand{\YPDFI}{{\fontfamily{fvs}\selectfont YPDF-Interactive}}

%
\newcommand{\deconum}[1]{{\protect\raisebox{-1pt}{{\LARGE #1}}}}

\newcommand{\visavis}{vis-\`a-vis}

\newcommand{\VersatileUX}{{\color{red!85!black}{\Fontauri Versatile}}%
{{\fontfamily{qhv}\selectfont\smaller UX}}}

\newcommand{\NDPCloud}{{\color{red!15!black}%
{\fontfamily{qhv}\selectfont {\smaller NDP C{\smaller LOUD}}}}}

\newcommand{\MThreeK}{{\color{blue!15!black}%
{\fontfamily{qhv}\selectfont {M3K}}}}


\newcommand{\lfNDPCloud}{{\color{red!15!black}%
{\fontfamily{qhv}\selectfont N{\smaller DP C{\smaller LOUD}}}}}

\newcommand{\textds}[1]{{\fontfamily{lmdh}\selectfont{%
\raisebox{-1pt}{#1}}}}

\newcommand{\dsC}{{\textds{ds}{\fontfamily{qhv}\selectfont \raisebox{-1pt}
{\color{red!15!black}{C}}}}}

\definecolor{tcolor}{RGB}{24,52,61}

\newcommand{\CCpp}{\resizebox{!}{7pt}{\AcronymText{C}}/\Cpp{}}
\newcommand{\NoSQL}{\resizebox{!}{7pt}{\AcronymText{NoSQL}}}
\newcommand{\SQL}{\resizebox{!}{7pt}{\AcronymText{SQL}}}

\newcommand{\NCBI}{\resizebox{!}{7pt}{\AcronymText{NCBI}}}

\newcommand{\HTXN}{\resizebox{!}{7pt}{\AcronymText{HTXN}}}
\newcommand{\lHTXN}{\resizebox{!}{8.5pt}{\AcronymText{HTXN}}}
\newcommand{\lsHTXN}{\resizebox{!}{9.5pt}{\AcronymText{\textcolor{tcolor}{HTXN}}}}


\usepackage{mdframed}

\newcommand{\cframedboxpanda}[1]{\begin{mdframed}[linecolor=yellow!70!blue,linewidth=0.4mm]#1\end{mdframed}}


\newcommand{\PVD}{\resizebox{!}{7pt}{\AcronymText{PVD}}}

\newcommand{\THQL}{\resizebox{!}{7pt}{\AcronymText{THQL}}}
\newcommand{\lTHQL}{\resizebox{!}{8pt}{\AcronymText{THQL}}}

\newcommand{\SDK}{\resizebox{!}{7pt}{\AcronymText{SDK}}}
\newcommand{\NLP}{\resizebox{!}{7pt}{\AcronymText{NLP}}}



\newcommand{\sapp}{\resizebox{!}{7pt}{\AcronymText{Sapien+}}}
\newcommand{\lsapp}{\resizebox{!}{8.5pt}{\AcronymText{Sapien+}}}
\newcommand{\lssapp}{\resizebox{!}{9.5pt}{\AcronymText{Sapien+}}}

\newcommand{\ePub}{\resizebox{!}{7pt}{\AcronymText{ePub}}}

%\lsLPF


\newcommand{\GIT}{\resizebox{!}{7pt}{\AcronymText{GIT}}}

\newcommand{\LPF}{\resizebox{!}{7pt}{\AcronymText{LPF}}}
\newcommand{\lLPF}{\resizebox{!}{8.5pt}{\AcronymText{LPF}}}
\newcommand{\lsLPF}{\resizebox{!}{9.5pt}{\AcronymText{LPF}}}

\makeatletter

\newcommand*\getX[1]{\expandafter\getX@i#1\@nil}

\newcommand*\getY[1]{\expandafter\getY@i#1\@nil}
\def\getX@i#1,#2\@nil{#1}
\def\getY@i#1,#2\@nil{#2}
\makeatother
	
\newcommand{\rectann}[9]{%
\path [draw=#1,draw opacity=#2,line width=#3, fill=#4, fill opacity = #5, even odd rule] %
(#6) rectangle(\getX{#6}+#7,\getY{#6}+#8)
({\getX{#6}+((#7-(#7*#9))/2)},{\getY{#6}+((#8-(#8*#9))/2)}) rectangle %
({\getX{#6}+((#7-(#7*#9))/2)+#7*#9},{\getY{#6}+((#8-(#8*#9))/2)+#8*#9});}


\definecolor{pfcolor}{RGB}{94, 54, 73}

\newcommand{\EPF}{\resizebox{!}{7pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}
\newcommand{\lEPF}{\resizebox{!}{8.5pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}
\newcommand{\lsEPF}{\resizebox{!}{9.5pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}


\newcommand{\XPDF}{\resizebox{!}{8.5pt}{\AcronymText{XPDF}}}

\newcommand{\GRE}{\resizebox{!}{8.5pt}{\AcronymText{GRE}}}

\newcommand{\lMOSAIC}{\resizebox{!}{8.5pt}{\AcronymText{MOSAIC}}}

\newcommand{\XML}{\resizebox{!}{7pt}{\AcronymText{XML}}}
\newcommand{\RDF}{\resizebox{!}{7pt}{\AcronymText{RDF}}}
\newcommand{\DOM}{\resizebox{!}{7pt}{\AcronymText{DOM}}}

\newcommand{\CLang}{\resizebox{!}{7pt}{\AcronymText{C}}}

\newcommand{\HNaN}{\resizebox{!}{7pt}{\AcronymText{HN%
\textsc{a}N}}}

\newcommand{\JSON}{\resizebox{!}{7pt}{\AcronymText{JSON}}}

\newcommand{\MeshLab}{\resizebox{!}{7pt}{\AcronymText{MeshLab}}}
\newcommand{\IQmol}{\resizebox{!}{7pt}{\AcronymText{IQmol}}}

\newcommand{\SGML}{\resizebox{!}{7pt}{\AcronymText{SGML}}}

\newcommand{\GUI}{\resizebox{!}{7pt}{\AcronymText{GUI}}}

\newcommand{\API}{\resizebox{!}{7pt}{\AcronymText{API}}}

\newcommand{\SDI}{\resizebox{!}{7pt}{\AcronymText{SDI}}}

\newcommand{\IDE}{\resizebox{!}{7pt}{\AcronymText{IDE}}}

\newcommand{\ThreeD}{\resizebox{!}{7pt}{\AcronymText{3D}}}

\newcommand{\FAIR}{\resizebox{!}{7pt}{\AcronymText{FAIR}}}

\newcommand{\QNetworkManager}{\resizebox{!}{7pt}{\AcronymText{QNetworkManager}}}
\newcommand{\QTextDocument}{\resizebox{!}{7pt}{\AcronymText{QTextDocument}}}
\newcommand{\QWebEngineView}{\resizebox{!}{7pt}{\AcronymText{QWebEngineView}}}
\newcommand{\HTTP}{\resizebox{!}{7pt}{\AcronymText{HTTP}}}


\newcommand{\lAcronymTextNC}[2]{{\fontfamily{fvs}\selectfont {\Large{#1}}{\large{#2}}}}

\newcommand{\AcronymTextNC}[1]{{\fontfamily{fvs}\selectfont {\large #1}}}


\colorlet{orr}{orange!60!red}

\newcommand{\textscc}[1]{{\color{orr!35!black}{{%
						\fontfamily{Cabin-TLF}\fontseries{b}\selectfont{\textsc{\scriptsize{#1}}}}}}}


\newcommand{\textsccserif}[1]{{\color{orr!35!black}{{%
				\scriptsize{\textbf{#1}}}}}}


\newcommand{\iXPDF}{\resizebox{!}{7pt}{\textsccserif{%
\textit{XPDF}}}}

\newcommand{\iEPF}{\resizebox{!}{7pt}{\textsccserif{%
\textit{ETSpf}}}}

\newcommand{\iSDI}{\resizebox{!}{7pt}{\textsccserif{%
\textit{SDI}}}}

\newcommand{\iHTXN}{\resizebox{!}{7pt}{\textsccserif{%
\textit{HTXN}}}}


\newcommand{\AcronymText}[1]{{\textscc{#1}}}

\newcommand{\AcronymTextser}[1]{{\textsccserif{#1}}}


\newcommand{\mAcronymText}[1]{{\textscc{\normalsize{#1}}}}

\newcommand{\FASTA}{{\resizebox{!}{7pt}{\AcronymText{FASTA}}}}
\newcommand{\SRA}{{\resizebox{!}{7pt}{\AcronymText{SRA}}}}
\newcommand{\DNA}{{\resizebox{!}{7pt}{\AcronymText{DNA}}}}
\newcommand{\MAP}{{\resizebox{!}{7pt}{\AcronymText{MAP}}}}
\newcommand{\EPS}{{\resizebox{!}{7pt}{\AcronymText{EPS}}}}
\newcommand{\CSV}{{\resizebox{!}{7pt}{\AcronymText{CSV}}}}
\newcommand{\PDB}{{\resizebox{!}{7pt}{\AcronymText{PDB}}}}

\newcommand{\TeXMECS}{\resizebox{!}{7pt}{\AcronymText{TeXMECS}}}

\newcommand{\NGML}{\resizebox{!}{7pt}{\AcronymText{NGML}}}

\newcommand{\Cpp}{\resizebox{!}{8.5pt}{\AcronymText{C++}}}

\newcommand{\WhiteDB}{\resizebox{!}{7pt}{\AcronymText{WhiteDB}}}

\colorlet{drp}{darkRed!70!purple}

%\newcommand{\MOSAIC}{{\color{drp}{\AcronymTextNC{\scriptsize{MOSAIC}}}}}

\newcommand{\MOSAIC}{\resizebox{!}{7pt}{\AcronymText{MOSAIC}}}


\newcommand{\mMOSAIC}{{\color{drp}{\AcronymTextNC{\normalsize{MOSAIC}}}}}

\newcommand{\MOSAICVM}{\mMOSAIC-\mAcronymText{VM}}

\newcommand{\sMOSAICVM}{\resizebox{!}{7pt}{\MOSAICVM}}
\newcommand{\sMOSAIC}{\resizebox{!}{7pt}{\MOSAIC}}

\newcommand{\LDOM}{\resizebox{!}{7pt}{\AcronymText{LDOM}}}
\newcommand{\Cnineteen}{\resizebox{!}{7pt}{\AcronymText{CORD-19}}}


\newcommand{\LXCR}{\resizebox{!}{7pt}{\AcronymText{LXCR}}}
\newcommand{\lLXCR}{\resizebox{!}{8.5pt}{\AcronymText{LXCR}}}
\newcommand{\lsLXCR}{\resizebox{!}{9.5pt}{\AcronymText{LXCR}}}

%\newcommand{\lMOSAIC}{{\color{drp}{\lAcronymTextNC{M}{OSAIC}}}}
\newcommand{\lfMOSAIC}{\resizebox{!}{9pt}{{\color{drp}{\lAcronymTextNC{M}{OSAIC}}}}}

\newcommand{\Mosaic}{\resizebox{!}{7pt}{\MOSAIC}}
\newcommand{\MosaicPortal}{{\color{drp}{\AcronymTextNC{MOSAIC Portal}}}}

\newcommand{\RnD}{\resizebox{!}{7pt}{\AcronymText{R\&D}}}
\newcommand{\QtCpp}{\resizebox{!}{8.5pt}{\AcronymText{Qt/C++}}}
\newcommand{\Qt}{\resizebox{!}{7pt}{\AcronymText{Qt}}}
\newcommand{\QtSQL}{\resizebox{!}{7pt}{\AcronymText{QtSQL}}}

\newcommand{\HTML}{\resizebox{!}{7pt}{\AcronymText{HTML}}}
\newcommand{\PDF}{\resizebox{!}{7pt}{\AcronymText{PDF}}}

\newcommand{\lGRE}{\resizebox{!}{7pt}{\AcronymText{GRE}}}

\newcommand{\p}[1]{

\vspace{.85em}#1}

\newcommand{\q}[1]{{\fontfamily{qcr}\selectfont ``}#1{\fontfamily{qcr}\selectfont ''}} 

%\newcommand{\deconum}[1]{{\textcircled{#1}}}


\renewcommand{\thesection}{\protect\mbox{\deconum{\Roman{section}}}}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}

\newcommand{\llMOSAIC}{\mbox{{\LARGE MOSAIC}}}
%\newcommand{\lfMOSAIC}{\mbox{M\small{OSAIC}}}

\newcommand{\llMosaic}{\llMOSAIC}
\newcommand{\lMosaic}{\lMOSAIC}
\newcommand{\lfMosaic}{\lfMOSAIC}


\newcommand{\llWC}{\mbox{{\LARGE WhiteCharmDB}}}

\newcommand{\llwh}{\mbox{{\LARGE White}}}
\newcommand{\llch}{\mbox{{\LARGE CharmDB}}}

\usepackage{enumitem}

\colorlet{dsl}{purple!20!brown}
\colorlet{dslr}{dsl!50!blue}

\setlist[description]{%
  topsep=10pt,
  labelsep=12pt,
  itemsep=12pt,               % space between items
  %font={\bfseries\sffamily}, % set the label font
  font=\normalfont\bfseries\color{dslr!50!black}, % if colour is needed
}

\setlist[enumerate]{%
  topsep=3pt,               % space before start / after end of list
  itemsep=-2pt,               % space between items
  font={\bfseries\sffamily}, % set the label font
%  font={\bfseries\sffamily\color{red}}, % if colour is needed
}

%\usepackage{tcolorbox}

\newcommand{\slead}[1]{%
\noindent{\raisebox{2pt}{\relscale{1.15}{{{%
\fcolorbox{logoCyan!50!black}{logoGreen!5}{#1}
}}}}}\hspace{.5em}}


\let\OldLaTeX\LaTeX

\renewcommand{\LaTeX}{\resizebox{!}{7pt}{\color{orr!35!black}{\OldLaTeX}}}

\let\OldTeX\TeX

\renewcommand{\TeX}{\resizebox{!}{7pt}{\color{orr!35!black}{\OldTeX}}}


\newcommand{\LargeLaTeX}{\resizebox{!}{8.5pt}{\color{orr!35!black}{\OldLaTeX}}}

\setlength\parindent{0pt}
%\setlength\parindent{24pt}
%\input{commands}


\newcommand{\lun}[1]{\raisebox{-4pt}{\fontfamily{qcr}\selectfont{%
\LARGE{\textbf{\textcolor{tcolor}{#1}}}}}\vspace{-2pt}}

\newcommand{\inditem}{\itemindent10pt\item}

\usepackage{soul}

\definecolor{hlcolor}{RGB}{114, 54, 203}
\colorlet{hlcol}{hlcolor!35}
\sethlcolor{hlcol}

\makeatletter
\def\SOUL@hlpreamble{%
	\setul{}{3ex}%         !!!change this value!!! default is 2.5ex
	\let\SOUL@stcolor\SOUL@hlcolor
	\SOUL@stpreamble
}
\makeatother

\usepackage{scrextend}
%\vspace*{3em}
\newenvironment{mldescription}{\vspace{1em}%
  \begin{addmargin}[4pt]{1em}
    \setlength{\parindent}{-1em}%
    \newcommand*{\mlitem}[1][]{\vspace{5pt}\par\medskip%
%\colorbox{hlcolor}{\textbf{##1}}\quad}\indent
\hl{ \textbf{##1} }\quad}\indent
}{%
  \end{addmargin}
  \medskip
}

\usepackage{marginnote}

\newcommand{\mnote}[1]{%
\vspace*{-2em}
\reversemarginpar
\raisebox{1em}{\marginnote{\parbox{4em}{%
\begin{mdframed}[innerleftmargin=4pt,
	innerrightmargin=1pt,innertopmargin=1pt,
	linecolor=red!20!cyan,userdefinedwidth=4em,
	topline=false,
	rightline=false]
{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
		\textit{#1}}}
\end{mdframed}}
	}[3em]}}

\newcommand{\mnotel}[1]{%
\vspace*{-2em}
\reversemarginpar
\raisebox{-4em}{\marginnote{\parbox{4em}{%
\begin{mdframed}[innerleftmargin=4pt,
	innerrightmargin=1pt,innertopmargin=1pt,
	linecolor=red!20!cyan,userdefinedwidth=4em,
	topline=false,
	rightline=false]
{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
		\textit{#1}}}
\end{mdframed}}
	}[3em]}}

\newcommand{\mnoteh}[3]{%
	\vspace*{#1}
	\reversemarginpar
	\raisebox{#2}{\marginnote{\parbox{4em}{%
				\begin{mdframed}[innerleftmargin=4pt,
					innerrightmargin=1pt,innertopmargin=1pt,
					linecolor=red!20!cyan,userdefinedwidth=4em,
					topline=false,
					rightline=false]
					{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
							\textit{#3}}}
				\end{mdframed}}
			}[3em]}}


\newcommand{\mnoteb}[1]{%
	\vspace*{1em}
	\reversemarginpar
	\raisebox{1em}{\marginnote{\parbox{4em}{%
				\begin{mdframed}[innerleftmargin=4pt,
					innerrightmargin=1pt,innertopmargin=1pt,
					linecolor=red!20!cyan,userdefinedwidth=4em,
					topline=false,
					rightline=false]
					{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
							\textit{#1}}}
				\end{mdframed}}
			}[3em]}}
	
\usepackage{wrapfig}

\usetikzlibrary{arrows, decorations.markings}
\usetikzlibrary{shapes.arrows}

\newcommand{\curicon}[2]{%
	\node at (#1,#2) [
	draw=black,
	%minimum width=2ex,
	inner sep=.7pt,
	fill=white,
	single arrow,
	single arrow head extend=3pt,
	single arrow head indent=1.5pt,
	single arrow tip angle=45,
	line join=bevel,
	minimum height=4.6mm,
	rotate=115
	] {};
}

\begin{document}
	
{\linespread{1.2}\selectfont

\vspace*{-7em}

\begin{center}
%{\relscale{1.2}{\fontfamily{qcr}\fontseries{b}\selectfont 
%{\colorbox{black}{\color{blue}{\llWC{} Database Engine \\and 
%\llMOSAIC{} Native Application Toolkit}}}}}

\colorlet{ctmp}{logoPeach!20!gray}
\colorlet{ctmpp}{ctmp!90!yellow}
\colorlet{ctmppp}{ctmpp!50!black}
\colorlet{ctmpppp}{ctmppp!90!logoRed}

\vspace{2em}


%{\colorbox{darkBlGreen!30!darkRed}{%
\begin{tcolorbox}
[
%%enhanced,
%%frame hidden,
%interior hidden
arc=2pt,outer arc=0pt,
enhanced jigsaw,
width=.92\textwidth,
colback=ctmpppp!30,
colframe=logoRed!30!darkRed,
drop shadow=logoPurple!50!darkRed,
%boxsep=0pt,
%left=0pt,
%right=0pt,
%top=2pt,
]
\hspace{22pt}
\begin{minipage}{.85\textwidth}	
\begin{center}	
{\setlength{\fboxsep}{12pt}
	\relscale{1.3}{{\fontfamily{qcr}\fontseries{b}\selectfont%
{Proposing a CORD-19 SDK Solution to Improve Machine Readability}
}}}
\end{center}
\end{minipage}
\end{tcolorbox}
\end{center}

\vspace{2.5em}
%\noindent\lun{Overview}
%\fontfamily{ptm}\fontsize{13pt}{18pt}\selectfont
%{\sectsp}

\begin{center}\parbox{0.9\textwidth}{\color{red!60!black}
\fontfamily{ptm}\fontsize{13pt}{15pt}\selectfont Supplemental to \textit{Advances in Ubiquitous Computing: 
Cyber-Physical Systems, Smart Cities and Ecological Monitoring}, 
edited by Amy Neustein, part of the Series 
\q{Advances in Ubiquitous Sensing Applications for Healthcare}, 
Elsevier, series editors Nilanjen Dey, Amira Ashour, 
and Simon James Fong.}
\end{center}

\vspace{2.5em}

\p{The forthcoming volume \textit{Advances in Ubiquitous Computing: 
Cyber-Physical Systems, Smart Cities and Ecological Monitoring} will 
prototype a data-publishing protocol that extends 
existing initiatives such as \q{Research Objects} 
and \FAIR{} (\q{Findable, Accessible, 
Interoperable, Reusable}), both of which 
reflect publishers' current emphasis on 
publicly sharing research data.  In the last 
few years, academic, governmental, and 
industry stakeholders have increasingly 
prioritized open-access data publication as a 
complement to the traditional ecosystem of 
peer-reviewed scientific literature.  
This new \q{data-centric} paradigm seeks to 
augment the value and accuracy research 
data by provisioning scientists with digital 
tools to find, evaluate, and reuse data 
which is relevant to their research.  
At the present moment, the urgency of sharing Covid-19 
research is further boosting this data-centric and open-access publishing model.  Indeed, Covid-19 
presents a unique opportunity for researchers 
and publishers to assess the strengths and 
weaknesses of current publishing technology, 
because we are now in a global environment where there 
is an unprecedented societal demand 
for pooling and accelerating research in 
one specific, easily demarcated domain 
(that of Covid-19 in particular and 
coronaviruses in general).}

\p{Realizing a new data-centric publishing paradigm 
requires more than just enouraging authors to 
prepare downloadable data sets to accompanying their research 
papers.  It also requires a technological 
infrastructure to support the curation of 
data sets and data set collections.  New digital solutions 
are needed to accommodate the sheer volume and diversity of 
peer-reviewed research work, and the implementation of 
these solutions will have to be guided by interdisciplinary 
theoretical models integrating computer science, knowledge 
engineering, biblioinformatics, as well as individual 
natural and social sciences.  In this vein, 
one goal of \textit{Advances in Ubiquitous Computing} 
is to advocate for an even more rigorous data publishing 
protocol, taking a multi-disciplinary and multi-paradigm 
perspective on the curation and structuring of 
published research data.  In particular, as a supplement to 
\textit{Advances}, several data sets will be published (or 
republished) relevant to subjects covered by 
chapters in the text, such as signal-processing, 
bioacoustics, and Natural Language Processing.  
The primary purpose of these datasets is to demonstrate 
data-management and software-development techniques 
discussed in the volume, particularly in the 
third chapter (the data sets are being curated by that 
chapter's author).  These data sets will introduce a 
so-called \q{\MOSAIC{}} protocol which, 
compared to ordinary Research Objects, 
places greater emphasis on (1) self-contained 
code libraries bundled with data sets to provide 
sophisticated computational features; (2) 
cross-referencing between data sets and published 
articles; and (3) unifying collections of data sets, 
spanning multiple publications, into an integral whole.  
The central elements of \MOSAIC{} (which stands for 
\q{Multiparadigm Ontologies for Scientific, Academic, 
and Technical Publishing}) will be outlined below.}

\p{In response to the contemporary Covid-19 crisis, 
my company, Linguistic Technology Systems, proposes 
to develop a concrete implementation of the 
\MOSAIC{} protocol targeting Covid-19 data sets, 
and particularly the \q{Covid-19 Open Research Dataset} (\Cnineteen{}) 
research collection recently published at the launch of a new 
White House initiative devoted to Covid-19.  In particular, 
we propose to create a Standard Development Kit 
(\SDK{}) that would facilitate the deployment of 
software targeting \Cnineteen{}.  In order to explain 
the role of this proposed \SDK{} in relation to 
the currently existing \Cnineteen{} resourses, 
this paper will examine the present state of 
\Cnineteen{}, and will point out certain limitations of 
existing \Cnineteen{} data that can be ameliorated with code 
libraries that would be bundled with our proposed 
\Cnineteen{} \SDK{}.}

\section{Assessing the CORD-19 Collection}
\p{The sudden emergence of Covid-19 as a 
medical and governmental priority presents a 
unique case-study of the limitations of our 
existing research-data platforms.  Publishers 
have, to some degree, recognized the extra-ordinary 
nature of the new coronavirus crisis and 
taken some commensurate measures; for example, 
several publishing houses (including Springer Nature, 
Wiley, Elsevier, the American Society for Microbiology, and the New England Journal 
of Medicine) have committed to releasing as 
open-access documents a collection of papers 
potentially helpful for doctors and 
policymakers responding to the pandemic --- 
some newly published and some dating back several 
years (the older research concerns viruses 
biologically similar to Covid-19).  On 
March 16, the White House announced a 
\q{call to action ... to develop new text and data mining techniques that can help the science community answer high-priority scientific questions related to COVID-19} and simultaneously released the \Cnineteen{} resources, 
which had been 
curated by a consortium of industry and academic 
institutions}.  The \Cnineteen{} collection, 
described as a \q{dataset} by the 
White House, is actually a 
\q{machine-readable Coronavirus literature collection} 
which includes article metadata and (in most cases) 
publication text  
for over 13,000 coronavirus research papers.  
This resource is accompanied by links to open-access 
document collections hosted on portals such as 
Springer Nature and Wiley Online.}

\p{Over all, then, \Cnineteen{} has two 
distinct parts: there are, first, links to 
web portals offering full access to a select 
group of publications (in human-readable formats 
like \PDF{} and \HTML{}) and, second, 
a much larger machine-readable corpus of article 
texts relevant to Covid-19.  The \Cnineteen{} 
collection was formulated with the explicit goal 
of promoting both text mining and data mining solutions 
to advance coronavirus research.  This means that 
\Cnineteen{} is intended to be used both as a document 
archive for text mining and as a repository for 
finding and obtaining coronavirus data for subsequent 
research.  Unfortunately, \Cnineteen{} is not particularly 
well structured for either text or data mining, for 
reasons that will be outlined shortly.  To some extent, 
this reflects \Cnineteen{} being conceived as 
just a starting point; a spur to further implementation.  
The White House announcement directly requests 
institutions to develop \textit{further} technologies 
which would help scientists and jurisdictions to 
take advantage of \Cnineteen{} as it was initially 
published.  In short, \Cnineteen{} was released with the 
explicit anticipation that industry or academia would 
augment the underlying data by layering on additional 
software.  Our proposed \Cnineteen{} \SDK{} would 
do just that, as a component providing analytic 
capabilities which make the raw \Cnineteen{} data 
more valuable, and also a toolkit through which 
other developers could create 
new solutions targeting \Cnineteen{}.}

\p{Lacunae in the raw \Cnineteen{} data 
--- the problems which require customized \Cnineteen{} software --- 
can be identified both in the text encoding through which 
\Cnineteen{} functions as a corpus for text mining, and 
in the research data potentially made 
available via \Cnineteen{} and its archived publications.  
These problems fall into three broad categories, 
which will be discussed on more detail over the 
next several paragraphs: 

%\vspace{-2em}
\begin{description}
	
\item[Transription Errors]  This problem 
arises from incorrect or simplified translation 
of publication texts into a machine-readable 
format, which can cause the machine-readable 
text archive to misrepresent the structure 
and content of documents, hindering text-mining 
technology that targets the archive.

\item[Poorly Indexed Research Data]  Although 
\Cnineteen{} provides a structured representation 
of a large collection of research 
\textit{papers}, there is no easy-to-use tool 
for finding research \textit{data} 
through \Cnineteen{}.

\item[Poorly Integrated Research Data]  The 
research data which \textit{can} be 
accessed through \Cnineteen{} evinces 
a wide variety of technical fields 
and formats, with distinct software 
requirements; as a result, it is a 
difficult task to merge and integrate 
different data sets related to 
Covid-19.  At present, \Cnineteen{} 
does not include any software tools 
or computer code that would facilitate 
data integration.   
\end{description}}

\p{With respect to text mining, an immediate problem lies 
in \Cnineteen{}'s archive-construction methodology: 
especially, how the text was parsed from \PDF{} files.  
This is a process 
which almost inevitably causes imprecise or inaccurate text representation, which can degrade the quality of 
the archive unless manual or automated corrections are made.  In particular, the \Cnineteen{} library evinces  
transcription errors, particularly in relation to 
technical or scientific phrases and terminology,  
and scientific notation may be improperly 
transcribed.  A formal designation such as \q{2{\textquotesingle}-C-ethynyl}
(to take one specific case) is encoded in \Cnineteen{} 
as \q{2 0 -C-ethynyl}, which could stymie text searches 
against the \Cnineteen{} corpus.  Moreover, there is no 
semantic marking identifying that the \q{2 0 -C-ethynyl} 
text segment has a specific technical meaning.  These 
errors or limitations arise in part from unavoidable 
anomalies which occur when reading texts from \PDF{} files 
rather than from machine-readable, structured 
formats such as \XML{}.}

\p{To address these problems, our proposed 
\SDK{} would try to cross-reference the 
\Cnineteen{} \JSON{} data against \XML{} or \LaTeX{} manuscripts 
which may have been saved by authors or publishers as part 
of the publication process.  At its most complete, this 
would entail requesting authors whose papers are indexed 
on \Cnineteen{} to submit source materials if they are 
available in a machine-readable format, such as \LaTeX{}, 
as well as requesting composit-related representations from 
affiliated publishers, with the goal of providing machine-readable 
access to publications in multiple formats.  
The \SDK{} would then include built-in client libraries 
with procedures to 
merge different representations of each document.}
 
\p{It is also worth observing that the \JSON{} format used 
for encoding \Cnineteen{} manuscripts presents some 
logistical challenges for any operations related to 
text-mining or to cross-referencing publications and 
data sets.  In particular, \Cnineteen{} makes 
partial use of \q{standoff annotation}; specifically, 
document features such as citations and references are 
notated through character offsets into the paragraph where 
they appear.  As a result, accurately reading these document 
elements requires synthesizing data points parsed from several 
distinct objects in the \JSON{} code, which is only feasible given 
a client library built to interface with the \Cnineteen{} files 
in accord with their specific schema.  Such a client 
library, again proposed as part of a 
\Cnineteen{} \SDK{}, would implement convenience procedures to handle 
recurring tasks, such as obtaining the full bibliographic 
reference affixed to a given location in a manuscript.}

\p{With respect to \textit{data} mining in the \Cnineteen{} 
context, the limitations in the currently available raw 
\Cnineteen{} data are even more pronounced than in the 
context of text mining.  In particular, 
neither the article 
metadata nor the full open-access document collections have 
any mechanism for actually obtaining data published 
alongside research papers, or even identifying which papers 
have accompanying data in the first place.  The Springer 
Nature collection illustrates the limitations of this relatively unstructured 
data-publishing approach (this following analysis will 
focus on Springer Nature, but the problems identified 
are no less pronounced on the other \Cnineteen{} portals 
--- 
if anything, because Springer Nature 
allows readers to browse articles in \HTML{} within the 
web portal directly, one can ascertain whether research data 
exists for an article without downloading and reading 
it; with other \Cnineteen{} resources it is actually 
harder to locate supplemental data when available).  As of mid-March, the Springer Nature portal 
encompassed 43 articles, 
of which 15 were accompanied by research data that 
could be separately downloaded (this number does 
not include papers that document research 
findings only indirectly, via tables or graphics 
printed inline with the text).  Collectively 
these articles referenced over 30 distinct data 
sets (some papers were linked to multiple 
data sets), forming a data collection which could 
be a valuable resource for Covid-19 research --- 
not only through the raw data made available 
but as a kernel around which new coronavirus data 
could accumulate.  However, there is currently no 
mechanism to make this overall collection available 
as a single resource.}

\p{This problem demonstrates, among other things, 
how the Research Object protocol is limited in applying 
only to \textit{single} articles.  As a result, there is no 
commensurate protocol for publishing \textit{groups} 
of articles which are tied to groups of data sets unified 
into an integral whole.  Open-access Covid-19 papers also 
reveal limitations of exiting online document portals, 
particularly with respect to how publications are 
linked to data sets.  In particular, there is no clear indication that a 
given paper is associated with downloaded data; usually 
readers ascertain this information only by reading or 
scrolling down to a \q{supplemental materials} or 
\q{data availability} addendum near the end of the article.  
Moreover, because the Springer Nature portal aggregates 
papers from multiple sources, there is no consistent 
pattern for locating data sets; each journal or 
publisher has their own mechanism for alerting readers 
to the existence of open-access data and allowing them 
to download the relevant data sets.}

\p{To address the \Cnineteen{} text and data mining 
limitations, our \SDK{} would provide capabilities 
including (1) a client library for manipulating and 
examining the \Cnineteen{} 
\JSON{} files; (2) supplementing the \Cnineteen{} files 
with alternative document representations which would 
offer a more detailed infoset for document content, 
wherever such files are available; (3) a machine-readable 
index of open-access data sets linked to \Cnineteen{} 
articles; and (4) software tools for downloading 
and manipulating these coronavirus data sets.  
In addition, our \SDK{} would provide tools for 
merging and integrating Covid-19 data from 
disparate sources.  The problems of data integration, 
and solutions offered by the proposed \SDK{}, are 
discussed in the following paragraphs.}

\section{Data Integration within CORD-19}
\p{Aside from the issues, described over the last several 
paragraphs, which are likely to hinder text and data mining 
across \Cnineteen{}, the 
collective group of Covid-19 data sets also illustrate 
the limitations of information spaces pieced 
together from disconnected raw data files with little 
additional curation.  The files included in this 
group of data sets encompass an array of file types 
and formats, including \FASTA{} (which stands for Fast-All, 
a genomics format), \SRA{} (Sequence Read Archive, for 
\DNA{} sequencing), \PDB{} (Protein Data Bank,  
representing the \ThreeD{} geometry of protein 
molecules), \MAP{} (Electron Microscopy Map), \EPS{} 
(Embedded Postscript), and \CSV{} (comma-separated values).  
There are also tables represented in Microsoft Word 
or Excel formats.  Although these various formats are 
reasonable for storing raw data, not all of them 
are actually machine-readable; in particular, 
the \EPS{}, Word, and Excel files need manual processing 
in order to use the information they provide in a 
computational manner.  A properly curated data collection 
would need to unify disparate sources 
into a common machine-readable representation (such as \XML{}).}

\p{Going further, productive data curation should also 
aspire to \textit{semantic} integration, unifying disparate 
sources into a common data model.  For example, multiple 
spreadsheets among the Springer Nature Covid-19 data sets 
hold sociodemographic and epidemiological information relevant 
to modeling the spread of the disease.  These different 
sources could certainly be integrated into a canonical 
social-epidemiology-based representational paradigm which 
recognizes the disparate data points which might be 
relevant for tracking the spread of Covid-19 (with the 
potential to unify data from many countries and 
jurisdictions).  This is not only a matter of data
\textit{representation} (viz., how data is physically 
laid out), but also of data types and computer code.  
According to the Research Object protocol, 
data sets should include a code base 
which provides convenient computational access to the 
published data.  In the case of Covid-19, this entails 
creating a sociodemographic and epidemiological code 
library optimized for Covid-19 information, which would 
be the primary access point for researchers seeking to 
use the data which has been published in conjunction with 
the 43 manuscripts examined here that were aggregated 
on Springer Nature, along with any other coronavirus 
research which comes online.  Similar comments 
apply not only to tabular data represented in spreadsheet 
or \CSV{} form, but to more complex molecular or 
microscopy data that needs specialized scientific software 
to be properly visualized.}

\p{Considering the overall space of Covid-19 data, it is 
unavoidable that some files require special applications 
and cannot be directly merged with the overall collection.  
For instance, there is no coherent semantics for 
unifying Protein Data Bank files with social-epidemiology; 
files of the former type have specific scientific uses and 
can only be understood by special-purpose software.  
Nevertheless, a well-curated data-set collection can 
make using such special-purpose data as convenient as 
possible.  In the case of Protein Data Bank, a downloadable 
Covid-19 archive can include source code for \IQmol{}, a 
molecular-visualization application that supports 
\PDB{} (among other file formats) and has few 
external dependencies (so it is relatively easy to 
build from source).}

\p{Indeed, a curated Covid-19 archive 
might include an enhanced version of \IQmol{} prioritizing 
Covid-19 research, with the goal of integrating biomolecular 
and social-epidemiological data as much as possible.  
For example, as Covid-19 potentially mutates in different 
ways in different geographic areas, it will be important 
to model the connections between \q{hard} scientific 
Covid-19 information and sociodemographics.  
As the pandemic evolves, genomic and biochemical information 
may be linked to particular strains of virus, which 
in turn are linked to sociodemographic profiles: certain 
strains will be more prevalent in certain populations.  
Consequently, models of Covid-19 variation will need to be 
formulated and then integrated with both chemical/molecular 
data and sociodemographic/epidemiological data.  Different 
Covid-19 strains then form a bridge linking these different 
sorts of information; researchers should be able to pass 
back and forth from molecular or genomic visualizations of 
Covid-19 to social-epidemiological charts and tables based 
on viral strains.  Ideally, capabilities for this 
sort of interdisciplinary data integration would be 
provide by a Covid-19 archive as enhancements to applications, 
such as \IQmol{}, that scientists would use to study the 
published data.}

\p{Logistically speaking, not all Covid-19 data is practical 
to reuse as a downloadable package.  This is especially 
true for genomics; several of the aforementioned 
43 coronavirus papers included data published via 
online data banks capable of hosting data sets that 
are too large for an ordinary computer.  In these 
situation scientists formulate queries or 
analytic scripts that are sent remotely to the online 
repositories, so that researchers access the actual 
published data only indirectly.  Nevertheless, access to 
these data sets can still be curated as part of 
a Covid-19 package; in particular, computer code 
can be provided which automates the process of 
networking with remote genomics archives through the 
accession numbers and file-formats which those archives 
recognize.  More generally, our proposed 
\SDK{} would provide a toolkit for composing client 
libraries targeting remote data services --- 
formulating queries and then parsing the results delivered 
in formats such as \XML{} or \JSON{} --- as well 
as concrete libraries for remote data sources repeatedly 
used by \Cnineteen{} data sets.}

\p{As a final point on the topic of integrating 
disparate \Cnineteen{} research data, note that 
an overarching framework for indexing Covid-19 data 
sets would also facilitate the process of cross-referencing 
article text and research data.  In particular, 
our proposed \SDK{} would introduce a 
system of \textit{microcitations} that apply 
to portions of manuscripts \textit{as well as} data sets.  
In the publishing context, a microcitation is defined as a 
reference to a partially isolated fragment of a larger 
document, such as a table or figure illustration, or a 
sentence or paragraph defining a technical term, 
or (in mathematics) the statement/proof of a definition, axiom, 
or theorem.  In data publishing, \q{data citations} are 
unique references to data sets in their entirety or to 
smaller parts of data sets.  A data microcitation is then a 
fine-grained reference into a data set: for example, 
\q{the precise data records actually used in a study} (as 
defined by the Federation of Earth Science Information Partners), 
one column in a spreadsheet, or one statistical parameter in a 
quantitative analysis.}

\p{The unique feature we propose 
for our \SDK{} would be to combine the text-mining and 
data-mining notions of microcitation into a unified 
framework.  In particular, text-based searches against 
the \Cnineteen{} corpus would try to find matches in the 
data sets indexed by our \SDK{}.  As a concrete example, 
a concept such as \q{expiratory flow} appears in \Cnineteen{} 
both as a table column in research data and as a medical concept 
discussed in research papers; a unified microcitation framework 
should there map \textit{expiratory flow} as a keyphrase 
to both textual locations and data set parameters.  
Similarly, a concept such as 
\textit{2{\textquotesingle}-C-ethynyl} (mentioned earlier 
in the context of transcription errors) 
should be identified both as a phrase in 
article text and as a molecular component 
present within compounds whose scientific 
properties are investigated through \Cnineteen{} 
research data, so that a search for this 
concept can trigger both publication and 
data-set matches. 
Implementing this kind of unified search mechanism requires that data 
sets be \textit{annotated} with techniques similar to 
those used for marking up Natural Language techniques; 
consequently, the proposed \SDK{} would include a custom 
data-annotation library, to be employed in conjunction with 
operations to aggregate \Cnineteen{} data sets into a 
common representational format.}

\section{The MOSAIC Protocol}
\p{As mentioned at the top, the \MOSAIC{} data-publishing 
model prioritizes the implementation of 
\textit{self-contained} data sets.  \q{Self-Contained} 
means that researchers who download a data set 
will not need to download \textit{additional} 
software in order to examine and reuse the data set.  
One important goal of the Research Object protocol 
is to make research data available with a minimum of 
additional effort.  Accordingly, Research Object should 
try to minimize external dependencies --- in particular, 
should minimize extent to which users need to install 
software beyond what is provided by the Research Object Bundle 
itself.  This \q{stand-alone} status can be achieved in 
various ways.  One option is to organize Research Objects 
around widely used scientific computing platforms, 
such as R, Jupyter, or Matlab; another is to bundle 
dependencies into a virtual platform, using tools such 
as ReproZip.  An alternative solution,
more in keeping with the 
vision of truly \textit{self-contained} 
Research Objects --- the one which is embraced by \MOSAIC{} and 
would be adopted for our proposed \Cnineteen{} \SDK{} --- 
is to provide a kind of out-of-the-box 
computing platform contained entirely within the 
downloadable code and data.  In this context, code bases 
which can be distributed in pure source fashion --- such 
as WhiteDB for a database engine or AngelScript 
(an embedded \Cpp{}-based scripting language) for a 
scripting engine --- are especially valuable.}

\p{Considering the inter-disciplinary nature of Covid-19 research, 
it is unavoidable that different scientists will need 
different sorts of specialized software to analyze the 
kinds of information relevant to their research.  The 
computational techniques applicable to diagnosing 
coronavirus infection are scientifically very different 
from those used for genomic or epidemiological studies 
of the disease; it is impractical to expect 
pathologists to use the same software as bioinformaticians 
studying the pathogen, or for either to use the same 
software as virologists modeling the potential or observed 
spread of the disease.  In short, even if 
scientists from disparate disciplines start with a 
common pool of raw data, they will need to 
analyze this data through a diverse set of 
supplemental computational tools, which will vary 
not only across disciplines but also in terms of 
the software and laboratory facilities available 
to different researchers through their institutions.  
In this sense it is impossible to unify all 
\Cnineteen{} data into a \textit{fully} self-contained 
information space.}

\p{Nevertheless, committing to \q{standalone} data publishing 
remains a valuable goal even in a context where published 
data sets will inevitably migrate to different 
digital ecosystems.  Although scientists may use 
external digital tools \textit{when necessary} to 
perform certain calculations, or to interface with 
laboratory equipment, we can hope to minimize the 
\textit{degree of variation} across different 
domain-specific extensions to \Cnineteen{}.  
Ideally, that is, the version of \Cnineteen{} 
(along with its supporting technology) 
found in a biomedical setting will be as 
similar as possible to that found in a 
biochemical context, or a health-policy 
contest.  By providing a self-contained computing 
framework is delivered as an integral package accompanying 
\Cnineteen{}, a multi-disciplinary \Cnineteen{} 
software package (which could be organized through our 
proposed \SDK{}) would limit the degree of 
technological divergence that would naturally 
tend to arise as \Cnineteen{} is leveraged by 
different scientific fields.  In the absence of 
any initiative to limit this drift, \Cnineteen{} 
could easily devolve into a federation of 
separate resources which have no interconnection 
apart from their nominal focus on Covid-19.}

\p{The vision of \textit{standalone} and \textit{self-contained} 
Research Object Bundles reveals new criteria that can 
be introduced as goals for data publication, analogous 
to \FAIR{}.  In particular, Research Objects should 
be (1) \textit{self-contained} (with few or no external 
dependencies), (2) \textit{transparent} (meaning that 
all computing operations should be implemented by 
source code within the bundle that can be examined 
as code files and within a debugging session), 
and (3) interactive (meaning that the bundle does not 
only include raw data but also software to interactively 
view and manipulate this data).  Research Objects which 
embrace these priorities will try to provide data visualization, 
persistence, and analysis through \GUI{}, database, and 
scripting engines that can be embedded as source 
code in the Research Object itself.  In particular, 
self-contained bundles can be organized around what 
in \MOSAIC{} is called a \q{triple-kernel} (or 
\MThreeK{}, for \MOSAIC{} Triple Kernel) architecture, 
referring to database, data visualization, and 
scripting engines.  A good prototype for 
triple-kernel implementation would gravitate 
toward components that can be unified into a 
common code base: for example, 
WhiteDB for the database kernel, AngelScript 
for the scripting kernel, and \Qt{} for 
the data visualization kernel (and also for networking).  
With the (freely available) \Qt{} libraries, 
each of these components can be built via 
the \Qt{} build system and distributed in source 
code fashion; there is no need to system-level 
installs or any other external build or install operations.} 

\p{A fully self-contained \MThreeK{} 
bundle as just described will support several 
different forms of analytic operations and 
queries: assuming the WhiteDB/AngelScript/\Qt{} 
architecture, this would include queries against 
the WhiteDB database engine, queries executed 
by running Angel scripts, and queries against 
remote services via \Qt{} Networking (to support 
non-downloadable data).  Hypergraph code and 
data representation can then support a unified 
\q{Transparent Hypergraph Query Language} 
(\THQL{}) which provides a common mechanism for 
accessing these different varieties of queries.  
\lTHQL{} is implemented via the notion of 
\q{hypergraph construction as intermediate 
representation}: specifically, the logic to construct 
queries is understood as a manifestation of the logic 
to populate hypergraphs (meeting certain formal criteria).  
Consequently, a hypergraph-construction language can 
serve as a kind of bytecode for compiling database, 
scripting, and networking queries in a common framework; 
\THQL{} provides an Intermediate Representation and 
Virtual Machine which dispatches queries to the 
respective \MThreeK{} kernels (in the prototype case, 
to the WhiteDB, AngelScript, and \Qt{} engines respectively).  
A canonical WhiteDB/AngelScript/\Qt{} \MThreeK{} framework 
can be facilitated by a Standard Development Kit, 
extending the AngelScript \SDK{} with WhiteDB and 
\Qt{} integration.}

\p{Having described a \MThreeK{} model for \textit{individual} 
Research Objects, one can then consider the development 
of data set \textit{collections} encompassing 
multiple research papers.  A protocol for 
Research Object aggregation can be developed by 
extending a \MThreeK{} \SDK{} with domain-specific 
code appropriate for the shared topics and 
subject-matter which unify the presented research 
work.  In \textit{Advances in Ubiquitous Computing}, 
for example, most of the chapter discussed either 
signal processing (particularly in the context 
of bioacoustics and 5G wireless networks) or 
Natural Language Processing, so a combined code base 
would include a mixture of signal-processing and 
\NLP{} libraries.  Analogously, a \Cnineteen{} \SDK{}, 
as proposed here, 
could bundle code related to different facets 
of Covid-19 research, from demographic and 
quantitative epidemiological modeling 
to networking with remote genomic data.}

\section{Conclusion}
\p{This paper has highlighted limitations of data 
sets published in conjunction with coronavirus articles made 
available as open-access resources on Springer Nature 
(and, by extension, \Cnineteen{}).  
The central point here is to argue for a distinct data-curation 
stage in the publication process, with data curators 
playing a role distinct from that of both authors and editors.  
Moreover, the discussion has hopefully highlighted problems 
with current data-sharing paradigms, even those such 
as the Research Object and \FAIR{} initiatives which are 
explicitly devoted to improving how open-access data sets are 
published.  \Cnineteen{} exposes several 
lacunae in the Research Object protocol, for example, 
which point to the need for a more detailed extension 
of this protocol.  In particular, an enhanced protocol 
should encompass: 

\begin{enumerate}[leftmargin=3pt]

\item{} A canonical framework for archiving collections 
of data sets, not only single data sets (and not only 
groups of data sets published with a single research 
paper).  For example, all data sets published alongside 
the 43 Springer Nature articles could be unified into a 
single collection.

\item{} A code base accompanying data-set collections 
designed to help research unify the information provided.  
Curating the overall collection would involve pooling 
disparate data into common representation, and 
implementing computer code which deserializes and processes 
the unified data accordingly.  For instance, \CSV{}, 
\EPS{}, and Microsoft Word/Excel tables could be migrated 
to \XML{}, \JSON{}, or 
a more complex common format (Chapter 3 of 
\textit{Advances in Ubiquitous Computing} presents the theoretical 
case for a \q{software-centric} representational format based 
on hypergraphs).  Customized computer code could then 
be implemented specifically to parse and merge the 
information present in single data sets within the 
overall collection.  This implementation would 
reciprocate the Research Object goal of unifying 
code and data, but again would operate at the level 
of an aggregate of research projects rather than a 
single Research Object.

\item{}  A unified data-set collection should 
be self-contained as much as possible, and should be 
built around a foundation where advanced computing 
capabilities are available in a transparent, 
standalone fashion, without requiring tools 
outside the collection itself.  One way to 
achieve this is via a \MThreeK{} architecture 
as outlined earlier; it is straightforward 
to publish the WhiteDB and AngelScpipt code 
bases within a Research Object collection, and 
to employ the \Qt{} ecosystem (e.g., the \Qt{} 
Creator Integrated Development Environment) as 
the underlying operational milieu for 
using and obtaining the data sets.  For example, 
it would be possible to implement \Qt{}-specific 
libraries for interfacing with the \Cnineteen{} library 
and with other Covid-19 dashboards and data sets.   

\item{}  A unified data-set collection should also provide 
prototyping and remote-access tools to interface with 
web-based information spaces that host data sets 
too large to be individually downloaded.  Ideally, 
these would include simulations of remote services 
analogous to PurpleData \visavis{} BigData, which 
would help scientists understand the design of 
the remote archives and how to interface with them.

\item{}  Finally, a unified research portal should  
influence the design of the web portals where associated 
texts are published.  It should be easy for readers to 
identify which articles have supplemental data files and 
to download those files if desired.  Moreover, 
textual links should be established between publication 
content and data sets --- for instance, a plot or 
diagram illustrating statistical or equational distributions 
should link to the portion of the data set from which that 
quantitative data is derived.
\end{enumerate}}

\p{The above discussion has considered the Springer Nature 
articles as a case-study, but analogous comments would 
apply to other Covid-19 related resources.  For 
example, John Hopkins University has created and deployed 
a Covid-19 \q{dashboard} tracking the spread of the virus 
(this is one of several dashboards that have come online 
for visualizing the evolution of the pandemic, with varying 
degrees of complexity); 
new data from which the web dashboard is generated is published 
via a \GIT{} archive roughly once daily.    If and 
when the reported Verily portal comes online, hopefully 
machine-readable access to that public data will be 
provided either via an analogous updated archive or 
via an \API{}.  Ideally, these disparate Covid-19 
projects will be interoperable: any code published 
in relation to the Springer Nature coronavirus 
collection, for example, could include components 
implemented to access the John Hopkins and 
(anticipated) Verily data sets as well as all 
the data brought in via the \Cnineteen{} articles.  
In particular, a useful starting-point for Covid-19 
data integration would be a single data type 
(e.g., a \Cpp{} class) whose specific purpose is 
to obtain the most recent available coronavirus 
data: one procedure to download the latest 
files from the John Hopkins dashboard, one procedure 
to search Springer Nature for new relevant content, 
etc.  Such a data type would then serve as a 
guide for obtaining Covid-19 information.    
Insofar as conscious effort is made to integrate 
all publicly accessible Covid-19 data via 
an overarching toolkit, it will be easier to continually 
accumulate new data sources as these come online.}

\p{This discussion has also used the Covid-19 crisis as a 
lens through which to examine data-publishing limitations 
in general.  These problems are not specific to 
coronavirus, but the almost unprecedented 
urgency of this epidemic exposes how science and the 
publishing industry are still struggling to 
develop technologies and practices which keep pace 
with the intersecting needs of systematic research 
and public policy.  An optimistic projection is 
that the crisis will spur momentum toward a more 
sophisticated data-sharing paradigm --- perhaps a 
generalization of the Research Object protocol 
toward data-set collections, with features as outlined 
above.  We hope to contribute to the emergence of 
such a protocol, so as to operationalize some 
of the ideas laid out in \textit{Advances in Ubiquitous 
Computing}.  It would be especially rewarding if 
an integrated data-set collection devoted to 
Covid-19 would serve as a first example and a test-bed 
for this new paradigm, given the potential public 
benefit of unifying disparate Covid-19 data 
as effectively as possible, where this technology 
can then be generalized to other medical 
priorities and other academic disciplines overall.}

\p{Supplemental to this overview, we can provide information 
about a proposed portal for publishing research papers 
and data sets conformant to the above enhancements to 
the Research Object protocol.  In particular, we can 
share the following: (1) description and demonstrations 
of a \Qt{} and hypergraph-based extension to 
WhiteDB, which we term WhiteCharmDB, that can be used 
to merge individual data sets and prototype remote-analytic 
interfacing logic; (2) documentation of a plugin mechanism, 
using \IQmol{} as a case-study, which integrates document 
viewers with scientific applications and which would be 
especially useful for interconnecting publications and 
data sets in an integrated research portal; 
(3) illustrations of VersatileUX, a library of \GUI{} components 
that could be adapted to provide custom front-ends 
for raw data published through an integrated research portal; 
and (4) an overview of an architecture for 
research portals which we call \lMOSAIC{}, 
that could guide the implementation of portals 
supporting data-set collections, such as a 
Covid-19 archive that could serve as a model 
for future work as well as a useful resource 
while the pandemic continues to present a global crisis.}

\p{}

\vspace{1.5em}
%\noindent\lun{ETS\textsc{pf} for Scientific and Technical Applications}

\end{document}


