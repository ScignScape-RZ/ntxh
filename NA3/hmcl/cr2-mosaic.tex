\documentclass[11pt,letterpaper]{article}


% pmml  arff  openannotation

%\usepackage[condensed,math]{anttor}
%\usepackage[T1]{fontenc}

%\usepackage[T1]{fontenc}
%\usepackage{tgtermes}

\usepackage[hang,flushmargin]{footmisc}

\usepackage{titlesec}

%\usepackage{sectsty}
%\sectionfont{\fontsize{13}{4}\selectfont}

\titleformat{\section}
  {\normalfont\fontsize{13}{15}\bfseries}{\thesection}{1em}{}

\let\OldPart\part

\renewcommand{\part}[1]{\OldPart{#1}%
%{\textcolor{darkRed}\hrule}
\vspace{-.5em}}

\titlespacing*{\section}
{0pt}{4ex plus 1ex minus .1ex}{-0.2ex plus .2ex}

\titlespacing*{\subsection}
{0pt}{3.5ex plus 1ex minus .1ex}{-.8ex plus .2ex}

%\usepackage{mathptmx}

\usepackage{eso-pic}

%\setlength\parindent{0pt}

\AddToShipoutPictureBG{%

\ifnum\value{page}>1{
\AtTextUpperLeft{
\makebox[20.5cm][r]{
\raisebox{-1.95cm}{%
{\transparent{0.3}{\includegraphics[width=0.29\textwidth]{e-logo.png}}	}} } }
}\fi
}

\AddToShipoutPicture{%
{
 {\color{blGreen!70!red}\transparent{0.9}{\put(0,0){\rule{3pt}{\paperheight}}}}%
 {\color{darkRed!70!purple}\transparent{1}\put(3,0){{\rule{4pt}{\paperheight}}}}
% {\color{logoPeach!80!cyan}\transparent{0.5}{\put(0,700){\rule{1cm}{.6cm}}}}%
% {\color{darkRed!60!cyan}\transparent{0.7}\put(0,706){{\rule{1cm}{.6cm}}}}
% \put(18,726){\thepage}
% \transparent{0.8}
}
}

\AddToShipoutPicture{%
\ifnum\value{page}=1
\put(257.5,942){%
	\transparent{0.7}{
		\includegraphics[width=0.2\textwidth]{logo.png}}}
\put(59,953){\textbf{{\fontfamily{phv}\fontsize{14}{14}\selectfont{}WHITE PAPER}}}
\fi
}	



\AddToShipoutPicture{%
\ifnum\value{page}>1
{\color{blGreen!70!red}\transparent{0.9}{\put(300,8){\rule{0.5\paperwidth}{.3cm}}}}%
{\color{inOne}\transparent{0.8}{\put(300,10){\rule{0.5\paperwidth}{.3cm}}}}%
{\color{inTwo}\transparent{0.3}\put(300,13){{\rule{0.5\paperwidth}{.3cm}}}}

%\ifnum\value{page}<39
\put(301,16){%
\transparent{0.7}{
\includegraphics[width=0.2\textwidth]{logo.png}}
}
%\fi

%\pgfmathparse{equal(\value{page},15)||equal(\value{page},20)?int(1):int(0)}

\ifnum\value{page}=44   %\pgfmathresult>0\relax
\put(182,139){
 {\setlength{\fboxsep}{.65em}\fontsize{9}{10}\selectfont

   {\color{white}{\parbox{11cm}{\vspace{7pt}\framebox{\begin{minipage}{.46\textwidth}
	  {\color{black}{\textit{For more information please contact:}}\\\textbf{\color{blGreen!40!blbl}{Amy Neustein, Ph.D., Founder and CEO}}\\  
       \textbf{{\color{blGreen!20!black}Linguistic Technology Systems \\
      amy.neustein@verizon.net \textbullet{} \textbf{(917) 817-2184} }}}
	 \end{minipage}}}}}}
}
\fi

{\color{blGreen!70!red}\transparent{0.9}{\put(5.6,5){\rule{0.5\paperwidth}{.4cm}}}}%
{\color{inOne}\transparent{1}{\put(5.6,10){\rule{0.5\paperwidth}{.4cm}}}}%
{\color{inTwo}\transparent{0.3}\put(5.6,15){{\rule{0.5\paperwidth}{.4cm}}}}

\fi
}

%\pagestyle{empty} % no page number
%\parskip 7.2pt    % space between paragraphs
%\parindent 12pt   % indent for new paragraph
%\textwidth 4.5in  % width of text
%\columnsep 0.8in  % separation between columns

%\setlength{\footskip}{7pt}

\usepackage[paperheight=14in,paperwidth=8.5in]{geometry}
\geometry{left=.83in,top=.5in,right=.81in,bottom=1.2in} %margins

\usepackage{etoolbox}% http://ctan.org/pkg/etoolbox
\makeatletter
% \patchcmd{<cmd>}{<search>}{<replace>}{<success>}{<failure>}
\patchcmd{\@part}{\par}{\quad}{}{}
\patchcmd{\@part}{\huge}{\Large}{}{}
\makeatother

\renewcommand{\partname}{\hspace{-1em}Part}

\renewcommand*\thepart{\Roman{part}:}

\renewcommand{\thepage}{\raisebox{2pt}{\arabic{page}}}

\renewcommand{\footnoterule}{%
	\kern -3pt
	\hrule width .92\textwidth height .5pt
	\kern 10pt
}


\usepackage[hyphens]{url}
\newcommand{\biburl}[1]{ {\fontfamily{gar}\selectfont{\textcolor[rgb]{.2,.6,0}%
{\scriptsize {\url{#1}}}}}}

%\linespread{1.3}

\newcommand{\sectsp}{\vspace{12pt}}

\usepackage{graphicx}
\usepackage{color,framed}

\usepackage{textcomp}

\usepackage{float}

\usepackage{mdframed}


\usepackage{setspace}
\newcommand{\rpdfNotice}[1]{\begin{onehalfspacing}{

\Large #1

}\end{onehalfspacing}}

\usepackage{xcolor}

\usepackage[hyphenbreaks]{breakurl}
\usepackage[hyphens]{url}

\usepackage{hyperref}
\newcommand{\rpdfLink}[1]{\href{#1}{\small{#1}}}
\newcommand{\dblHref}[1]{\href{#1}{\small{\burl{#1}}}}
\newcommand{\browseHref}[2]{\href{#1}{\Large #2}}

\colorlet{blCyan}{cyan!50!blue}

\definecolor{darkRed}{rgb}{.2,.0,.1}


\definecolor{blGreen}{rgb}{.2,.7,.3}

\definecolor{darkBlGreen}{rgb}{.1,.3,.2}

\definecolor{oldBlColor}{rgb}{.2,.7,.3}

\definecolor{blColor}{rgb}{.1,.3,.2}

\definecolor{elColor}{rgb}{.2,.1,0}
\definecolor{flColor}{rgb}{0.7,0.3,0.3}

\definecolor{logoOrange}{RGB}{108, 18, 30}
\definecolor{logoGreen}{RGB}{85, 153, 89}
\definecolor{logoPurple}{RGB}{200, 208, 30}

\definecolor{logoBlue}{RGB}{4, 2, 25}
\definecolor{logoPeach}{RGB}{255, 159, 102}
\definecolor{logoCyan}{RGB}{66, 206, 244}
\definecolor{logoRed}{rgb}{.3,0,0}

\newcommand{\colorq}[1]{{\color{logoOrange!70!black}{\q{\small\textbf{#1}}}}}

\definecolor{inOne}{rgb}{0.122, 0.435, 0.698}% Rule colour
\definecolor{inTwo}{rgb}{0.122, 0.698, 0.435}% Rule colour

\definecolor{outOne}{rgb}{0.435, 0.698, 0.122}% Rule colour
\definecolor{outTwo}{rgb}{0.698, 0.435, 0.122}% Rule colour

\colorlet{linkcolor}{flColor!60!red}


\hypersetup{
	colorlinks=true,
	citecolor=blCyan!40!green,
	filecolor=magenta!30!logoBlue,
	urlcolor=blue,
    linkcolor=linkcolor!70!black,
%    allcolors=blCyan!40!green
}


\usepackage[many]{tcolorbox}% http://ctan.org/pkg/tcolorbox

\usepackage{transparent}

\newlength{\bsep}
\setlength{\bsep}{-1pt}
\let\xbibitem\bibitem
\renewcommand{\bibitem}[2]{\vspace{\bsep}\xbibitem{#1}{#2}}

\newenvironment{cframed}{\begin{mdframed}[linecolor=logoPeach,linewidth=0.4mm]}{\end{mdframed}}

\newenvironment{ccframed}{\begin{mdframed}[backgroundcolor=logoGreen!5,linecolor=logoCyan!50!black,linewidth=0.4mm]}{\end{mdframed}}


%\usepackage[T1]{fontenc}

%\usepackage{aurical}
% \Fontauri

\usepackage{gfsdidot}
\usepackage[T1]{fontenc}

%\makeatletter
%\f@family,  cmr, T1, n, m,
%\f@encoding,
%\f@shape,
%\f@series,
%\makeatother



%\usepackage{LibreBodoni}

%\usepackage{fontspec}
%\setmainfont{QTBengal}

\usepackage{relsize}

\newcommand{\bref}[1]{\hspace*{1pt}\textbf{\ref{#1}}}

\newcommand{\pseudoIndent}{

\vspace{10pt}\hspace*{12pt}}

\newcommand{\YPDFI}{{\fontfamily{fvs}\selectfont YPDF-Interactive}}

%
\newcommand{\deconum}[1]{{\protect\raisebox{-1pt}{{\LARGE #1}}}}

\newcommand{\visavis}{vis-\`a-vis}

\newcommand{\VersatileUX}{{\color{red!85!black}{\Fontauri Versatile}}%
{{\fontfamily{qhv}\selectfont\smaller UX}}}

\newcommand{\NDPCloud}{{\color{red!15!black}%
{\fontfamily{qhv}\selectfont {\smaller NDP C{\smaller LOUD}}}}}

\newcommand{\MThreeK}{{\color{blGreen!45!black}%
{\fontfamily{qhv}\fontsize{10}{8}\selectfont {M3K}}}}


\newcommand{\lfNDPCloud}{{\color{red!15!black}%
{\fontfamily{qhv}\selectfont N{\smaller DP C{\smaller LOUD}}}}}

\newcommand{\textds}[1]{{\fontfamily{lmdh}\selectfont{%
\raisebox{-1pt}{#1}}}}

\newcommand{\dsC}{{\textds{ds}{\fontfamily{qhv}\selectfont \raisebox{-1pt}
{\color{red!15!black}{C}}}}}

\definecolor{tcolor}{RGB}{24,52,61}

\newcommand{\CCpp}{\resizebox{!}{7pt}{\AcronymText{C}}/\Cpp{}}
\newcommand{\NoSQL}{\resizebox{!}{7pt}{\AcronymText{NoSQL}}}
\newcommand{\SQL}{\resizebox{!}{7pt}{\AcronymText{SQL}}}

\newcommand{\SPARQL}{\resizebox{!}{7pt}{\AcronymText{SPARQL}}}

\newcommand{\NCBI}{\resizebox{!}{7pt}{\AcronymText{NCBI}}}

\newcommand{\HTXN}{\resizebox{!}{7pt}{\ATexttclr{HTXN}}}

\newcommand{\TDM}{\resizebox{!}{7pt}{\AcronymText{TDM}}}

\newcommand{\lHTXN}{\resizebox{!}{7.5pt}{\ATexttclr{H}}%
\resizebox{!}{6.5pt}{\ATexttclr{TXN}}}

\newcommand{\lsHTXN}{\resizebox{!}{9.5pt}{\ATexttclr{HTXN}}}

\newcommand{\LAF}{\resizebox{!}{7pt}{\AcronymText{LAF}}}

\newcommand{\UDpipe}{\resizebox{!}{7pt}{\AcronymText{UDpipe}}}

\newcommand{\C}{\resizebox{!}{7pt}{\AcronymText{C}}}

\newcommand{\FCS}{\resizebox{!}{7pt}{\AcronymText{FCS}}}

\newcommand{\GAVI}{\resizebox{!}{7pt}{\AcronymText{GAVI}}}
\newcommand{\ArcGIS}{\resizebox{!}{7pt}{\AcronymText{ArcGIS}}}
\newcommand{\QGIS}{\resizebox{!}{7pt}{\AcronymText{QGIS}}}

\newcommand{\GIS}{\resizebox{!}{7pt}{\AcronymText{GIS}}}
\newcommand{\AngelScript}{\resizebox{!}{7pt}{\AcronymText{AngelScript}}}



\usepackage{mdframed}

\newcommand{\cframedboxpanda}[1]{\begin{mdframed}[linecolor=yellow!70!blue,linewidth=0.4mm]#1\end{mdframed}}


\newcommand{\PVD}{\resizebox{!}{7pt}{\AcronymText{PVD}}}

\newcommand{\SDK}{\resizebox{!}{7pt}{\AcronymText{SDK}}}
\newcommand{\NLP}{\resizebox{!}{7pt}{\AcronymText{NLP}}}

\newcommand{\AXF}{\resizebox{!}{7pt}{\ATexttclr{AXF}}}

\newcommand{\HyperGraphDB}{\resizebox{!}{7pt}{\AcronymText{HyperGraphDB}}}

\newcommand{\AllegroGraph}{\resizebox{!}{7pt}{\AcronymText{AllegroGraph}}}

\newcommand{\Grakenai}{\resizebox{!}{7pt}{\AcronymText{Graken.ai}}}


\newcommand{\lAXF}{\resizebox{!}{7.5pt}{\ATexttclr{A}}%
\resizebox{!}{6.5pt}{\ATexttclr{XF}}}


\newcommand{\lsAXF}{\resizebox{!}{8.5pt}{\ATexttclr{AXF}}}

\newcommand{\AXFD}{\resizebox{!}{7pt}{\ATexttclr{AXFD}}}

\newcommand{\CBICA}{\resizebox{!}{7pt}{\AcronymText{CBICA}}}

\newcommand{\IORT}{\resizebox{!}{7pt}{\AcronymText{IORT}}}


\newcommand{\SeDI}{\resizebox{!}{7pt}{\AcronymText{SeDI}}}
\newcommand{\RSNA}{\resizebox{!}{7pt}{\AcronymText{RSNA}}}

\newcommand{\CER}{\resizebox{!}{7pt}{\AcronymText{CER}}}
\newcommand{\PACS}{\resizebox{!}{7pt}{\AcronymText{PACS}}}

\newcommand{\DICOM}{\resizebox{!}{7pt}{\AcronymText{DICOM}}}

\newcommand{\CT}{\resizebox{!}{7pt}{\AcronymText{CT}}}

\newcommand{\LOINC}{\resizebox{!}{7pt}{\AcronymText{LOINC}}}

\newcommand{\RadLex}{\resizebox{!}{7pt}{\AcronymText{RadLex}}}


\newcommand{\OMOP}{\resizebox{!}{7pt}{\AcronymText{OMOP}}}
\newcommand{\PCORnet}{\resizebox{!}{7pt}{\AcronymText{PCORnet}}}
\newcommand{\FHIR}{\resizebox{!}{7pt}{\AcronymText{FHIR}}}

\newcommand{\CaPTk}{\resizebox{!}{7pt}{\AcronymText{CaPTk}}}

\newcommand{\VIOLIN}{\resizebox{!}{7pt}{\AcronymText{VIOLIN}}}



\newcommand{\lAXFD}{\resizebox{!}{7.5pt}{\ATexttclr{A}}%
\resizebox{!}{6.5pt}{\ATexttclr{XFD}}}


\newcommand{\IJST}{\resizebox{!}{7pt}{\AcronymText{IJST}}}

\newcommand{\BioC}{\resizebox{!}{7pt}{\AcronymText{BioC}}}

\newcommand{\CoNLL}{\resizebox{!}{7pt}{\AcronymText{CoNLL}}}
\newcommand{\CoNLLU}{\resizebox{!}{7pt}{\AcronymText{CoNLL-U}}}

\newcommand{\sapp}{\resizebox{!}{7pt}{\AcronymText{Sapien+}}}
\newcommand{\lsapp}{\resizebox{!}{8.5pt}{\AcronymText{Sapien+}}}
\newcommand{\lssapp}{\resizebox{!}{9.5pt}{\AcronymText{Sapien+}}}

\newcommand{\ePub}{\resizebox{!}{7pt}{\AcronymText{ePub}}}

%\lsLPF


\newcommand{\GIT}{\resizebox{!}{7pt}{\AcronymText{GIT}}}

%\definecolor{atColor}{RGB}{11, 71, 17}


\DeclareMathVersion{fordg}
\SetSymbolFont{letters}{fordg}{OML}{cmr}{b}{n}

\definecolor{atcColor}{RGB}{96, 17, 12}
%\textcolor{tcolor}{

\newcommand{\ATextCClr}[1]{\textcolor{atcColor}{\textbf{#1}}}

\newcommand{\ATexttclr}[1]{\textcolor{tcolor}{\textbf{#1}}}

\newcommand{\AIMConc}{\resizebox{!}{7.5pt}{\ATextCClr{AIM-Concepts}}}
\newcommand{\lAIMConc}{\resizebox{!}{8pt}{\ATextCClr{AIM-Concepts}}}

\newcommand{\HGXF}{{\resizebox{!}{7.5pt}{\ATexttclr{HGXF}}}}
\newcommand{\lHGXF}{{\resizebox{!}{8pt}{\ATexttclr{HGXF}}}}
\newcommand{\sHGXF}{{\resizebox{!}{6pt}{\ATexttclr{HGXF}}}}

\newcommand{\CRtwo}{{\resizebox{!}{7.5pt}{\ATextCClr{CR2}}}}
\newcommand{\lCRtwo}{{\resizebox{!}{8pt}{\ATextCClr{CR2}}}}
\newcommand{\sCRtwo}{{\resizebox{!}{6pt}{\ATextCClr{CR2}}}}


\newcommand{\THQL}{\resizebox{!}{7.5pt}{\ATexttclr{THQL}}}
\newcommand{\lTHQL}{\resizebox{!}{8pt}{\ATexttclr{THQL}}}

\newcommand{\HDICOM}{\resizebox{!}{7.5pt}{\ATexttclr{{\large h}-DICOM}}}

\newcommand{\hVaImm}{\resizebox{!}{7.5pt}{\ATexttclr{{\large h}-VaImm}}}


\newcommand{\PhaonVI}{\resizebox{!}{7.5pt}{\ATexttclr{Phaon-VI}}}



\definecolor{atColor}{RGB}{50, 22, 40}
\newcommand{\ATextClr}[1]{\textcolor{atColor}{\textbf{#1}}}

\newcommand{\DgDb}{{\mathversion{fordg}%
\makebox{\raisebox{-3pt}{\resizebox{!}{11pt}{\ATextClr{%
\rotatebox{17}{$\varsigma$}}}}\hspace{-4pt}%
\resizebox{!}{6.5pt}{\ATextClr{D\hspace{-2pt}B}}}}}


\newcommand{\lDgDb}{{\mathversion{fordg}%
\resizebox{!}{12pt}{\ATextClr{%
\rotatebox{17}{$\varsigma$}}}}\hspace{-4pt}%
\resizebox{!}{6.5pt}{\ATextClr{D\hspace{-2pt}B}}}}}

\newcommand{\URL}{\resizebox{!}{7pt}{\AcronymText{URL}}}
\newcommand{\CSML}{\resizebox{!}{7pt}{\AcronymText{CSML}}}
\newcommand{\LPF}{\resizebox{!}{7pt}{\AcronymText{LPF}}}
\newcommand{\lLPF}{\resizebox{!}{8.5pt}{\AcronymText{LPF}}}
\newcommand{\lsLPF}{\resizebox{!}{9.5pt}{\AcronymText{LPF}}}

\newcommand{\AI}{\resizebox{!}{7.5pt}{\AcronymText{AI}}}
\newcommand{\lAI}{\resizebox{!}{8pt}{\AcronymText{AI}}}

\newcommand{\Jupyter}{\resizebox{!}{7pt}{\AcronymText{Jupyter}}}
\newcommand{\Python}{\resizebox{!}{7pt}{\AcronymText{Python}}}
\newcommand{\IDN}{\resizebox{!}{7pt}{\AcronymText{IDN}}}
\newcommand{\JPG}{\resizebox{!}{7pt}{\AcronymText{JPG}}}
\newcommand{\JPEG}{\resizebox{!}{7pt}{\AcronymText{JPEG}}}
\newcommand{\PNG}{\resizebox{!}{7pt}{\AcronymText{PNG}}}
\newcommand{\TIFF}{\resizebox{!}{7pt}{\AcronymText{TIFF}}}
\newcommand{\REPL}{\resizebox{!}{7pt}{\AcronymText{REPL}}}

\newcommand{\MIFlowCyt}{\resizebox{!}{7pt}{\AcronymText{MIFlowCyt}}}
\newcommand{\GatingML}{\resizebox{!}{7pt}{\AcronymText{Gating-ML}}}
\newcommand{\flowCL}{\resizebox{!}{7pt}{\AcronymText{flowCL}}}


\makeatletter

\newcommand*\getX[1]{\expandafter\getX@i#1\@nil}

\newcommand*\getY[1]{\expandafter\getY@i#1\@nil}
\def\getX@i#1,#2\@nil{#1}
\def\getY@i#1,#2\@nil{#2}
\makeatother
	
\newcommand{\rectann}[9]{%
\path [draw=#1,draw opacity=#2,line width=#3, fill=#4, fill opacity = #5, even odd rule] %
(#6) rectangle(\getX{#6}+#7,\getY{#6}+#8)
({\getX{#6}+((#7-(#7*#9))/2)},{\getY{#6}+((#8-(#8*#9))/2)}) rectangle %
({\getX{#6}+((#7-(#7*#9))/2)+#7*#9},{\getY{#6}+((#8-(#8*#9))/2)+#8*#9});}


\definecolor{pfcolor}{RGB}{94, 54, 73}

\newcommand{\EPF}{\resizebox{!}{7pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}
\newcommand{\lEPF}{\resizebox{!}{8.5pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}
\newcommand{\lsEPF}{\resizebox{!}{9.5pt}{\AcronymText{ETS{\color{pfcolor}pf}}}}


\newcommand{\XPDF}{\resizebox{!}{7pt}{\AcronymText{XPDF}}}

\newcommand{\GRE}{\resizebox{!}{7pt}{\AcronymText{GRE}}}
\newcommand{\CAS}{\resizebox{!}{7pt}{\AcronymText{CAS}}}

\newcommand{\lMOSAIC}{%
\resizebox{!}{8pt}{\ATexttclr{M}}%
\resizebox{!}{6pt}{\ATexttclr{OSAIC}}}

\newcommand{\XML}{\resizebox{!}{7pt}{\AcronymText{XML}}}
\newcommand{\RDF}{\resizebox{!}{7pt}{\AcronymText{RDF}}}
\newcommand{\DOM}{\resizebox{!}{7pt}{\AcronymText{DOM}}}

\newcommand{\Java}{\resizebox{!}{7pt}{\AcronymText{Java}}}


\newcommand{\ParaView}{\resizebox{!}{7pt}{\AcronymText{ParaView}}}
\newcommand{\Octave}{\resizebox{!}{7pt}{\AcronymText{Octave}}}
\newcommand{\ROOT}{\resizebox{!}{7pt}{\AcronymText{ROOT}}}
\newcommand{\CERN}{\resizebox{!}{7pt}{\AcronymText{CERN}}}
\newcommand{\MQFour}{\resizebox{!}{7pt}{\AcronymText{MQ4}}}
\newcommand{\VISSION}{\resizebox{!}{7pt}{\AcronymText{VISSION}}}

\newcommand{\ReproZip}{\resizebox{!}{7pt}{\AcronymText{ReproZip}}}
\newcommand{\BioCoder}{\resizebox{!}{7pt}{\AcronymText{BioCoder}}}

\newcommand{\Covid}{\resizebox{!}{7pt}{\AcronymText{Covid-19}}}


\newcommand{\HMCL}{{\resizebox{!}{7.5pt}{\ATexttclr{HMCL}}}}
\newcommand{\DSPIN}{{\resizebox{!}{7.5pt}{\ATexttclr{D-SPIN}}}}


\newcommand{\CLang}{\resizebox{!}{7pt}{\AcronymText{C}}}

\newcommand{\HNaN}{\resizebox{!}{7pt}{\AcronymText{HN%
\textsc{a}N}}}

\newcommand{\JSON}{\resizebox{!}{7pt}{\AcronymText{JSON}}}
\newcommand{\UV}{\resizebox{!}{7pt}{\AcronymText{UV}}}


\newcommand{\PET}{\resizebox{!}{7pt}{\AcronymText{PET}}}
\newcommand{\MRI}{\resizebox{!}{7pt}{\AcronymText{MRI}}}


\newcommand{\MeshLab}{\resizebox{!}{7pt}{\AcronymText{MeshLab}}}
\newcommand{\IQmol}{\resizebox{!}{7pt}{\AcronymText{IQmol}}}

\newcommand{\SGML}{\resizebox{!}{7pt}{\AcronymText{SGML}}}

\newcommand{\WhiteDB}{\resizebox{!}{7pt}{\AcronymText{\makebox{WhiteDB}}}}

\newcommand{\CrossRef}{\resizebox{!}{7pt}{\AcronymText{CrossRef}}}

\newcommand{\ASCII}{\resizebox{!}{7pt}{\AcronymText{ASCII}}}

\newcommand{\GUI}{\resizebox{!}{7pt}{\AcronymText{GUI}}}
\newcommand{\UI}{\resizebox{!}{7pt}{\AcronymText{UI}}}


\newcommand{\URI}{\resizebox{!}{7pt}{\AcronymText{URI}}}
\newcommand{\DTD}{\resizebox{!}{7pt}{\AcronymText{DTD}}}

\newcommand{\API}{\resizebox{!}{7pt}{\AcronymText{API}}}

\newcommand{\JATS}{\resizebox{!}{7pt}{\AcronymText{JATS}}}


\newcommand{\SDI}{\resizebox{!}{7pt}{\AcronymText{SDI}}}
\newcommand{\SDIV}{\resizebox{!}{7pt}{\AcronymText{SDIV}}}

\definecolor{atColor}{RGB}{50, 22, 40}
\newcommand{\ATextClr}[1]{\textcolor{atColor}{\textbf{#1}}}

\newcommand{\DgDb}{\makebox{\raisebox{-3pt}{\resizebox{!}{11pt}{\ATextClr{%
\rotatebox{17}{$\varsigma$}}}}\hspace{-4pt}%
\resizebox{!}{6.5pt}{\ATextClr{D\hspace{-2pt}B}}}}

\newcommand{\lDgDb}{\makebox{\raisebox{-3pt}{%
\resizebox{!}{12pt}{\ATextClr{%
\rotatebox{17}{$\varsigma$}}}}\hspace{-4pt}%
\resizebox{!}{6.5pt}{\ATextClr{D\hspace{-2pt}B}}}}


\newcommand{\IDE}{\resizebox{!}{7pt}{\AcronymText{IDE}}}

\newcommand{\OWL}{\resizebox{!}{7pt}{\AcronymText{OWL}}}

\newcommand{\Kaggle}{\resizebox{!}{7pt}{\AcronymText{Kaggle}}}


\newcommand{\ViSion}{\resizebox{!}{7pt}{\AcronymText{ViSion}}}

\newcommand{\CWL}{\resizebox{!}{7pt}{\AcronymText{CWL}}}

\newcommand{\ThreeD}{\resizebox{!}{7pt}{\AcronymText{3D}}}
\newcommand{\TwoD}{\resizebox{!}{7pt}{\AcronymText{2D}}}

\newcommand{\medInria}{\resizebox{!}{7pt}{\AcronymText{medInria}}}
\newcommand{\ThreeDimViewer}{\resizebox{!}{7pt}{\AcronymText{3DimViewer}}}

\newcommand{\FAIR}{\resizebox{!}{7pt}{\AcronymText{FAIR}}}

\newcommand{\QNetworkManager}{\resizebox{!}{7pt}{\AcronymText{QNetworkManager}}}
\newcommand{\QTextDocument}{\resizebox{!}{7pt}{\AcronymText{QTextDocument}}}
\newcommand{\QWebEngineView}{\resizebox{!}{7pt}{\AcronymText{QWebEngineView}}}
\newcommand{\HTTP}{\resizebox{!}{7pt}{\AcronymText{HTTP}}}


\newcommand{\lAcronymTextNC}[2]{{\fontfamily{fvs}\selectfont {\Large{#1}}{\large{#2}}}}

\newcommand{\AcronymTextNC}[1]{{\fontfamily{fvs}\selectfont {\large #1}}}


\colorlet{orr}{orange!60!red}

\newcommand{\textscc}[1]{{\color{orr!35!black}{{%
						\fontfamily{Cabin-TLF}\fontseries{b}\selectfont{\textsc{\scriptsize{#1}}}}}}}


\newcommand{\textsccserif}[1]{{\color{orr!35!black}{{%
				\scriptsize{\textbf{#1}}}}}}


\newcommand{\iXPDF}{\resizebox{!}{7pt}{\textsccserif{%
\textit{XPDF}}}}

\newcommand{\iEPF}{\resizebox{!}{7pt}{\textsccserif{%
\textit{ETSpf}}}}

\newcommand{\iSDI}{\resizebox{!}{7pt}{\textsccserif{%
\textit{SDI}}}}

\newcommand{\iHTXN}{\resizebox{!}{7pt}{\textsccserif{%
\textit{HTXN}}}}


\newcommand{\AcronymText}[1]{{\textscc{#1}}}

\newcommand{\AcronymTextser}[1]{{\textsccserif{#1}}}


\newcommand{\mAcronymText}[1]{{\textscc{\normalsize{#1}}}}

\newcommand{\FASTA}{{\resizebox{!}{7pt}{\AcronymText{FASTA}}}}
\newcommand{\SRA}{{\resizebox{!}{7pt}{\AcronymText{SRA}}}}
\newcommand{\DNA}{{\resizebox{!}{7pt}{\AcronymText{DNA}}}}
\newcommand{\MAP}{{\resizebox{!}{7pt}{\AcronymText{MAP}}}}
\newcommand{\EPS}{{\resizebox{!}{7pt}{\AcronymText{EPS}}}}
\newcommand{\CSV}{{\resizebox{!}{7pt}{\AcronymText{CSV}}}}
\newcommand{\PDB}{{\resizebox{!}{7pt}{\AcronymText{PDB}}}}

\newcommand{\WebGL}{{\resizebox{!}{7pt}{\AcronymText{WebGL}}}}
\newcommand{\Docker}{{\resizebox{!}{7pt}{\AcronymText{Docker}}}}


\newcommand{\OBO}{{\resizebox{!}{7pt}{\AcronymText{OBO}}}}

\newcommand{\XOCS}{{\resizebox{!}{7pt}{\AcronymText{XOCS}}}}

\newcommand{\ChemXML}{{\resizebox{!}{7pt}{\AcronymText{ChemXML}}}}

\newcommand{\TeXMECS}{\resizebox{!}{7pt}{\AcronymText{TeXMECS}}}

% pmml  arff  openannotation

\newcommand{\PMML}{\resizebox{!}{7pt}{\AcronymText{PMML}}}
\newcommand{\ARFF}{\resizebox{!}{7pt}{\AcronymText{ARFF}}}
\newcommand{\IeXML}{\resizebox{!}{7pt}{\AcronymText{IeXML}}}

\newcommand{\SeCo}{\resizebox{!}{7pt}{\AcronymText{SeCo}}}


\newcommand{\HDFFive}{\resizebox{!}{7pt}{\AcronymText{HDF5}}}

\newcommand{\NGML}{\resizebox{!}{7pt}{\ATexttclr{NGML}}}


\newcommand{\Cpp}{\resizebox{!}{7pt}{\AcronymText{C++}}}

%\newcommand{\\WhiteDB{}}{\resizebox{!}{7pt}{\AcronymText{\WhiteDB{}}}}

\colorlet{drp}{darkRed!70!purple}

%\newcommand{\MOSAIC}{{\color{drp}{\AcronymTextNC{\scriptsize{MOSAIC}}}}}

\newcommand{\MOSAIC}{\resizebox{!}{7pt}{\ATexttclr{MOSAIC}}}


\newcommand{\mMOSAIC}{{\color{drp}{\AcronymTextNC{\normalsize{MOSAIC}}}}}

\newcommand{\MOSAICVM}{\mMOSAIC-\mAcronymText{VM}}

\newcommand{\sMOSAICVM}{\resizebox{!}{7pt}{\MOSAICVM}}
\newcommand{\sMOSAIC}{\resizebox{!}{7pt}{\MOSAIC}}

\newcommand{\LDOM}{\resizebox{!}{7pt}{\AcronymText{LDOM}}}
\newcommand{\Cnineteen}{\resizebox{!}{7pt}{\AcronymText{CORD-19}}}

\newcommand{\lCnineteen}{\resizebox{!}{7.5pt}{\AcronymText{CORD-19}}}


\newcommand{\MOL}{\resizebox{!}{7pt}{\AcronymText{MOL}}}

\newcommand{\ACL}{\resizebox{!}{7pt}{\AcronymText{ACL}}}

\newcommand{\LXCR}{\resizebox{!}{7pt}{\AcronymText{LXCR}}}
\newcommand{\lLXCR}{\resizebox{!}{8.5pt}{\AcronymText{LXCR}}}
\newcommand{\lsLXCR}{\resizebox{!}{9.5pt}{\AcronymText{LXCR}}}

%\newcommand{\lMOSAIC}{{\color{drp}{\lAcronymTextNC{M}{OSAIC}}}}
\newcommand{\lfMOSAIC}{\resizebox{!}{9pt}{{\color{drp}{\lAcronymTextNC{M}{OSAIC}}}}}

\newcommand{\Mosaic}{\resizebox{!}{7pt}{\MOSAIC}}
\newcommand{\MosaicPortal}{{\color{drp}{\AcronymTextNC{MOSAIC Portal}}}}

\newcommand{\RnD}{\resizebox{!}{7pt}{\AcronymText{R\&D}}}

\newcommand{\MIBBI}{\resizebox{!}{7pt}{\AcronymText{MIBBI}}}

\newcommand{\JVM}{\resizebox{!}{7pt}{\AcronymText{JVM}}}
\newcommand{\ECL}{\resizebox{!}{7pt}{\AcronymText{ECL}}}

\newcommand{\ChaiScript}{\resizebox{!}{7pt}{\AcronymText{ChaiScript}}}

\newcommand{\TCP}{\resizebox{!}{7pt}{\AcronymText{TCP}}}

\newcommand{\lQt}{\resizebox{!}{8.5pt}{\AcronymText{Qt}}}
\newcommand{\QtCpp}{\resizebox{!}{8.5pt}{\AcronymText{Qt/C++}}}
\newcommand{\Qt}{\resizebox{!}{7pt}{\AcronymText{Qt}}}

\newcommand{\QtSQL}{\resizebox{!}{7pt}{\AcronymText{QtSQL}}}

\newcommand{\HTML}{\resizebox{!}{7pt}{\AcronymText{HTML}}}
\newcommand{\PDF}{\resizebox{!}{7pt}{\AcronymText{PDF}}}

\newcommand{\R}{\resizebox{!}{7pt}{\AcronymText{R}}}
\newcommand{\SciXML}{\resizebox{!}{7pt}{\AcronymText{SciXML}}}

\newcommand{\MPF}{\resizebox{!}{7pt}{\ATexttclr{MPF}}}


\newcommand{\lGRE}{\resizebox{!}{7.5pt}{\AcronymText{GRE}}}

\newcommand{\p}[1]{

\vspace{.9em}#1}

\newcommand{\q}[1]{{\fontfamily{qcr}\selectfont ``}#1{\fontfamily{qcr}\selectfont ''}} 

%\newcommand{\deconum}[1]{{\textcircled{#1}}}

\renewcommand{\thesection}{\protect\hspace{-1.5em}}
%\renewcommand{\thesection}{\protect\mbox{\deconum{\Roman{section}}}}
\renewcommand{\thesubsection}{\protect\hspace{-1em}}

\newcommand{\llMOSAIC}{\mbox{{\LARGE MOSAIC}}}
%\newcommand{\lfMOSAIC}{\mbox{M\small{OSAIC}}}

\newcommand{\llMosaic}{\llMOSAIC}
\newcommand{\lMosaic}{\lMOSAIC}
\newcommand{\lfMosaic}{\lfMOSAIC}

%\newcommand{\dsC}{}

\newcommand{\textds}[1]{{\fontfamily{lmdh}\selectfont{%
\raisebox{-1pt}{#1}}}}

\newcommand{\ltextds}[1]{{\fontfamily{lmdh}\fontsize{12}{11}\selectfont{%
\raisebox{-1pt}{#1}}}}

\newcommand{\dsC}{{\textds{ds}{\fontfamily{qhv}\selectfont \raisebox{-1pt}{C}}}}
\newcommand{\ldsC}{{\textds{ds}{\fontfamily{qhv}\selectfont \raisebox{-1pt}{C}}}}

\newcommand{\MdsX}{\resizebox{!}{9pt}{\ATexttclr{\raisebox{-1pt}{{\fontfamily{lmdh}\selectfont M}}\fontfamily{qhv}\selectfont dsX}}}
\newcommand{\lsMdsX}{\resizebox{!}{10.5pt}{\ATexttclr{\raisebox{-1pt}{{\fontfamily{lmdh}\selectfont M}}\fontfamily{qhv}\selectfont dsX}}}
\newcommand{\lMdsX}{\resizebox{!}{9.5pt}{\ATexttclr{\raisebox{-1pt}{{\fontfamily{lmdh}\selectfont M}}\fontfamily{qhv}\selectfont dsX}}}


\newcommand{\llWC}{\mbox{{\LARGE WhiteCharmDB}}}

\newcommand{\llwh}{\mbox{{\LARGE White}}}
\newcommand{\llch}{\mbox{{\LARGE CharmDB}}}

\usepackage{enumitem}
%\usepackage{listings}

\colorlet{dsl}{purple!20!brown}
\colorlet{dslr}{dsl!50!blue}

\setlist[description]{%
  topsep=11pt,
  labelsep=22pt, leftmargin=10pt,
  itemsep=9pt,               % space between items
  %font={\bfseries\sffamily}, % set the label font
  font=\normalfont\bfseries\color{dslr!50!black}, % if colour is needed
}

\setlist[enumerate]{%
  topsep=3pt,               % space before start / after end of list
  itemsep=-2pt,               % space between items
  font={\bfseries\sffamily}, % set the label font
%  font={\bfseries\sffamily\color{red}}, % if colour is needed
}

%\usepackage{tcolorbox}

\newcommand{\slead}[1]{%
\noindent{\raisebox{2pt}{\relscale{1.15}{{{%
\fcolorbox{logoCyan!50!black}{logoGreen!5}{#1}
}}}}}\hspace{.5em}}


\let\OldLaTeX\LaTeX

\renewcommand{\LaTeX}{\resizebox{!}{7pt}{\color{orr!35!black}{\OldLaTeX}}}

\let\OldTeX\TeX

\renewcommand{\TeX}{\resizebox{!}{7pt}{\color{orr!35!black}{\OldTeX}}}


\newcommand{\LargeLaTeX}{\resizebox{!}{8.5pt}{\color{orr!35!black}{\OldLaTeX}}}

\setlength\parindent{0pt}
%\setlength\parindent{24pt}
%\input{commands}

\newcommand{\lun}[1]{\raisebox{-4pt}{\fontfamily{qcr}\selectfont{%
\LARGE{\textbf{\textcolor{tcolor}{#1}}}}}\vspace{-2pt}}

\newcommand{\inditem}{\itemindent10pt\item}

\usepackage{soul}

\definecolor{hlcolor}{RGB}{114, 54, 203}
\colorlet{hlcol}{hlcolor!35}
\sethlcolor{hlcol}

\makeatletter
\def\SOUL@hlpreamble{%
	\setul{}{3ex}%         !!!change this value!!! default is 2.5ex
	\let\SOUL@stcolor\SOUL@hlcolor
	\SOUL@stpreamble
}
\makeatother

\usepackage{scrextend}
%\vspace*{3em}
\newenvironment{mldescription}{\vspace{1em}%
  \begin{addmargin}[4pt]{1em}
    \setlength{\parindent}{-1em}%
    \newcommand*{\mlitem}[1][]{\vspace{5pt}\par\medskip%
%\colorbox{hlcolor}{\textbf{##1}}\quad}\indent
\hl{ \textbf{##1} }\quad}\indent
}{%
  \end{addmargin}
  \medskip
}

\usepackage{marginnote}

\newcommand{\mnote}[1]{%
\vspace*{-2em}
\reversemarginpar
\raisebox{1em}{\marginnote{\parbox{4em}{%
\begin{mdframed}[innerleftmargin=4pt,
	innerrightmargin=1pt,innertopmargin=1pt,
	linecolor=red!20!cyan,userdefinedwidth=4em,
	topline=false,
	rightline=false]
{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
		\textit{#1}}}
\end{mdframed}}
	}[3em]}}

\newcommand{\mnotel}[1]{%
\vspace*{-2em}
\reversemarginpar
\raisebox{-4em}{\marginnote{\parbox{4em}{%
\begin{mdframed}[innerleftmargin=4pt,
	innerrightmargin=1pt,innertopmargin=1pt,
	linecolor=red!20!cyan,userdefinedwidth=4em,
	topline=false,
	rightline=false]
{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
		\textit{#1}}}
\end{mdframed}}
	}[3em]}}

\newcommand{\mnoteh}[3]{%
	\vspace*{#1}
	\reversemarginpar
	\raisebox{#2}{\marginnote{\parbox{4em}{%
				\begin{mdframed}[innerleftmargin=4pt,
					innerrightmargin=1pt,innertopmargin=1pt,
					linecolor=red!20!cyan,userdefinedwidth=4em,
					topline=false,
					rightline=false]
					{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
							\textit{#3}}}
				\end{mdframed}}
			}[3em]}}


\newcommand{\mnoteb}[1]{%
	\vspace*{1em}
	\reversemarginpar
	\raisebox{1em}{\marginnote{\parbox{4em}{%
				\begin{mdframed}[innerleftmargin=4pt,
					innerrightmargin=1pt,innertopmargin=1pt,
					linecolor=red!20!cyan,userdefinedwidth=4em,
					topline=false,
					rightline=false]
					{{\fontfamily{ppl}\fontsize{12}{0}\selectfont
							\textit{#1}}}
				\end{mdframed}}
			}[3em]}}
	
\usepackage{wrapfig}

\usetikzlibrary{arrows, decorations.markings}
\usetikzlibrary{shapes.arrows}

\newcommand{\curicon}[2]{%
	\node at (#1,#2) [
	draw=black,
	%minimum width=2ex,
	inner sep=.7pt,
	fill=white,
	single arrow,
	single arrow head extend=3pt,
	single arrow head indent=1.5pt,
	single arrow tip angle=45,
	line join=bevel,
	minimum height=4.6mm,
	rotate=115
	] {};
}

\makeatletter
\def\@cite#1#2{[\textbf{#1\if@tempswa , #2\fi}]}
\def\@biblabel#1{[\textbf{#1}]}
\makeatother


%\let\origref\ref
%\renewcommand{\ref}[1]{{\LARGE #1}}

%\def\ref#1{\textbf{\origref{{\LARGE #1}}}}

\setlength{\footnotesep}{0pt}

\renewcommand{\thefootnote}{\textcolor{logoGreen!80!logoBlue}{{\fontfamily{qcr}\fontseries{b}\fontsize{10}{4}\selectfont\arabic{footnote}}}}


\newcommand{\LVee}{{\colorbox{cyan!40!yellow}{\textcolor{red!70!navy}{\textbf{\LARGE$\vee$}}}}}
\newcommand{\LWedge}{{\colorbox{cyan!40!yellow}{\textcolor{red!70!navy}{\textbf{\LARGE$\wedge$}}}}}

\renewcommand{\LVee}{}
\renewcommand{\LWedge}{}


\urlstyle{same}

\usepackage[preserveurlmacro]{breakurl}

\newcommand{\bhref}[1]{\href{#1}{\burl{#1}}}

%\setmainfont{QTChanceryType}

\begin{document}

\setlength{\skip\footins}{18pt}	
	
{\linespread{1.25}\selectfont

\vspace*{1.5em}

\begin{center}
%{\relscale{1.2}{\fontfamily{qcr}\fontseries{b}\selectfont 
%{\colorbox{black}{\color{blue}{\llWC{} Database Engine \\and 
%\llMOSAIC{} Native Application Toolkit}}}}}

\colorlet{ctmp}{logoPeach!20!gray}
\colorlet{ctmpp}{ctmp!90!yellow}
\colorlet{ctmppp}{ctmpp!50!black}
\colorlet{ctmpppp}{ctmppp!90!logoRed}
\colorlet{ctmcyan}{ctmpppp!70!cyan}

\colorlet{ctmppppy}{ctmppp!60!orange}

%{\colorbox{darkBlGreen!30!darkRed}{%
\begin{tcolorbox}
[
%%enhanced,
%%frame hidden,
%interior hidden
arc=2pt,outer arc=0pt,
enhanced jigsaw,
width=\textwidth,
colback=ctmppppy!40,
%colback=ctmcyan!50,
colframe=logoRed!30!darkRed,
drop shadow=logoPurple!50!darkRed,
%boxsep=0pt,
%left=0pt,
%right=0pt,
%top=2pt,
]
%\hspace{22pt}
\begin{minipage}{\textwidth}	
\begin{center}	
{\setlength{\fboxsep}{32pt}
	\relscale{1.2}{{\fontfamily{qcr}\fontseries{b}\selectfont%
{New Database Engineering and 
Archive Construction Technology to Accelerate 
Bio-Imaging, Biomedical Engineering, and Covid-19 Research}
}}}
\end{center}
\end{minipage}
\end{tcolorbox}
\end{center}

%\vspace{1em}
\vspace*{2pt}
\begin{center}
\parbox{.85\textwidth}{%
{\fontfamily{fvs}\fontsize{9}{9}\selectfont   
LTS (Linguistic Technology Systems) is founded by 
Amy Neustein, Ph.D., Series Editor of {\textbf{Speech Technology 
and Text Mining in Medicine and Health Care}} (de Gruyter); 
Editor of {\textbf{\makebox{Advances} in Ubiquitous Computing: 
Cyber-Physical Systems, Smart Cities, 
and Ecological \makebox{Monitoring}}} 
(Elsevier, 2020); 
co-author (with Nathaniel Christen) 
of {\textbf{Cross-Disciplinary Data Integration 
and Conceptual Space Models
for Covid-19}} 
(Elsevier, 2021); and co-editor of 
{\textbf{Medical Image Processing and Machine Learning}}
(Institution of Engineering and 
Technology, forthcoming).}}\end{center}

\vspace*{14pt}	

\textcolor{darkRed}{\textbf{Team}\vspace{3pt}
\hrule}
\begin{description}

\item[Principal Investigator:]  Dr. James A. Rodger,  
Professor of Management Information 
Systems and Decision Sciences at Indiana University of 
Pennsylvania

\item[Administrative Officer:]  Dr. Amy Neustein, 
founder of Linguistic Technology Systems (LTS)

\item[Contributors]  
\begin{itemize}\item[]

\item Nathaniel Christen, Lead Software Architect, LTS

\item Professor Amita Nandal, 
Department of Department of Computer and 
Communication Engineering at Manipal University, Jaipur

\item Professor Arvind Dhaka, 
Department of Department of Computer and 
Communication Engineering at Manipal University, Jaipur; 
recently visiting scholar at 
University of Varna, Bulgaria

\item Professor Todor Ganchev, Vice Rector of Research 
at University of Varna, Bulgaria

\end{itemize}

\end{description}

\vspace{-3pt}

\textcolor{darkRed}{\hrule}

\section{Executive Summary}
\p{LTS is building a pair of 
SARS-CoV-2/Covid-19 repositories 
to advance both Covid-19 research and new 
methodologies for data-integration and \AI{} research in general.  
The first of these repositories, the 
\q{Cross-Disciplinary Repository 
for Covid-19 Research} (\CRtwo{}), is \textit{empirically-focused} 
--- aggregating published data sets into a common research 
platform so as to promote Covid-19-related data mining.  
The second of these repositories, \q{\lAI{} Methodology and 
Conceptual Space Theory} (\AIMConc{}), is \textit{methodology-focused} 
--- providing a collection of code libraries implementing 
techniques applicable both to data integration and to Artificial Intelligence 
research.  \lAIMConc{} prioritizes analytic methods and 
data representations --- such as hypergraphs, fuzzy sets, and 
conceptual spaces --- which have broad applications in 
software engineering as well as \AI{}.  Both \CRtwo{} and 
\AIMConc{} are companion resources to the forthcoming Elsevier 
volume (authored by the LTS team) titled \textit{Cross-Disciplinary Data 
Integration and Conceptual Space Models
for Covid-19.}  In this volume, we will provide concrete 
examples of Covid-19 data integration/analytics by 
examining data sets included in the \CRtwo{} repository; 
we will likewise demonstrate analytic techniques by examining 
computer code included in the \AIMConc{} repository.}

\p{\AIMConc{}, with respect to methodology, will focus on 
Conceptual Space Theory, which is  
a valuable unifying framework connecting to both 
fuzzy sets as an \AI{} analytic strategy and to 
hypergraph models as a data representation strategy.  
In addition to \AIMConc{} providing \AI{} code libraries based 
on Conceptual Spaces, \CRtwo{} will likewise feature  
conceptual space models, applying this paradigm as a framework 
for describing research data.  To this end, 
we will be introducing an updated version of 
Conceptual Space Markup Language (\CSML{}) 
--- expanded in order to serve as a general-purpose 
dataset-description language.  \lCRtwo{} and \lAIMConc{} will be 
connected by a new hypergraph database protocol, which 
we are calling 
\q{Transparent Hypergraph Query Language} (\THQL{}).  
The \CRtwo{} archive will include an implementation 
of this protocol used to aggregate \CRtwo{} data into 
a common format.  Likewise, the methodology presented in 
the \AIMConc{} archive will be examined via demonstrations 
of how techniques and algorithms of the \AIMConc{} libraries 
can be applied to data hosted in a \THQL{} database.  
%The \THQL{} technology has many concrete applications 
%outside the context of Covid-19, penetrating vertical 
%markets such as pharmaceuticals, manufacturing, fintech, 
%healthcare, educational software, and bioinformatics.
}

\p{The principle goal shared by both \CRtwo{} and \AIMConc{} is 
to provide a \textit{common} programming infrastructure which facilitates 
the implementation of algorithms and \GUI{} components that 
synthesize data across different scientific fields and 
methodologies.  This common infrastructure is based primarily 
on \Qt{} --- a \Cpp{} application-development framework --- and 
secondarily on established scientific/medical code 
projects such as \CaPTk{} (the Cancer Imaging 
Phenomics Toolkit, from the Center for Biomedical Image 
Computing and Analytics), \BioCoder{},\footnote{See 
\bhref{https://jbioleng.biomedcentral.com/articles/10.1186/1754-1611-4-13}} and \ReproZip{} (a tool for packaging 
scientific software dependencies).  In addition to utilities 
and build tools, the programming core for \CRtwo{} and \AIMConc{} 
will include \Cpp{} libraries to read and manipulate 
data in formats commonly used for clinical, diagnostic, 
imaging, sociodemographic/sociogeographic, 
and medical-outcome reporting.  Moreover, LTS will 
provide a new library, called \q{\MOSAIC{},} 
to help build applications for visualizing and 
reusing data sets.}
    
\p{The \MOSAIC{} Data-Set Explorer (\MdsX{}) is a suite of 
code libraries which can be used to 
build native, desktop-style applications 
for viewing data sets.  An \MdsX{} 
\q{data-set application} is an application 
customized and tailored to a particular 
data set, or to a repository including multiple 
data sets.  Data-set applications can, if desired, be 
developed as \textit{notebooks}, 
with features inspired by \q{computational notebook} 
technologies such as \Jupyter{} and 
\Kaggle{}.  The \MOSAIC{} libraries 
include code for a \Qt{} Creator plugin 
which allows the \Qt{} Creator \IDE{} 
(Integrated Development Environment) 
to be used as a computational notebook.  In addition to 
serving as standalone applications in themselves, 
\MdsX{} notebooks can be embedded in other 
applications; for instance, in scientific software.  
Authors can therefore use \MdsX{} to create 
interactive presentations extending their 
publications: demonstrating research 
methods or explicating theoretical concepts.  
If desired, authors can supplement their 
work with formal specifications of research 
protocols or workflows, conforming to standards 
such as those defined by various 
\q{Minimum Information for Biological and 
Biomedical Investigations} (\MIBBI{}) recommendations.  
In addition to structured documentation of 
their work, these models can be used 
to integrate research applications into 
the host software where they may be embedded, 
insofar as descriptions of analytic steps 
that can be computationally reproduced 
also delineates the functionality 
that should be linked to host application 
\UI{} events as part of the embding 
process --- for instance, as in \CaPTk{}, 
by leveraging \Qt{} signal/slot reactive programming.  
The contours of 
the interface whereby a data-set application 
interacts with a host program thereby 
serves as one documentation of 
research/analytic workflows.}

\p{\lMdsX{} is paired with \MOSAIC{} \textit{portal}, 
a code library for hosting data sets and 
publications, and \MOSAIC{} \textit{plugins}, 
which allow \MOSAIC{} to be used in pre-existing 
applications.  The \MOSAIC{} portal code includes 
custom \LaTeX{} commands for building annotated, 
indexed \PDF{} files, and a custom \PDF{} viewer 
which can read \MOSAIC{} annotations and 
utilize their information to interoperate 
with data-set applications.  \lMOSAIC{} data-set 
applications can also customize this \PDF{} 
viewer to add functionality specific to 
its data models and scientific subject-matter.}

\p{In general, \MOSAIC{} applications are 
designed to be distributed in source-code fashion.  
They are, by default, written in \Cpp{} and based on 
the \Qt{} application-development framework.  
\lMOSAIC{} is structured so that sophisticated 
data-set applications can be built with few 
(or no) external dependencies apart from \Qt{} 
itself.  In the typical scenario, users would 
build and run \MdsX{} applications inside 
the \Qt{} Creator \IDE{}.  However, \MdsX{} applications can 
also be configured so that they (or some 
functionality they provide) can be run 
from a command line --- which allows them to 
participate in multi-application workflows --- 
or bundled as plugins or source-code extensions 
into larger software components.  
When embedded in host applications, 
the \q{\MOSAIC{} plugin framework} 
(\MPF{}) allows \MOSAIC{} plugins 
to send data back and forth to 
one another, thereby permitting 
their host applications to 
interoperate.}

\p{In addition to user-interface code, 
\MOSAIC{} notebooks will generally include 
code for reading data from a file (or 
potentially from a database or web resource).  
Each notebook may therefore depend on 
a code library managing specific file 
types and data formats.  Ideally, these 
libraries should be distributed in source-code 
fashion with the notebook code itself.  
To facilitate the construction of notebooks, 
\CRtwo{} will provide parsers for several 
file formats commonly used in biomedicine, 
such as \OMOP{} 
(from the Observational Medical Outcomes Partnership), 
\PCORnet{} (defined by the Patient-Centered Clinical 
Research Network), \FHIR{} (Fast Healthcare Interoperability 
Resources), \RadLex{} (Radiology Lexicon), \LOINC{} 
(Logical Observation Identifiers Names and Codes), 
\FCS{} (Flow Cytometry Standard), 
\PMML{} (Predictive Model Markup Language), 
\ARFF{} (Attribute-Relation File Format), 
and \HDFFive{} (Hierarchical Data Format version 5).}

\p{This paper is intended as an introduction 
both to the the \CRtwo{} and \AIMConc{} repositories 
and to new technologies, such as \MOSAIC{}, 
which we are developing alongside them.   
Part I of this paper will outline the repositories 
in greater detail; Parts II and III will focus on the 
new technologies; and Part IV will examine 
specific biomedical areas (such as diagnostic 
imaging and vaccine/immunology research) 
from the perspective of these technologies.}

\part{The Covid-19 Repository}
\section{Introduction}
\p{In an effort to accelerate scientific discovery and therapeutic 
intervention for Covid-19, LTS is engaged in the curation of 
two new repositories: the 
\q{Cross-Disciplinary Repository 
for Covid-19 Research} (\CRtwo{}); and the 
\q{\lAI{} Methodology and Conceptual Space Theory}
(\AIMConc{}).  The purpose of these repositories is to 
centralize \textit{disparate} Covid-19-related data and 
code into a common research platform.   
This code and data, as much as possible, will be marshaled into a common, 
hypergraph-based representation format, and a 
common \Cpp{}-based programming environment.  In 
building the \CRtwo{} repository, 
LTS will develop and demonstrate new database engineering 
and dataset/repository construction 
technologies.  These technologies have deep market 
penetration-potential in many areas, including, but not limited to, 
pharmaceuticals, manufacturing, fintech, 
healthcare, educational software, and bioinformatics.}

\p{Some of the \CRtwo{} code/data will be hosted on 
GitHub at \href{https://github.com/Mosaic-DigammaDB/CRCR}{Mosaic-DigammaDB/CRCR} 
(for data aggregation) and 
\href{https://github.com/Mosaic-DigammaDB/CRCR}{Mosaic-DigammaDB/LingTechSys} 
(for code previews).  
The latter repository includes preview code sampling 
database engine features, such as the logic for constructing a 
database in shared memory, encoding data types for persistence, 
and so forth.  
The data management tools developed 
for the \CRtwo{} repository have a broad range of use-cases, and 
can be customized for different projects.  
Companies or research groups interested in a more 
substantial code preview are invited to contact LTS
to discuss their projects and requirements in greater detail.}

\p{The main challenge when curating a 
data repository such as \CRtwo{} is reconciling 
heterogeneous data formats.  
In response to this challenge, LTS has 
focused on hypergraph-based data models 
which can unify many different information 
structures into one common structure.  
In particular, \CRtwo{} will introduce 
a special \q{Hypergraph Exchange Format} (\HGXF{}) 
which can take the place of disparate 
tabular or graph file formats 
(comma-separated values, numeric python, 
spreadsheets, graph-network serializations, 
etc.), so as to merge data sets into a common 
\textit{machine-readable} archive.  In addition, \CRtwo{} will 
introduce a new protocol for engineering 
hypergraph databases, called 
\q{Transparent Hypergraph Query Language}
(\THQL{}).  Databases conformant to the \THQL{} model will 
be able to export data in hypergraph-based 
formats, thereby generating data sets 
which can be used as published, citable 
Research Objects.  \lCRtwo{} will demonstrate 
\THQL{} via a new database engine called \q{DigammaDB} 
(or \DgDb{}) which 
serves as a \q{Reference Implementation} for 
\THQL{}.  In \CRtwo{}, \DgDb{} functions as 
a prototype and reference example for \THQL{}, used 
to curate data sets before their final 
form is exported into the main data repository.  
LTS can also customize commercial versions of a \THQL{} 
engine tailored to the requirements of 
individual projects.} 
    
\p{\lTHQL{} is designed with a priority on 
application development.  In particular, 
any instantiation of \THQL{} should 
provide data persistence capabilities 
through (as much as possible) self-contained 
code libraries that can be included in 
source-code form within an overall application.  
\lTHQL{} is designed to integrate seamlessly 
with native, desktop-style standalone 
applications.  In short, \THQL{} 
represents an unprecedented combination of 
native desktop-style software development 
and hypergraph database engineering.} 

\p{Complementing \THQL{}'s application-development 
focus, \CRtwo{} will also introduce \q{Dataset Creator,} 
(\dsC{}), a new tool for curating research data sets.  
The main feature of Dataset Creator is 
its use of native software components 
(called \q{Dataset Applications}), 
allowing researchers to view, manipulate, and 
reuse research data.  In sum, the typical 
Research Object built with \dsC{} 
will include self-contained source code 
implementing a customized desktop application 
providing access to the accompanying data set.  
In addition to \GUI{} code, each 
Dataset Application will supply \q{data-access} 
code libraries for 
parsing the raw data-set files, so as to obtain 
the information visualized within the 
\GUI{} classes of the Dataset Application.  
These data-access libraries offer machine-readable access 
to the raw data, permitting subsequent researchers to 
reuse the data-access software libraries so as to transform, 
filter, or analyze the published data in 
the context of replication studies and/or 
novel research projects.} 
  
\p{Development of \MOSAIC{}, \THQL{} and \dsC{} is 
concomitant with \CRtwo{}; the \CRtwo{} repository will 
provide a practical test-bed for validating 
this new technology.  Accordingly, the 
following sections will describe \CRtwo{} 
in greater detail; later  
sections will more information about 
\MOSAIC{}, \THQL{} and \dsC{}.}

\section{The Cross-Disciplinary Repository for Covid-19 Research}
\p{The sudden emergence of Covid-19 as a global crisis has 
cast a spotlight on computational and technological challenges 
which, in the absence of a catastrophic pandemic, would 
rarely rise to public attention.  In particular, an effective 
response to the dangers of \makebox{SARS-CoV-2} requires coordinated 
policy making integrating diverse modes of scientific inquiry.  
Genomic, biomolecular, epidemiological, socio-demographic, clinical, 
and radiological information are all pertinent to Covid-19.  
In this environment, it is important that the 
empirical foundations for expert recommendations --- which 
in turn drive public policies of enormous social and 
economic consequence --- be transparently documented 
and critically examined.  The proper synergy between government 
and science depends on data centralization: given 
the gaps in our current Covid-19 knowledge, it is 
understandable that different jurisdictions will craft responses to 
the pandemic in different ways.  There is no central authority 
with sufficient epistemic force to legitimize homogeneous 
mandates across the entire country.  However, such 
policy differences should be a consequence of alternative interpretations of 
scientific knowledge or the diverse needs of local communities  
--- rather than being a haphazard consequence of governments 
working with divergent, competing, and poorly integrated data.}

\p{The current administration, along with numerous corporate and academic 
entities, has clearly recognized the need for a more 
centralized paradigm for sharing Covid-19 data.  For example, 
the White House spearheaded a scientific initiative to 
develop \Cnineteen{}, an open-access corpus of over 46,000 
peer-reviewed publications related to Covid-19, which 
were transformed into a common machine-readable representation 
so as to promote text and data mining.  Similarly, 
large institutions such as Google, Johns Hopkins, and 
Springer Nature have all implemented some form of coronavirus 
data-sharing platform targeted to both scientists and 
policy makers.  However, these two aspects of the 
corporate/academic contributions to Covid-19 data sharing 
(exemplified by the \Cnineteen{} White House initiative and by 
institution-generated portals, respectively) 
have been incomplete, for opposite but complementary 
reasons.  Specifically, \lCnineteen{} is highly structured and tightly 
integrated, but it focuses primarily on text mining and 
scientific documents, not \textit{research} data.  
While it is possible to find data 
sets about Covid-19 through \Cnineteen{}, the 
techniques to do so are both cumbersome and non-scalable.  
On the other hand, projects such as the Johns Hopkins coronavirus 
\q{dashboard} provide accessible data sets, yet these 
projects are isolated and do not offer the level of 
structure and integration evinced by \Cnineteen{}.  In 
short, an optimal Covid-19 research platform 
would merge the structural text-mining rigor of 
\Cnineteen{} with the data-centric focus of  
isolated projects that share Covid-19 data 
with the scientific community, policy makers, 
and the general public.}

\p{The benefit of \CRtwo{} 
is that it can accelerate Covid-19 research by 
(1) pooling a diverse collection of data sets into a 
single resource which scientists can utilize; 
(2) serving as the prototype for larger research 
portals that can aggregate new Covid-19 data 
that will emerge from hospitals, labs, and 
academic institutions in the future; (3) formalizing 
a framework for aggregating patient narratives 
to accurately capture first-hand subjective symptomatology of 
the patient suffering from Covid-19; and 
(4) accelerating the implementation of novel 
data-integration and software-development 
technologies which can contribute to scientific 
progress \visavis{} Covid-19 
in particular, and biomedical/scientific computing 
methodology in general.  These principles, in turn, 
will shape the design of \CRtwo{}.  An ideal data-sharing 
ecosystem should merge data from multiple sources, but should do so 
in a fashion which yields a machine-readable totality, 
analogous to \Cnineteen{}'s structuration with respect to 
text mining.  The merit of \CRtwo{} therefore lies not 
only in the data which it will encompass but also in 
novel technology that it will concretize for constructing 
data repositories adhering to these goals 
--- aggregating data, but also instantiating novel 
data-integration and database engineering strategies.}

\p{Given these varying goals, \CRtwo{} can provide value at different 
scales of realization.  Relatively small data 
sets serve several scientific and computational 
purposes: (1) they can provide researchers 
with a mental picture of how data in different 
disciplines, projects, and experiments is structured; 
(2) they can serve as a prototype and testing 
kernel for technologies implemented to manipulate 
data in relevant formats and encodings; and 
(3) they can lay the foundation for data-integration 
strategies.  For example, when designing a 
representation format and/or implementing code 
to merge different data formats into a single 
structure (or meta-structure), it is useful 
to work with small, representative examples 
of the data structures involved, so as not 
to complicate the integration logic with 
computational details solely oriented to 
scaling up the data-management logistics.  
As a result, \CRtwo{} can provide a 
testbed for implementing data-integration 
technologies which can scale up as needed.  
To fulfill this mission, \CRtwo{} can aggregate 
relatively small data sets which have 
previously been published on academic and research 
portals, such as Springer Nature, Dryad, and DataVerse.  
At the same time, a more substantial 
(and not necessarily fully open-access) Covid-19 
data-set collection would also be beneficial to the 
scientific and policy-making community.  Ideally, then, 
\CRtwo{} will be paired with a larger technology which shares 
a similar implementational strategy but with different 
accession paradigms, allowing for an open-ended 
collection of Covid-19 data which users may 
selectively access (instead of a single package 
that users may acquire as an integrated resource).  
The common denominator in both cases 
(whether the focus is on relatively smaller or 
larger data sets) is the 
importance of deploying novel and contemporary 
data-integration techniques to centralize 
Covid-19 research as much as possible.  
Accordingly, this summary will briefly explain 
how \CRtwo{} can accelerate Covid-19 data integration 
on both a practical and technological level.}
 
\section{Methodology for Covid-19 Data Integration}
\p{As indicated above, pertinent Covid-19 data is drawn 
from multiple scientific disciplines.  On a technological level, 
Covid-19 data is documented via a wide array of file 
types and data formats.  This diversity presents technological 
challenges: if a Covid-19 information space encompasses 
files representing 25 different incompatible 
formats, users would need 25 different technologies 
to fully benefit from this data.  In many 
cases, however, data incompatibilities are 
merely superficial --- an important subset of 
Covid-19 data, for example, has a common 
tabular meta-model, even if the data is 
realized in discordant technologies (spreadsheets, 
relational databases, comma-separated-value or 
Numeric Python files, and so forth).  Applying \CRtwo{}'s technology, 
one level of data integration can thus 
be achieved simply by encoding tabular 
structure into a common representation: any field in a table 
can be accessed via a record number and a column name and/or 
index.  In some cases, more rigorous integration is also 
possible --- for example, by identifying situations where 
columns in one table correspond semantically or conceptually 
to those in another table.  In either case, 
it is reasonable to assume that a single abstract data 
format lies behind surface data-expression in patterns 
such as spreadsheets and comma-separated values 
(\CSV{}), so that all files in an 
archive encoding spreadsheet-like data can be 
migrated to a common model.}

\p{Other forms of clinical and epidemiological inputs are often 
more amenable to graph-like representations.  For instance, 
trajectories of viral transmission through 
person-to-person contact is obviously an instance 
of social network analysis.  Similarly, models of 
clinical treatments and outcomes can take graph-like 
form insofar as there are causal or institutional 
relations between discrete medical events: 
a certain clinical observation \textit{causes} a 
care team to request a laboratory analysis, 
which \textit{yields} results that \textit{factor} 
into the team's decision to \textit{administer} some 
treatment (e.g., a drug \textit{from} a particular 
provider \textit{with} a specific chemical structure), which 
observationally \textit{results} in the patient improving 
and eventually \textit{being} discharged.  In short, 
patient-care information often takes the form 
--- at least conceptually --- of a network comprised 
of different \q{events,} each event involving some 
observation, action, intervention, or decision made 
by care providers, and where the important data 
lies in how the events are interconnected: both their 
logical relationships (e.g., cause/effect) and their 
temporal dynamics (how long before a drug leads to a 
patient's improvement; how much time elapses between admission to 
a hospital and discharge).  These graph-like representations 
are a natural formalization of \q{patient-centered} data 
models.}

\p{Using \CRtwo{}'s associated software 
(for example, importing Covid-19 data sets into a 
\THQL{} database), a higher level of 
data integration can then be 
achieved by merging tabular and graph-like models into a 
single \textit{hypergraph} format.  A 
significant subset of Covid-19 data (or, more generally, 
any clinical/biomedical information) 
conforms to either tabular or graph structures; 
thus it is feasible to unify all of this information 
into a common framework.  A graph-plus-table 
architecture is generally considered some form of 
Hypergraph model, and indeed \CRtwo{} adopts a hypergraph 
paradigm to merge many different sorts of information into a 
common structure.  In particular, \CRtwo{} introduces 
a new \q{Hypergraph Exchange Format} (\HGXF{}) which 
can provide a text encoding of many files that, 
when originally published, embodied a diverse 
array of file-types requiring a corresponding 
array of different technologies.  \lCRtwo{} 
will include specialized computer code that 
would enable machine-readability of the \HGXF{} files, 
and use them to create hypergraph-database instances.  
In short, \CRtwo{} will promote Covid-19 data integration 
by translating a wide range of files into a common 
\HGXF{} format.\footnote{\CRtwo{} data sets 
are not required to compile all files 
to a hypergraph format; in particular, 
sciences requiring substantial quantitative 
analysis --- e.g., biomechanics or genomics --- 
express data via encodings optimized for relevant 
mathematical operations, and have parser libraries 
optimized for these specific formats.  For 
these files \sCRtwo{} will generally provide an 
\sHGXF{} encoding supplying data 
\textit{about} the original file, with information 
concerning the file type, preferred software components 
for viewing/manipulating its data, etc.,  
so that the contents of non-\sHGXF{} files 
can be indirectly included into the \sCRtwo{} 
hypergraph-based ecosystem.}}

\section{Hypergraph Data Models and Multi-Application Networks}
\p{As has been outlined thus far, via the \CRtwo{} 
technology most Covid-19 data can be wholly or partially integrated 
into a single hypergraph framework, which accordingly simplifies 
the process of designing software applications and 
algorithms to analyze and manipulate this data.  
Specifically, software components can employ 
a single code library to obtain, read, consume, 
and store data, rather than needing to re-implement 
this logic for a large number of different file formats 
and/or database models.}

\p{Quality software (especially in the clinical and 
biomedical context) demands a balance between 
applications which are either too broad or too narrow 
in scope.  On the one hand, doctors often complain 
that homogeneous Electronic Health Record systems (where 
every digital record or observation is managed by a single 
all-encompassing application) are unwieldy and hard to work 
with.  This is understandable, because the clinical tasks 
of health care workers with different specializations can be very 
different.  On the other hand, doctors also complain about 
software and information systems which are so balkanized 
that they must repeatedly switch between different, non-interoperable 
applications.  In short, clinical, diagnostic, and research software 
should be neither too homogeneous nor too isolated; finding the 
proper balance between these extremes is, no doubt, 
a major challenge to the usability of electronic health 
systems going forward.}

\p{Against this background \CRtwo{} demonstrates novel 
solutions to this problem: it focuses on the dimensions 
of data acquisition and management that are specific 
to individual scientific or medical specializations, 
while also identifying requirements that are 
consistent across domains.  Scientific software 
generally needs to hone in 
on the data visualization and analytic 
requirements of particular disciplines; for example, 
biochemists use different programs than 
astrophysicists.  However, much 
of the code underlying scientific applications 
has nothing to do with these high-level 
models or theories, but is simply a 
fulfillment of basic data-management 
functionality --- data storage, accession, provenance, 
searching, user validation, and so forth.  
In effect, the computational requirements 
of scientific and biomedical software can 
be partitioned into two classes: (1) 
domain-specific logic which reflects the 
quantitative or theoretical models of 
narrow scientific fields; and (2) 
data-management logistics which can be 
realized within a central access hub, rather 
than being re-implemented by each application 
in isolation.}

\p{In short, \CRtwo{} architecture  
conceives of a central hub responsible for storing 
data and serving as a common access point 
--- providing the \q{gateway} where authorized 
users can gain access to heterogeneous information 
spaces utilized by an array of domain-specific 
software applications.  Since peer applications  
would not be directly responsible for data persistence 
or user identity management, they can focus 
on their specific data analysis and visualization 
capabilities.  The central hub, serving multiple 
peer applications, is then a heterogeneous data space 
managing information from multiple applications while 
also tracking information about the applications 
themselves: helping users to identify and launch the 
software which is most directly relevant to their 
clinical or research needs at the moment.  Meanwhile, 
because peer applications are jointly connected to a 
central hub, it is possible to implement scientific 
workflows where one application may send and receive 
data from its peers, allowing applications to complement 
each others' capabilities.}

\p{This multi-application networking architecture 
has precedents in some of the current database and 
engineering technologies.  For example, many hospitals and 
medical institutions employ some version of a 
\q{Data Lake,} pooling disparate data sources into 
a heterogeneous aggregate which is then accessed 
by multiple client applications.  Similarly, Machine Learning 
and Artificial Intelligence often adopts \q{software agents} 
or analytic modules in contexts such as Online 
Analytic Processing, which again represent semi-autonomous 
software components sharing an originary 
data hub.  Web applications, too, often act as domain-specific 
subsidiaries deferring operational requirements, such 
as user authentication or transaction processing, to a 
central web service.  The limitation of multi-application 
networks in these existing contexts are that the 
software agents involved are generally \q{lightweight,} 
with relatively primitive user-interface design.  
By contrast, the hypergraph technology introduced with \CRtwo{} 
will support multi-application networking in the context of more 
substantial desktop-style scientific applications.  In sum, 
the novel hypergraph technology developed by LTS offers a 
hybrid of the development methodologies 
employed for desktop scientific software and those 
applicable to multi-agent heterogeneous data stores, like 
a Semantic Data Lake.  To accomplish these goals, 
\CRtwo{} will utilize 
a new hypergraph database engine, coded in the \Cpp{} 
programming language, which has a unique focus on 
supporting native \GUI{} applications from the ground 
up, including persisting application state and 
storing application documentation within 
the database itself.}

\section{A New Paradigm for Data Sharing and Data Transparency}
\p{One exceptional feature of Covid-19 research is the 
extent of public attention focused on scientific discoveries 
about the disease.  Academic and commercial research teams 
find themselves in an unprecedented situation where 
there is unusual pressure to accelerate the 
Research and Development process, and a concomitant demand for 
a novel level of transparency and openness.  For example, 
vaccine development protocols are being fast-forwarded 
to take months instead of years, and information about 
the development process (such as trial results and 
scheduling) will likely be shared with the public much 
more than is standard practice.  This new reality, 
in turn, calls for a commensurate evolution in 
the technology for public data-sharing.}

\p{In conventional biomedical R\&D, much of the research 
data is proprietary, and revealed only in restricted 
contexts to select parties (such as the Food and Drug 
Administration).  Data which \textit{is} then publicly shared 
tends to be tied to published research papers 
in peer-reviewed literature, primarily read 
by a relatively small, specialist audience.  All of this is 
changing with SARS-CoV-2: companies pursuing 
Covid-19 R\&D (in the context of vaccine trials, for example) 
are facing pressure to publicly share their results 
as soon, and as transparently, as possible; and 
policy makers, scientists, and journalists are no 
less looking for quick access to research data directly, 
rather than circuitously through academic publications.}

\p{\lCRtwo{} will introduce the new Dataset Creator technology 
targeted toward this new environment of direct, transparent 
data-access (\dsC{} will be discussed in greater detail below).  
Data sets created 
via this technology therefore implement the \q{Research Object Protocol,} 
which mandates that research data be bundled with code allowing 
scientists to analyze and manipulate the information in the 
corresponding data set.  The Research Object framework was 
designed by a consortium of academic and governmental 
entities, such as the National Institutes of Health, to 
promote a paradigm for data publishing which prioritizes 
multi-faceted research tools over \q{raw} data that 
can be difficult to reuse in the absence of supporting code.  
In particular, Research Objects 
should be (as much as possible) \textit{self-contained,} 
which means that scientists do not need external 
software dependencies to access and study the data --- any special 
code which is a prerequisite to using this data should 
be included, alongside the raw data, as part of the 
Research Object itself.}

\p{Dataset Creator enables 
standalone, self-contained, and full-featured 
native/desktop applications to be uniquely implemented 
for each data set, distributed in source-code fashion 
along with raw research data (\dsC{} Dataset Applications 
use \Qt{} by default to provide native \GUI{} classes, 
tailored to the relevant Research Object).  Adopting such a 
data-curation method makes data sets easier to 
use across a wide range of scientific disciplines, 
because the data sets are freed from having to rely on domain-specific 
software (software which may be commonly used in one scientific 
field but is unfamiliar outside that field).  
In addition, Research Objects composed with \dsC{} can 
be integrated into Multi-Application Networks  
because 
the dataset applications are autonomous native \GUI{} applications that 
can easily interoperate via \Qt{} messaging protocols.}

\p{Of course, most of the \CRtwo{} data sets are 
previously-published work composed via older technology. 
Many of these resources, created with a wide range of software 
products, predate (or fail to apply) contemporary specifications 
such as the Research Object Protocol; not 
every \CRtwo{} data set will have the full set of 
features described in this section.  However, 
\CRtwo{} will try to maximize the value of each data set 
by translating them into a \Qt{}-based format --- in 
particular, \CRtwo{} will provide \Qt{} code for 
reading \HGXF{} files, as well as a \Qt{}-based hypergraph 
representation library.  Following the 
data integration methods outlined earlier, much of the 
\CRtwo{} data can be merged into a \Qt{}-based framework, 
which can facilitate the implementation of 
new, more sophisticated Dataset Applications 
as the information in \CRtwo{} gets reused 
for subsequent research.  \lCRtwo{} will also include 
\Qt{}-based software, such as a customized \PDF{} viewer, which 
will help researchers utilize the corpus in its 
entirety.  For example, \CRtwo{}'s \PDF{} viewer 
will include special code to connect \PDF{} files 
with data sets via \q{micro-citations,} as discussed in the 
next section.}

\section{Supporting Data Micro-Citations to Improve Machine Readability}
\p{The \CRtwo{} database engine supports annotating individual components 
of a database --- a technology sometimes referred to as 
\q{micro-citation.}  Data micro-citations are references to 
integral parts of a data set, such as an individual table, 
or a single row/record or column in a table.  Micro-citations allow 
these integral parts within the data set to be cited by and linked 
to publications, for purposes of machine readability and 
attribution.  As an example, preliminary vaccine trials often 
target a patient cohort selected for demographic or medical 
criteria matching the population who would most benefit from 
the vaccine.  These criteria for selecting the cohort for 
the vaccine study are usually described in the texts of the 
articles.  However, these criteria are also identified 
within the data set by socio-demographic 
data which is part of the information generated 
by the trial. \hspace{-6pt} By making these connections between criteria 
discussed in the article and those 
represented in the corresponding data set explicit, 
text and data mining can be \textit{merged} as analytic 
tools targeting a data repository, so that 
%\lCRtwo{} is therefore 
%designed to optimize the convenience and rigor of micro-citations 
%both within data sets and within research papers.  
machine reading is able to mine not just article text but the 
corresponding data.}  

%, accelerating the 
%extraction of current scientific information that is 
%of use to scientists and policy makers.}

%\vspace{1em}
\p{One reason why micro-citations are important is that they 
clarify the scientific meaning attributed to data set  
elements by connecting these elements to scientific concepts and 
\q{controlled vocabularies} (such as a list of drug names, 
diseases, proteins, etc.).  
For instance, micro-citations allow table columns to be 
mapped to statistical parameters, enabling their 
empirical properties (such as min/max values and distribution) 
to be queried by text and data mining software.  Likewise, 
\CRtwo{} enables dimensional 
and measurement annotations to describe the empirical and experimental 
significance of the measured or calculated quantities which 
are stored in a database.  Such quantity dimensions model the 
conceptual roles which particular parameters perform: 
e.g., the axiation \q{mJ/cm$^2$}(millijoule per 
square centimeter) indicates the intensity of 
ultraviolet light --- any table (or other 
data aggregate) having a column or field with this dimension 
is intrinsically associated with observations or experiments 
pertaining to \UV{} light.  Consequently, to 
locate data sets relevant for research about the clinical uses of 
antiviral \UV{} radiation, 
one method is to search for data fields dimensionalized 
in terms of joule or millijoule per square centimeter.  
As this example illustrates, data micro-citation 
--- via annotations on data fields, statistical parameters, 
and table columns --- is an important data-mining tool.  
In short, constructing micro-citations within a database 
serves two distinct benefits: (1) to aid data mining; and 
(2) to enable granular links (joining specific 
parts of articles to corresponding parts of the data set in 
the data set repository --- analogous to hyperlinks 
between web pages) to be established between 
publications and data sets, making it \textit{easier} for 
researchers to find the specific information most 
relevant to their own research.}

\section{Code Libraries and Data Sets for Analytic Methodology}
\p{As a companion to \CRtwo{}, whose essential purpose 
is to present \textit{empirical} observations concerning Covid-19, 
we are curating the \AIMConc{} archive, which is focused 
on code that supports Artificial Intelligence and 
concrete data-integration methodology.  At the 
core of \AIMConc{} is a collection of code libraries 
implementing analytic techniques and representations 
used by \AI{} researchers as well as by software 
engineers --- in particular, different varieties of hypergraphs; 
fuzzy sets/logic; and conceptual spaces.  In order to integrate this code 
into a common programming framework, the \AIMConc{} libraries are 
ported or modified when necessary, with \C{} or \Cpp{} 
as the primary development language and \Qt{}/qmake as 
the primary development framework and build system.
\lCRtwo{} and \AIMConc{} will be interconnected 
by employing \CRtwo{} data sets as concrete 
examples for demonstrating how \AIMConc{} libraries 
may be used within software applications.   
Here are some of the methodological approaches 
(based in part on how Covid-19 research can provide 
concrete use-cases) which will be important 
components of \AIMConc{}:

\begin{description}
\item[Computational Epidemiology]  Methods in this 
category generally concern the simulation of disease 
transmission given a set of initial parameters 
(such as an infectious agent's reproduction rate 
and an average degree of person-to-person contact), 
often concomitant with empirical data, tracking how a disease 
has spread within some observable subpopulation.  
The empirical findings, therefore, suggest 
\textit{a posteriori} which statistical model best 
fits the actual nature of the disease in question.  
The accuracy of this analysis depends, in part, on 
how well the observed subpopulation mirrors the 
susceptible population as a whole; but it also 
depends on the accuracy of the mathematical formulae 
translating initial parameters into projected 
epidemiological simulations to be compared against 
\textit{a posteriori} data.  As such, concrete expositions 
of these mathematical frameworks --- in particular, 
computer code implementing the calculations 
which drive a simulated model --- 
constitute a computational asset in their own 
right.  \lAIMConc{} will include several  
code libraries that have been published 
as tools investigating different quantitative 
epidemiological models.  Although some epidemiological simulations 
rely primarily on mathematical equations, most 
epidemiology libraries internally use graph-based models, 
often employing weighted graphs where edges denote, 
for example, the probability of viral transmission 
between people in close contact.  As a result, 
distinct epidemiological models can sometimes 
be merged into a common analytic framework, 
using custom quantitative algorithms that are composed 
within a common graph-traversal framework.   

\item[Event Modeling]  Event modeling, sometimes called 
\q{Entity-Event Modeling,} is an emerging data-analytic 
trend which focuses on events, rather than objects, 
as the most fundamental form of observation within a data 
space.  Conceptually, the rationale for this paradigm 
is that every property which is attributed to 
some object can be tied to a specific event wherein the given property 
was measured, observed, discovered, or inferred.  
For instance, asserting that a patient is 
\textit{infected} with SARS-Cov-2  
implies that a \textit{test} for SARS-Cov-2 
was positive.  Focusing attention on the \textit{event} 
(viz., the test and its result) allows the 
data model to accrue information in a more detailed manner: 
in the case of SARS-Cov-2 tests, the relevant testing method/kit used; false 
positive/negative rates in the population; the time ellapsed between the 
onset of symptoms (if any) and the test being ordered 
and the interval until the results are provided; the observations 
or factors (such as the subject's exposure 
to an infected family member) which 
caused health care providers to order the test; and so on.  
Many of these details might be included in a conventional 
database as well; however, focusing on events allows the 
relevant information to be obtained more easily in that 
the event model supplies a temporal and operational 
organization for the underlying information.  
Event models are especially useful when 
temporal sequencing and duration are important 
considerations for uncovering scientific 
facts about the phenomenon being studied.  In the 
context of infectious diseases, intervals such 
as the length of time between exposure 
and contagiousness, length of time between exposure 
and the appearance of symptoms, or the duration of hospital 
stays, are essential to our understanding of the disease's 
biology. 

\item[Fuzzy Sets and Fuzzy Logic]  Mathematically, 
fuzzy logic is often conceived in terms of replacing a 
simple binary logic (\q{true}/\q{false}) with a 
more complex multi-valued logic, wherein properties 
may be true or false to varying degrees.  Thus, on the 
surface, this introduces quantitative models 
(such as the calculation of the conjunction or disjunction 
of fuzzy predicates) as a substitute for purely 
logical reasoning.  In practice, however, fuzzy 
sets often have the opposite effect: this theory 
gives rise to methods which can simplify empirical 
analyses by \textit{eliminating} quantitative 
formulae --- in particular, parameters of a continuous 
variable.  Fuzzy set models tend to simplify data 
spaces by collapsing continuous quantities into discrete 
cases (e.g., \textit{low}, \textit{medium}, \textit{high}), 
or grouping objects into similarity clusters.  
Via these operations, data models that depend on 
computationally intensive mathematical variables 
can be replaced by qualitative models, often 
representable in graph form (where graph edges 
may designate membership in a prototype-class, 
or the evolution of some observable according to 
several property classes, each of which 
collapses a spectrum of granular cases into a single 
prototype).  Fuzzy methods can be shown to be 
effective simplifications of complex data models 
by demonstrating that analyses conducted via 
qualitative reconstructions of a data space, 
for a given set a observations, 
are comparable to results obtained from more 
quantitative methods and/or are a good fit to 
empirical data.  Research data sets which emerge 
from fuzzy methods can be qualitative models 
derived from quantitative data spaces, as 
well as code libraries which perform the relevant 
transformations.

\item[Conceptual Space Theory]  Conceptual spaces, 
which have some similarities to Fuzzy Sets, 
have likewise been proposed as a paradigm 
for modeling both scientific knowledge and Artificial 
Intelligence.  Conceptual Space Theory is 
rooted in cognitive and linguistic investigations 
of how humans formulate and understand concepts 
(extending to scientific theories, and in particular 
how we \q{process} scientific information to infer 
facts about the world).  Conceptual spaces have, therefore, 
been proposed as a language for representations or descriptions 
of scientific knowledge, with an emphasis on how 
scientific models are built up from individual 
conceptual parameters (such as points in space/time 
or notions of length, heat, speed/acceleration, electric 
charge, etc.).  As scientific ideas become formalized, 
our intuitive conceptualization of spatial or observational 
quantities gets translated into physical, mathematical, or 
statistical details: scales and units of measurement, 
observational value ranges, statistical levels 
(Nominal, Ordinal, Interval, Ratio), and so forth.  
Formal implementations of conceptual spaces, therefore, 
focus attention on the dimensional and measurement 
properties of data parameters (e.g. scales/units of measurement) 
--- information which is usually not made explicit in the 
publishing of data sets --- and on how these parameters aggregate into 
conceptual units.  Such aggregates include things like how two 
geographical coordinates that define a 
geospatial location, in Geographical Information Systems; 
or how shape and color, in combination, characterize 
visible objects, such as in image segmentation.  One conceptual-space represenation 
is Conceptual Space Markup Language (\CSML{}), which 
\CRtwo{} will update in order to document scientific parameters 
within data sets.  Other conceptual-space models and 
analytic libraries have been developed in the context 
of Artificial Intelligence, where the goal is to 
simulate the patterns of human conceptualization within 
\AI{} engines; this work can then be incorporated into 
the \AIMConc{} repository.
   
\item[Cognitive Discourse Analysis and 
Conceptual Role Semantics]  Cognitive discourse 
theory is similar to Conceptual Space Theory in 
investigating the overlap between conceptualization 
and scientific knowledge --- or, more generally, 
all of our observed facts or empirical beliefs 
as they are encoded in language.  Although it is 
obvious that most of our language is based in 
concrete beliefs about the world around us, this 
basic linguistic principle is not usually 
captured with full rigor within formal reconstructions 
of linguistic expressions.  The essential paradigm 
in cognitive linguistics is the notion of 
cognitive \textit{grounding} --- that is, how every 
object and event included in a sentence 
connects to the speaker's fundamental understanding 
of the situation around them, and its relevant 
facts and observables.  While all linguistic theories 
acknowledge grounding, cognitive analysis 
develops a detailed theory of how grounding works in 
all of its aspects: there are multiple dimensions of 
grounding, because we connect objects/events 
to situations in many different ways.  For example, 
objects relevant to a given event play distinct 
empirical roles (the \textit{agent}, which 
\textit{causes} something --- some change --- to happen; 
the \textit{patient}, which is thereby changed/affected; 
and potentially secondary participants such as the 
instrument by which the change is effected; the 
\q{benefactor,} i.e., the object for whose benefit 
the change is caused, etc.).  These conceptual differences 
are rigorously treated within Conceptual Role Semantics, 
which shares a similar philosophical orientation to 
Cognitive Discourse Analysis.  As a practical tool 
for analyzing language, these two methodologies 
provide a matrix of classifications for objects 
and events (and their corresponding linguistic 
units) in terms of the situational background 
where every object and event (in a given discursive 
context) is perceived.  Formally, these classifications 
then provide a layer of annotation which capture 
linguistic details at a cognitive level more rigorous 
than usually found in Natural Language Processing.  
\lAI{} methods in this field focus on automatically 
identifying cognitive-discursive patterns in language, 
although data sets can also be formed by manually 
annotating language samples according to 
cognitive-discourse and conceptual-role vocabularies.  
In either case (whether via manual or \AI{}-driven 
annotations), Cognitive Discourse Analysis represents 
one emerging technique for the representation 
of natural-language assets --- such as Patient Narratives 
--- for the purposes of applying advanced text/data mining 
strategies (in the case of Patient Narratives, 
to extract critical patient symptomology often 
buried in circumlocutory discourse). 
\end{description} 
}


\section{Adding Patient Narratives to Covid-19 Data}
\p{In addition to aggregating published data sets, \CRtwo{} 
may be used as a repository for collecting new Covid-19 
information.  With that in mind, we are prioritizing the 
design of a standard for storing and accessing 
natural-language text representing patients' subjective 
symptom descriptions, which is quite useful for 
diagnostic/prognostic assessments of patients 
infected by Covid-19.}

\p{Just as \CRtwo{} envisions a curation of published 
data sets for data mining to improve machine-readability 
of Covid-19 research, LTS also sees the 
benefit of a repository of patient narratives prepared 
for text mining, to improve machine readability of the 
open-ended symptom descriptions offered by patients.  
While \CRtwo{} does not need to specify how these 
narratives should be collected, it will implement 
a common representational format 
so that patient narratives can be pooled, similar to  
to how \Cnineteen{} research texts are merged and 
encoded with a system that permits annotation.}

\p{In modeling patient narratives, this technology 
will be oriented toward the scientific-computing 
ecosystem outlined in the previous section.  
In particular, we assume that \GUI{}-based desktop 
applications will be the primary instruments for 
data collection and analysis; this means that 
the encoding of patient narratives may, at times, 
need to be paired with \GUI{} or multi-media content.  For example, 
the software for patients to submit 
medical history information could also allow 
them to pair (text-form) narratives with 
graphics indicating the location of their pain or 
discomfort.  Furthermore, the software could 
allow narratives to be accompanied 
by an audio file where patients could cough/speak into 
a microphone.  Given this 
range of possible inputs, patient-narrative 
encodings must be flexible enough to 
include diverse multi-media content.}

\p{As described earlier, an information space 
adapted for multiple peer applications should encompass 
capabilities for saving application state 
(the current visual appearance of the program), which 
includes features for modeling instances of 
\GUI{} classes.  This technology provides the 
necessary infrastructure for managing patient 
narratives.  For example, consider a multi-media 
intake form where patients may describe symptoms by 
placing icons (representing pain or discomfort) 
against anatomic silhouettes (head/body, back/front, 
extremities, and so forth).  
As patients use such a multi-media form, \GUI{} application 
state corresponds to the patient's subjective symptomology; 
in this way the graphics-based representation of symptoms 
could then be incorporated into the overall patient 
narrative.  This is an example of how 
application-persistence logic can be marshaled to 
the related project of curating patient narratives.}

%\pagestyle{empty}

%\p{Further documentation of text-encoding methodology applicable 
%to both patient narratives and publications associated 
%with \CRtwo{} research data is available on the \CRtwo{} 
%web site, such as \href{https://raw.githubusercontent.com/Mosaic-DigammaDB/CRCR/master/%cr2.pdf}{here} (this is a downloadable \PDF{} link; 
%visit the repository to see the larger archive structure).}

\part{Computational Notebooks and the MOSAIC Data-Set Explorer}
\section{Data Sets and Data Publishing: bridging the gap}

\p{\lMOSAIC{} is designed to bridge the 
gap between scientific software and 
scientific data sets.  While increasing 
volumes of open-access research data is 
becoming available to readers, researchers, 
and scientists, this data is not always 
published in a manner which facilitates 
reuse and interoperability with scientific 
software --- the kinds of applications 
that scientists themselves use to conduct 
and examine experiments or simulations.  
Moreover, the software-development ecosystem 
which is evolving around data publishing 
(as far as exchange protocols, file formats, 
development tools, and so forth) is  
methodologically removed from the engineering 
norms and principles of most scientific 
software.  As such, a technical gap 
exists between the data-publishing and 
scientific-computing ecosystems seen 
as software-engineering domains.  
\lMOSAIC{} is a suite of tools 
which can help bridge that gap.}

\p{Recent years have seen an increasing emphasis 
(in the academic and scientific worlds) on 
\textit{data publishing}: sharing research 
data and experimental results/protocols via 
web portals complementing those that host 
scientific papers.  Published data sets 
now take a position alongside books and articles 
as primary publicly-accessible outputs of 
scientific projects.  Coinciding with this 
increased volume of raw data, there has 
also emerged an ecosystem of tools allowing 
researchers to find, view, explore, and 
reuse data sets.  These tools enhance the 
value of published data, because they decrease 
the amount of effort which scientists need 
to make use of data sets in productive ways.}

\p{Unfortunately, however, this ecosystem 
of tools does not include extensive work 
on software \textit{applications} for 
accessing and using published data sets.  
Prominent publishes (Elsevier, Springer, Wiley, 
de Gruyter, etc.) have all developed 
suites of components for manipulating data 
sets and data/code repositories, including 
\API{}s, search portals, Semantic Web ontologies
(and other forms of Controlled Vocabularies) and 
cloud-based computing or visualization engines, 
founded on technologies such as \Jupyter{}, \Docker{}, 
and \WebGL{}.  However, none of these 
publishers actually provide \textit{applications} 
for accessing data sets outside of the 
online resources where data sets are indexed.  
While these online portals can provide a 
basic overview of the data sets, publishers 
do not provide tools to help researchers 
rigorously use any data sets once they 
are downloaded.  Moreover, the ecosystem 
for manipulating published research is largely 
disconnected from the software  
applications which scientists actually 
use to do research.  The ability to work 
with data-publishing tools has not been 
implemented within most 
scientific-computing environments.}

\p{These lacunae may be explained in part by publishers' 
and scientists' hopes of creating cloud-hosted 
environments that can themselves serve as fully featured 
scientific-computing frameworks, with the ability 
to run code, evaluate queries, interactively 
display \TwoD{} and \ThreeD{} graphics, and 
maintain user and session state so that 
researchers can suspend and resume their work 
at different times.  In these cloud environments, 
users may run computations and generate complex 
graphics on remote processing units, with relatively 
little data or code-execution stored or performed 
on their own computers.  Such employment of remote, 
virtual programming environments is sometimes necessary 
when interacting with extremely large data repositories; 
and can be a convenient way to explore data sets 
in general, especially if a user is unsure whether 
or not a given data set is in fact germane to 
their research.  Investigating data via cloud 
services spares the researcher from having to 
download the data set directly (along with 
the additional software and requirements which 
are often needed to make downloaded data 
functionally accessible).}

\p{However, cloud-based data access is 
limited in important ways, which makes relying 
solely on cloud services to provide the 
filaments of a research-data ecosystem a dubious 
idea.  One problem is that cloud services 
are, despite their technical features, essentially 
just web applications under the hood; as such, 
they are susceptible to the same User Experience 
degradation as any other web service --- subpar 
performance due to network latency, poor connectivity, 
and the simple fact that web-based graphics can 
never be as responsive or as compelling as 
desktop software, which can interact directly 
with the local operating system and react 
instantaneously to user actions.  A second, 
more serious problem is that could-computing 
environments are computationally and 
architecturally different than the native-application 
contexts where scientific software usually 
operates.  Insofar as researchers develop 
new analytic techniques, implement new algorithms, 
or write custom code to process the data 
generated by a new experiment, these 
computational resources are usually 
formulated in a local-processing environment that 
cannot be translated, without extra 
effort, to the cloud.}

\p{To be sure, scientists can sometimes 
\q{package} their experimental and analytic 
methods into a coherent framework, such as a 
\Jupyter{} notebook, which serves as both a 
demonstration and a precis of their 
research work.  Indeed, tools such 
as \Jupyter{} (which packages code, data, 
and graphics into a self-contained \Python{}-based 
programming environment) are useful in part 
because the content shared via these 
systems (e.g. \Jupyter{} \q{notebooks}) needs 
to be deliberately curated; building a 
notebook is a kind of summarial follow-up to 
actual research work.  The intellectual discipline 
involved in packaging up one's research via 
such tools may be a valuable stage in the 
scientific process, but even then the programming 
environment where research code and data 
is publicly shared is fundamentally different 
than the environment where the research is 
actually carried out.  As a consequence, 
sharing research indirectly via cloud 
services or \q{notebook}-oriented 
frameworks like \Jupyter{} is not truly 
conducive to either reuse or replication.  
To actually replicate a course of 
investigation, it is more thorough 
to employ the same (or at least 
functionally equivalent) software for 
data acquisition, analysis, and validation 
as the original software; and to incorporate 
published data in new projects, the data 
should be shared in such a way that 
the original research data, code, and 
protocols can be absorbed into a new 
research context, including the software 
used by the research team.}  

\p{Cloud-based 
services, which provide only an overview 
of research data, with limited analytic 
and imaging/visualization functionality 
compared to actual scientific software, 
do not substantially promote data 
replication and reuse insofar as 
these cloud services are functionally 
disconnected from scientific applications 
themselves.}

\p{This is the motivation behind \MdsX{}, which 
is used to implement \textit{native,} 
\textit{desktop-style} applications 
for accessing research data of different 
kinds.  Within the overall space of 
published data sets we can find specific 
variations, such as \textit{data repositories} 
comprising multiple data sets; \textit{image corpora} 
designed as test beds for Machine Vision and 
diagnostic-imaging methods; \textit{simulations} 
which involve not only raw data but digital 
experiments that can be re-run as a way 
to access the data; and so forth.  Each of 
these various kinds of data sets present 
different sorts of interactive specifications 
which must be implemented by the data-set explorer 
software.}

\p{While executed as a native application 
--- not a cloud service --- \MdsX{} nevertheless incorporates 
the important ideas from contemporary data publishing 
(including ideas originating in the cloud context): 
workflow models, notebooks, access to publishers' 
\API{}s, etc.  In short, \MOSAIC{} can be seen as akin to 
a cloud-based data-publishing platform 
where the \q{cloud} is replaced by a 
scientific-computing application.  Instead of 
being hosted remotely (\q{on the cloud}), 
\MdsX{} components are hosted within a local desktop application.  
This host application may be pre-existing 
program, or a custom host implemented 
to allow \MdsX{} data-sets to be explored in standalone 
fashion.}

\p{The overall \MOSAIC{} framework as such  
spans three different technical areas: portals, 
plugins, and applications.  
\MOSAIC{} Portal comprises server-side 
logic for hosting data sets, publications, 
and cloud services utilized by 
native \MOSAIC{} applications.  
\lMOSAIC{} Plugins are extensions 
to larger applications allowing 
data sharing between plugins and 
\MOSAIC{} Portal services 
(and between plugins themselves, 
embedded in distinct applications).  
Finally, \MdsX{} applications and 
notebooks can be either standalone 
or embedded; in the latter case 
an \MdsX{} plugin can also serve as an 
\MdsX{} notebook, if it provides a 
\GUI{} structured according to the 
basic \MdsX{} design.}
\input{pics/rad2}

\p{Whether or not \MdsX{} plugins serve 
as notebooks, they can be useful enhancements 
to host applications, allowing for 
more robust data-sharing and multi-application 
networking.  \lMOSAIC{} plugins, for example, 
would allow applications to send and 
receive data using the \HGXF{} format; 
and also to support workflows described 
via the \q{Hypergraph Multi-Application Configuration 
Language} discussed below.  These workflows allow different 
applications' capabilities to complement one 
another; a simple example is shown in 
Figure~\ref{fig:rad2}, where a 
\ThreeD{} tissue model is built 
from a \TwoD{} image series, 
and Figure~\ref{fig:rad3}, where the same 
\ThreeD{} model is exported to a \ThreeD{} 
graphics application.  Using \MOSAIC{} 
plugins allows the file transfer between 
the two applications to be performed 
automatically, rather than requiring 
users to manually save and then reopen the file.}

\p{Although minimal \MOSAIC{} plugins provide 
some useful functionality, the main goal of 
\MOSAIC{} is to support more complex, \GUI{} 
based interactive notebooks which can be 
plugged in to other applications as well 
as run standalone.  These notebooks will 
be discussed in the next section.}

\section[The Structure of MdsX Notebooks]{The Structure of \protect\lsMdsX{} Notebooks}

\p{A common feature of software through which users 
study and reuse research data sets is some 
form of \q{interactive notebooks,} often 
called \q{computational notebooks} --- 
digital resources combining data, code, 
and graphics/visuals.  The main feature of 
notebook-oriented design is the idea of 
interactive code editing, where changes in 
the code directly leads to changes in a visual 
display (such as a plot or diagram) which is 
viewed alongside the code.  This setup allows 
developers to present or demonstrate data 
sets, and associated code, in an exploratory 
and interactive manner.}

\p{The exact details of how notebooks are designed 
and implemented varies between different technologies, 
although the concept is most clearly associated 
with \Jupyter{}, which is a coding and presentation 
environment based on \Python{}.  Whatever the 
underlying programming environment, computational notebooks 
--- or as \MdsX{} uses the term, \q{interactive/digital 
notebooks} (\IDN{}s) --- have several software-engineering 
requirements, including a scripting environment and 
a data-visualization layer, wherein data sets 
or numeric models are transformed into \TwoD{} or 
\ThreeD{} graphics (charts, diagrams, etc.).  
Moreover, the scripting layer needs to be 
connected to the data-visualization layer so 
that scripts can modify the data-to-graphics 
transformations.  A further requirement is 
functionality to load pre-existing data sets from 
saved files or from a web resource.}\input{pics/rad3}

\p{Beyond these general features, \IDN{} 
programming can take different forms and 
prioritize different styles of user 
interaction.  The \MdsX{} approach recognizes 
that it is often more convenient to 
interact with applications through 
\GUI{} actions --- buttons, tabs, context 
menus, and so forth --- than by typing 
in commands (whether or not these are 
executed immediately in \REPL{}, or 
\q{read-eval-print-loop,} fashion, or are 
stored in scripts).  As such, \IDN{}s 
should not differ in design too noticeably 
from conventional \GUI{} windows or dialog 
boxes --- they should not be little more 
than \q{\REPL{}s with plots.}  On the other 
hand, rigorous \GUI{} programming 
calls for a carefully organized 
set of mappings from potential user 
actions to application responses.  
Whether on the scripting or 
the \GUI{} coding level, in short, 
implementations need a degree of abstraction 
more general than the underlying event-handling 
and procedure-calling logic which forms 
the application's concrete operational 
behavior.  This semi-abstracted layer 
can be described in terms of \q{meta-classes,} 
\q{meta-objects,} \q{tools,} \q{transitions,} 
\q{services,} and so forth: the common 
denominator in different contexts is some 
notion of a structure which can be called 
a \q{meta-procedure,} similar to an ordinary 
computational procedure in having inputs and 
outputs, but embodying a level of abstraction 
somewhat removed from concrete procedures.  
In particular, meta-procedures are not 
directly implemented; instead, some algorithm 
is necessary to determine, given a description 
of a meta-procedure with its outputs and 
context, what concrete procedure (or set of 
procedures) should actually run.  Moreover, 
meta-procedures need some notion of delayed 
execution: there is a logical gap between 
\q{marking} (using the language of petri-net 
theory), i.e., fully specifying the input 
parameters consumed by a meta-procedure, and a 
meta-procedure's actual execution.  
As such, meta-procedural markings can be 
built up in stages, with input data coming 
from multiple sources (including scripts and 
\GUI{} elements).  For a concrete example, 
consider the process of filling out a web 
form, wherein entries typed in to the form 
fields are validated, one at a time, 
before the form can be submitted.  
In these cases, the step-by-step process of 
entering and validating individual fields 
corresponds to incremental marking, and 
\q{hitting the submit button} corresponds 
to meta-procedure execution.}

\p{In short --- although different systems 
use different terminology --- any \IDN{} 
programming environment needs a mechanism 
to incrementally define and execute 
meta-procedure calls.  The 
implementational foundations of that 
mechanism (hypergraphs, workflow engines, 
state monads, etc.) depend on the underlying 
programming environment.  The \MdsX{} approach 
borrows ideas primarily from \HyperGraphDB{} and 
\SeCo{}, which is a notebook-programming environment 
based on \HyperGraphDB{}.  As in \SeCo{}  (and \Jupyter{}), units of 
marking and execution are called \textit{cells}.  
The main difference between \MdsX{} and \SeCo{}  (apart 
from \Cpp{} instead of \Java{} being 
the underlying programming language) is 
that \MdsX{} cells are not intended, in the general 
case, to be typed in by programmers directly.  
Instead, \MdsX{} cells are normally constructed 
behind the scenes, on the basis 
of \GUI{} component state, user actions, 
or scripting input.  However, once 
constructed, they can be manipulated 
like \SeCo{}  cells, both in terms of 
functionality and in terms of rationale: 
they can be used as a log of user 
actions, for undo/redo, for defining 
workflows, for generating scripts, 
and so on.  In particular, the 
mappings from \GUI{} actions to 
application handlers can be defined 
(and extended) by annotating the 
relevant \GUI{} elements with 
meta-procedure cells. This also 
allows data sets to be 
annotated with micro-citations 
(which were discussed in Part I).}

\p{As a \Cpp{} environment, \MdsX{} 
uses an embedded \q{virtual machine} 
to interpret meta-procedure cells; 
application-level event handlers are 
not automatically exposed to 
a scripting interface as they would be 
in a \JVM{} or \Python{} environment.  
However, \MdsX{} also supports scripting 
via a choice of languages, similar to 
\SeCo{}.  The primary scripting language 
used with \MdsX{} is \AngelScript{}, although 
other \C{}/\Cpp{} based languages 
(Embeddable Common Lisp, \ChaiScript{}, 
etc.) can work as well.  To support 
various scripting languages, modules 
loaded into \MdsX{} need to provide a 
meta-procedural interface declaration, 
and the desired scripting language also 
needs a bridge to work with these 
declarations (which is generally usable 
across all datasets and modules).  
Such a bridge will be provided by 
default for \AngelScript{} and \ECL{} 
(Embeddable Common Lisp), and similar 
tools could be implemented for other languages.}

\p{The typical \MdsX{} notebook combines, at a 
minimum, some graphical element
(although \q{object-based} notebooks 
are less graphically focused, as explained 
next section) --- 
such as an image to be analyzed and/or 
a plot/diagram to be populated with 
data --- along with a user-interface 
\q{panel} for interacting with 
the graphics.  This panel partially takes the place of 
a script-composition or \REPL{} frame, although 
such a frame is implicitly present, normally 
behind the scenes. 
%(users can view it if desired).  
Notebooks can then 
load data files, and representations of 
the loaded data (e.g., text serializations) 
may thereby also become part of the notebook 
content, able to be visualized in their 
own frame.  Notebooks in general, accordingly, can 
have four varieties of frames (graphics 
views, navigation panels, data panels, 
and meta-procedure logs) although not 
every available frame may be explicitly 
constructed and/or visible at a given point 
in the user's session.  There may also be 
multiple instances of graphics frames.  
In any case, the layout and state of 
these various frames --- what frames are 
visible, and their current content 
--- define notebook \textit{state} which 
can be saved, restored, and shared.  
Loading a data set into a \MdsX{} notebook 
therefore involves loading a particular 
initial state, defined as part of the 
data set, arranged in part to serve as 
a useful starting-point for users to 
explore and visualize the relevant data.  
Each of these kinds of frames corresponds 
to a particular aspect of software 
implementations, requiring its own 
strategies and paradigms.  The following 
sections will review these various 
programming concerns one at a time.}
\input{pics/oxyalt.tex}

\section{Image Analysis and Data Visualization}

\p{The central graphical element of an 
\MdsX{} notebook is either a \TwoD{} or \ThreeD{} 
image loaded from an image file (in formats 
such as \PNG{}, \JPEG{}, \DICOM{}, \TIFF{}, 
etc.), or else a \TwoD{} or \ThreeD{} plot, 
chart, or diagram.  The functionality of 
the notebook will therefore differ depending 
on whether the central graphics is an image 
loaded from a file (called 
an \q{image-based} notebook), or a data visualization 
constructed from a data set or some 
mathematical formulae (called a \q{diagram-based 
notebook}).  A third option are \q{object-based} 
notebooks, whose center viewports display 
structured information mostly in 
textual form (for example, a table or tree view), 
but this section will focus on graphics-oriented 
notebooks (Figure~\ref{fig:oxy} shows a contrast 
between an object-based view, in the background, 
and a graphics-based view, which has been 
opened floating above it).}

\p{Diagram-based \MdsX{} notebooks can be implemented with 
different diagram-plotting engines.  The default 
implementations support \Qt{} Charts (a built-in \Qt{} 
module) as well as the \textbf{qtcustomplot} and 
\textbf{JKQCustomPlotter} libraries.  Diagram-based 
notebooks need to implement subclasses of 
\MdsX{} frames for a navigation panel, graphics view, 
meta-procedure view, and data view, as well as a 
\q{\MdsX{} diagram} subclass occupying the diagram view.  
In this case, the primary responsibility of 
the meta-procedural layer is to interface with 
functionality provided by the diagram/plotting 
engine.  Since most coding details are derived from 
these engines' object models and classes, they lie 
mostly outside the scope of this paper.}

\p{Image-based notebooks, on the other hand, 
need to integrate several different 
areas of functionality.  As such, setting 
up the image view is only one step in 
constructing such a notebook; additional 
programming is needed to support image annotation, 
analysis, and feature extraction.  
In order to integrate these different layers 
of functionality, \MdsX{} provides a 
\q{Data Structure Protocol for Image-Analysis Networking} 
(\DSPIN{}) which defines communication rules 
between image-related software subsystems 
in Object-Oriented terms.  \DSPIN{} 
objects are comprised of four 
more specific objects or layers, describing 
different aspects of the shared image and 
how it should be processed.  These 
four layers are defined as follows:

\begin{description} 

\item[Metadata Layer]  This object presents 
metadata describing the image format and acquisition 
facts.  If the surrounding \DSPIN{} object represents an 
image series (rather than a single image), the 
metadata object would also declare the 
size of the collection and how individual 
images should be referenced.  The metadata 
should include a file path or resource identifier 
asserting where the image can be acquired from 
(which in the case of a series can be a zipped 
folder or a list of resource paths).  More 
specific metadata depends on the image or images' 
graphical format; to properly load images in 
most formats (such as \PNG{}, \JPEG{}, \TIFF{}, 
and \DICOM{}) applications need to specify 
details such as dimensions, resolution, and 
color depth.  Of course, some of this information 
is stored internally within the image file 
(depending on its format), but certain formats 
require some metadata to be shared along with 
the image itself (moreover, it is often convenient 
to have basic information available without needing 
to extract it from binary image streams).  Details 
concerning which forms of metadata are appropriate for 
which image formats can be determined based 
on image code libraries, such as 
\textbf{libpng}, \textbf{libtiff}, or 
\DICOM{} clients.  If both end-points of a 
\DSPIN{} communication have the same 
libraries installed, the sending application 
will have a clear idea of how much supplemental 
data is needed over and above what will be 
read from image files directly.  If there 
are uncertainties in library alignment between 
the two end-points, the sending application should 
consider serializing a more detailed summary of 
the image, providing any information that would 
ordinarily be read from the graphic file. 
  
\item[Annotation Layer]
Almost all image analysis --- whether done by 
humans or by software --- results in either 
some form of statistical representation of 
an image's properties, or a complex of 
data which presents information about 
(and may visually overlay) the image, 
particularly in the form of annotations.  
Image annotations are arrows, line segments, 
or \TwoD{} closed shapes that call attention 
to a point or region inside the image, 
usually with some additional label or commentary.  
The basis of each annotation is therefore 
some zero-dimensional or two-dimensional 
region (or a set of zero-dimensional 
control points; or, occasionally, a one-dimensional  
line or curve), so annotations require a mechanism 
for designating regions.\footnote{See 
\bhref{https://pubs.rsna.org/doi/full/10.1148/rg.324115080} 
as an attempt to standardize image-annotation descriptions.}  The same 
issues apply to asserting feature-vectors 
with respect to an image region rather than 
the image as a whole.  Consequently, 
both annotations and feature vectors 
can be seen as equivalent varieties of 
constructions which isolate and then 
define data structures on zero-, one-, 
and/or two-dimensional subimages 
(feature vectors on the entire image 
can accordingly be treated as a special 
case).    

\item[Contextual Layer]
Contextual information associated with an 
image can include metadata 
or supplemental details that are not 
directly relevant to the image, but 
convey facts about how the image connects 
to a broader context where it was obtained, 
and for what purpose.  An example of 
contextual data would be the part of 
\DICOM{} headers that include patient 
or clinical information, rather than 
metadata about image format or 
dimensions.  

\item[Procedural/Provenance/Objectives Layer]  Represents 
prior or future analyses or transformations 
relevant to the state of the image, their objectives, 
and their contribution to the current image 
(or to a different derived image-instance resulting from 
a transformation algorithm).\footnote{See  
\bhref{https://hal.archives-ouvertes.fr/hal-00805699/document} 
as an attempt to standardize image-processing objectives.}

\end{description}
}


\p{In \MdsX{}, \DSPIN{} objects are 
associated with image-based notebooks in 
that the notebook components 
(the navigation panel and graphics, data, 
and meta-procedural controllers) jointly refer 
to a common \DSPIN{} object for all 
image-related data.  Image-analysis routines 
conducted within the notebook may then, subsequently, yield 
additional data structures bundled into the 
overarching \DSPIN{} object.  This 
overarching object can accordingly be exported or 
saved, alongside (and as an extension of) 
notebook state.}

\p{Analytic operations available through an 
image-based notebook may be provided by 
the notebook itself, or by a host application 
where \MdsX{} is embedded.  In the latter 
case, the notebook needs to construct the 
proper calls to the host application, using 
the meta-procedural controller as a bridge 
to ambient capabilities.  For instance, if 
an \MdsX{} notebook is developed as a plugin 
to \CaPTk{}, the notebook would interface 
with the host \CaPTk{} application via the 
formats and programming constructs which 
\CaPTk{} recognizes (specifically, 
the Common Workflow Language and the 
\Qt{} signal/slot mechanism).  
This specific scenario --- embedding \MdsX{} 
in \CaPTk{} --- is employed as a demonstration 
and case-study for embedding notebooks 
in host application in general.  
The \CaPTk{} workflow protocol also forms a 
basis for the meta-procedural view and 
controllers, discussed next.}

\section[MdsX Meta-Procedure Controllers]{\protect\lsMdsX{} Meta-Procedure Controllers}

\p{The meta-procedural layer of an \MdsX{} notebook 
is responsible for handling events generated 
by a corresponding navigational panel, or at 
least those events which have a 
non-trivial impact on notebook/session 
state and data.  The visual representation of 
meta-procedural commands and history is 
provided by a meta-procedural \q{view,} which 
is normally invisible, but notebooks may 
choose to allow users to \q{unhide} this view.  
The meta-procedural controller is responsible 
for generating the meta-procedural view (if 
applicable) and responding to user events 
within this view; it is also responsible for 
maintaining an inventory of objects summarizing 
available metaprocedures, \GUI{} elements, 
and the mappings between them.}
\input{pics/about.tex}

\p{In general, the \GUI{} elements in these 
meta-procedural mappings are referred to as 
\q{visual objects,} and are represented 
in the meta-procedural controller context via 
application-unique identifiers (not raw pointers).  
Similarly, \q{meta-procedural objects} encapsulate 
information about meta-procedures themselves.  
This controller does not directly connect 
\GUI{} events to event handlers; instead, it 
receives information about these connections when 
the notebook is loaded.  The controller is however 
responsible for implementing \textit{incremental 
execution} wrapping event callbacks (or any other 
relevant procedure).  Incremental execution 
means that the controller may create 
temporary \q{execution contexts} and gradually 
build up the data which, given a sufficiently 
complete \q{marking,} can lead to the 
meta-procedure being \q{fired.}  
Incremental marking includes the possibility that 
notations of where values should be obtained 
may take the place of values themselves, 
prior to actual execution.\footnote{For 
example, validation of meta-procedural inputs 
might lead to the user being shown a notification, 
causing them to alter the inputs over all. 
These kinds of cases indicate why deferring 
the actual evaluation of expressions yielding 
the inputs should potentially be delayed until 
the metaprocedure execution.}  Each 
preliminary stage --- that is, each pre-firing 
addition to the execution context --- may in 
turn be generated by events originating elsewhere 
in the application (such as the 
navigation panel), and the meta-procedural controller 
should model both the history of these pre-firing stages 
and the origin and nature of the events which 
triggered them.  An execution context 
may then be \textit{reified}, representing the 
cumulative pre-firing stages as a data structure 
that can be matched to a meta-procedure's 
outcomes: for example, noting the 
inputs or steps producing the specific appearance 
of a diagram or image in the graphics view, 
or the parameters configured to instantiate 
a workflow model.  Reified meta-procedural 
execution contexts can then be shared as 
objects with components responsible for 
communicating or preserving information about the 
notebook.  For instance, if a notebook graphic is 
included in a publication, the reified context 
could then be associated with that image as an 
annotation, and used to reconstruct notebook 
state when the notebook is launched from a 
document viewer in the context of the 
published graphic.}
\input{pics/x2.tex}

\p{In general, the information represented 
by the meta-procedural controller is not only 
relevant for the reactive operations of the 
notebook, responding to user actions; 
it also serves to document the notebook's 
properties, and potentially to connect 
the notebook with data sets and/or 
publications.  Many operations which can be 
performed within a notebook are associated 
with a given scientific or theoretical 
concept, or a statistical parameter modeled 
within a data set.  As such, it is possible 
for the notebook to maintain a list of these 
concepts, so as to create an interactive 
glossary or to interoperate with a document 
viewer.  As an example, Figure~\ref{fig:oxy} shows a 
context-menu action based on the concept 
of \q{oxygenated air flow,} which is also 
discussed in the scientific article 
on which the depicted data set is based.  
This concept also has a visual expression in 
one table column shown (in the background) on 
Figure~\ref{fig:oxy}.  As demonstrated 
in Figure~\ref{fig:about}, the data-set application includes 
code to explain technical concepts in pop-up 
dialog boxes, and also to link to the 
page/paragraph in the article where that corresponding 
concept is first (or most thoroughly) defined/mentioned.  
Establishing these conceptual connections 
between an \MdsX{} notebook, data set, and 
technical publication is facilitated by 
annotating both meta-procedural capabilities 
and \GUI{} elements with references to 
relevant technical/scientific concepts; these 
annotations, when defined, are represented through the 
meta-procedural controller.}

\p{This specific genre of annotations --- applying to 
meta-procedure and visual objects which are 
linked to scientific concepts --- is included 
within a broader \MOSAIC{} annotation system, covered 
in Part III.  A rigorous overview of this annotation 
framework depends on describing the data-structuring 
protocols recognized by \MOSAIC{}, which are 
reviewed in the next section.}

\section[MdsX Data Controllers: Viewing Data Files in MdsX]{\lsMdsX{} Data Controllers: Viewing Data Files in \lsMdsX{}}
\p{The data view and controller in \MdsX{} 
notebooks are responsible for displaying 
textual serializations of data structures, 
particularly the specific data sets that 
form the core of a data-set application.  
Raw data, of course, may be serialized 
in many different formats, and 
spread over multiple files.  However, 
\MOSAIC{} natively recognizes its own 
data format based on hypergraphs  
(\HGXF{}) and provides a light-weigth database 
engine (\DgDb{}) for persistent 
storage of notebook state.  Thus, \HGXF{} 
can be used to serialize raw data viewed 
in a \MdsX{} notebook as well as overall 
notebook/application state.  Both 
\HGXF{} and \DgDb{} will be discussed 
in Part III.}

\p{Apart from simply showing the text contained 
in a file, data views should provide information 
about the file type and format --- in particular, 
should indicate the code libraries which are 
being used to parse the serialized files.  
In the ideal situation, an \MdsX{} notebook will link 
against (or include) a \Cpp{} library to read 
files in the associated format, and will also 
provide \Cpp{} classes to hold deserialized 
data.  In simple cases, each file handled 
by an \MdsX{} data controller corresponds 
to a list of \Cpp{} objects of the same type 
(in more complex cases the data would involve 
multiple types).  In either scenario, the 
\Cpp{} classes defining these de-serialized 
objects are distinct from those used to 
parse the files, but an obvious connection 
exists between these two groups of classes.  
The data controller is then responsible 
for documenting these inter-class connections, 
and also for helping users learn about the 
relevant classes if they so choose.  
Assuming an \MdsX{} notebook is embedded in 
\Qt{} Creator, this can include passing a 
message to the \Qt{} Creator application requesting that 
the relevant \Cpp{} files be loaded in to the 
file-view pane.}

\p{In addition to providing information about 
data files themselves, \MdsX{} notebooks 
may find it appropriate to provide data 
provenance summaries.  This is illustrated 
in Figures~\ref{fig:x2} and \ref{fig:i2}, 
showing two applications using a \MOSAIC{} 
plugin to interoperate.  Figure~\ref{fig:x2} 
demonstrates how users may view 
basic plugin info.  Figure~\ref{fig:i2} 
demonstrates a similar dialog box, but 
one which is specifically relevant to 
tracing data provenance; the dialog 
shows information about the file or 
files sent between the applications.}
\input{pics/i2.tex}

\p{When sharing data between two different applications, 
it is prerequisite that each application (called 
\q{end points}) have the 
capability to read the data/file format involved.  
This can be confirmed by checking that both 
end points include the same code libraries 
(for special-purpose data formats) or 
by translating the data to a generic format 
such as \XML{} or \JSON{}.  Many data formats 
can be encoded in multiple ways, some using 
special-purpose parsers and some based on \XML{} 
or other canonical resource types.  A flexible 
data-sharing protocol should therefore 
leave open the possibility that several 
\q{rounds} of communication may be required 
to determine how the sending application 
should encode the information to be transferred, 
so that the receiving application will be 
able to decode it.}

\p{Given these considerations, applications and 
plugins which seek to support multi-application 
networking should, for the most flexibility, 
support data sharing via both specialized, 
narrowly-designed data formats and 
generic, multi-purpose formats.  One 
rationale for using hypergraph-based 
data serialization is that hypergraphs 
present a common meta-model able to 
represent almost all computationally 
meaningful data structures; as a result, 
hypergraphs are (theoretically) superior 
to both tree-form constructions such 
as \XML{}, and tabular conventions such 
as \CSV{} and \SQL{}.  Accordingly, 
a maximally flexible data-sharing protocol 
would include the option of allowing 
hypergraph-based formats (such as \HGXF{}) 
as a fallback data encoding option, when 
it cannot be established that both 
endpoints of an exchange share a more 
specific data-format library.}

\p{Whether the applications are using 
more generic or more specific serialization 
formats, data exchange also requires 
that each application implement the requisite logic 
to deal with the data once it is 
deserialized.  For each data format, 
then, one can formally or informally 
define not only how the associated 
data structures are serialized/encoded, 
but also specify what is necessary for a 
code library which properly handles 
information from this data once it is 
deserialized/decoded.  This is the kind 
of information that would be presented 
to the user via \MdsX{} data views, along with 
the raw data.  It is also information that 
may need to be structurally represented 
for database or workflow engines.  Consider 
the case of a very heterogeneous data store, 
such as a Semantic Lake.  Such an 
information space is essentially a decentralized 
database capable of storing many different kinds 
of information, used in turn by numerous 
different applications.  In general, 
individual applications will only understand 
and interact with smaller parts of the 
overall data space: each aggregate unit of 
data is associated with a set of requirements 
that applications must satisfy so as 
to properly manage that particular information.  
For these reasons, properly modeling such 
application requirements is an important 
aspect of heterogeneous database engineering.}

\p{A defining feature of \textit{heterogeneous} 
data stores, as compared to more \textit{homogenous} 
database engines, is that no single schema or 
technological infrastructure can directly 
represent all of the information contained 
in the heterogeneous space.  This basic 
point is consequential for different aspects 
of the associated database engineering, such as 
how to efficiently save, validate, and query 
all the data in the information space.  
In conventional database theory, 
it is assumed that a single query engine 
will interact with all data in a database, 
and the database's logical and storage 
schema are constructed so as to optimize 
query evaluation (to get the right results, 
quickly).  When considering 
\textit{heterogeneous} data spaces, however, 
we have to anticipate cases where no 
single query-evaluation strategy 
(or set of algorithms) will be natively 
applicable to every aggregate data unit.  
Instead, query evaluation may need to 
be performed as a workflow spanning 
multiple software components, even multiple 
applications, each performing their 
own filter, transform, or \q{reduce} 
(in the map-reduce sense) operations. 
There is therefore an engineering overlap 
between \textit{query evaluation over 
heterogeneous data space} and 
\textit{multi-application workflows}.  
The database engineering technologies 
discussed in Part III attempt to 
make this connection explicit by 
integrating database technology with 
representations of workflows and 
multi-application networks, 
specifically through a newly 
designed configuration language 
(\HMCL{}), discussed in the next section.}


\part{Hypergraph Models for Database Engineering and 
Multi-Application Networking}

\section{Hypergraph Multi-Application Configuration Language}
\p{The purpose of \HMCL{} is to represent 
multi-application networking protocols with greater 
procedural detail and specificity than provided 
by traditional workflow or Interface Definition 
languages.  Within the scope of multi-application 
workflows, a good starting point are  
the protocols adopted by \CaPTk{}; also relevant are 
then analogous protocols connecting 
semi-autonomous software agents in other contexts: 
\ParaView{} extensions, \Qt{} Creator Plugins, \Octave{} 
scripts, and \ROOT{} modules, for example 
(\ROOT{} referring to the physics platform developed 
at \CERN{}) --- restricting attention to the 
\Qt{}/\Cpp{} ecosystem.  Further afield, a similar 
sense of semi-autonomy can be found in hybrid \GUI{} 
and scripting platforms such as \Jupyter{} (in the 
\Python{} context), \Kaggle{}, and \SeCo{}.  
These platforms are familiar to data visualization 
and machine-learning engineering (where a script 
may analyze data and a \GUI{} component follow up 
by presenting a chart or dataplot summarizing the 
analysis), as well as other quantitative 
domains outside the natural scientists 
(in financial services, for example, scripting 
formats such as \MQFour{} execute investment strategies 
while associated \GUI{} components present 
finance-related visual objects, such as candlestick charts.)  
Similar workflow models have been developed in 
theoretically-informed frameworks such as 
\VISSION{}, which is paired with programming constructs 
such as \q{metaclasses} and \q{dataflow 
interfaces}.\footnote{See \bhref{https://pdfs.semanticscholar.org/1ad7/c459dc4f89f87719af1d7a6f30e6f58dff17.pdf}; note that 
this is a different project than the ViSion Ontology 
referenced earlier.}     
The common element in all of these systems is the 
presence of a central \GUI{} application whose 
capabilities are enhanced by customizable extensions with 
their own analytic and/or \GUI{} features: these 
extensions are neither fully autonomous 
applications nor simple scripts that may automate 
certain capabilities of the main application but 
do not fundamentally extend its functionality.  
This intermediate position --- neither wholly 
autonomous nor functionally subservient --- 
is expressed by the idea of \textit{semi-autonomous} 
software agents.}


\p{Rigorous models of application networks among semi-autonomous 
components acquire an extra level of complexity precisely 
because of this intermediate status: protocol definitions 
have to specify both the functional interdependence 
and the operational autonomy of different parts of 
the application network.  We propose to 
address this complexity by adopting a hypergraph-based 
paradigm for procedural and type modeling, from 
which derives our proposed \HMCL{} 
format.\footnote{A preliminary characterization 
of hypergraph-based type systems (and their 
corresponding representation of detailed 
procedural signatures and requirements) 
can be found in Nathaniel Christen's chapter 
(chapter 3) in \textit{Advances in Ubiquitous Computing: 
Cyber-Physical Systems, Smart Cities and Ecological Monitoring}, 
edited by Dr. Neustein 
(see \bhref{https://www.elsevier.com/books/advances-in-ubiquitous-computing/neustein/978-0-12-816801-1}).} 
The goal of \HMCL{} is to define protocols for 
inter-application messaging by focusing on 
procedures enacted by both applications before and 
after each stage in the communication.  This 
is not (in general) a \q{Remote Method Invocation} 
or \q{Simple Object Access Protocol} where one 
application explicitly initiates a specific procedure 
for the second to perform; instead, the chain of 
procedure calls (which we call the multi-application
\textit{operational semantics}) is more indirect, 
with \textit{requests} and \textit{responses} that 
may be mapped to different procedure-sequences in 
different contexts.  Nevertheless, although 
one application does not need detailed 
knowledge of the other's internal procedure 
signatures (which would break encapsulation), 
a rigorous messaging protocol can be developed 
by specifying requirements on the relevant 
procedures implemented by each application.  
Developers can then explicitly state that 
a given set of procedures implements a given 
\HMCL{} protocol (the multi-application documentation 
thereby has more detailed information about 
application-specific procedures than each 
application has \visavis{} its peers).  The 
functional interdependence between applications can 
accordingly be modeled by defining protocols which 
must be satisfied by procedure-sets internal 
to each end-point --- the relevant information 
from an integrative standpoint is not the 
actual procedures involved, but confirmation 
that the applicable procedure sets adhere 
to the relevant multi-procedural protocol.}

\p{Procedural modeling is not only significant 
in the context of two or more separate applications 
sharing data; for some dimensions of software 
implementation even apart from multi-application 
networking, it is essential (or at least expedient) 
to employ rigorous procedural descriptions.  
One case in point is testing, particular 
integration testing and \GUI{} unit testing 
--- procedural models can help programmers 
both formulate and implement test suites.  
Indeed, testing in a \GUI{} context is 
more complex then testing libraries 
whose behavior can be analyzed via 
command-line programs; with \GUI{}s 
it is necessary to simulate or manually 
perform user actions for which a 
test entity seeks to demonstrate proper 
application response.  For \GUI{}s, correspondingly, 
testing requires either special-purpose 
versions of the application User Interface 
or a protocol wherein an external test suite 
communicates with the main application 
(or some combination of the two). 
Accordingly, in many 
cases the connection between a 
test suite and its target applications 
serves as a de facto multi-application network, 
so that workflow models can also be employed 
as testing engines (Figure~\ref{fig:testing} 
shows an example of a test suite which 
interacts with the data-set application showed 
in earlier screenshots via \TCP{} networking).}
\input{pics/testing}
 
\p{Furthermore, as discussed at the end of Part II, 
multi-application networking also overlaps 
with \textit{heterogeneous database engineering} 
insofar as multiple applications 
(or multiple semi-autonomous agents) need 
to share a common data store.  This 
means that information in the data space 
is characterized in part by procedural protocols 
guiding the use of particular aggregate data 
units.  The \THQL{} format, for 
representing hypergraph data in a database 
context, is designed in part to allow 
these protocol specifications to be 
encoded directly in the database.}

\section{Native/Desktop Application Development with THQL}
\p{As described earlier, \THQL{} is a database 
engineering protocol which prioritizes 
data-persistence components that can be 
included in source code fashion within application-development 
projects.  \lTHQL{} is \q{transparent} in that 
all layers of data persistence and 
query processing logic are provided 
via self-contained source-code libraries.  
The complete database functionality can 
then be statically examined via the source-code 
files, and dynamically examined by running the 
client application through a debugger.  Moreover, 
because all \THQL{} source code is bundled with 
application code, \THQL{} can be configured to 
integrate seamlessly into its environing client-application  
logic.  For example, \THQL{} can be 
extended to natively recognize client-specific 
datatypes as data fields, or to execute 
client algorithms as query parameters.}

\p{As a query \textit{language,} \THQL{} can 
be instantiated either by special languages 
with their own syntax and semantics 
(analogous to \SQL{} or \SPARQL{}), or 
as an interface and pattern specified 
for a conventional language, such as 
\Cpp{}.  In the latter guise, 
\THQL{} provides a common protocol for 
database tasks (e.g.  
constructing, updating, querying, 
and backing up database instances).  
Each procedure comprising the 
\THQL{} protocol is assigned a 
specific role, so that the protocol 
can be abstractly modeled as a 
set of data-management roles 
mapped to corresponding procedure 
implementations.  On this basis, 
custom query languages can be 
constructed by exposing each role-procedure 
to a scripting interface.  For example, 
\THQL{}'s Reference Implementation 
(DigammaDB) exposes the protocol-specific 
procedures via a set of pointers to \Cpp{} functions.  
Consequently, parsing a query language is then rendered a 
straightforward process of mapping query 
expressions to the requisite procedures, 
whose corresponding handle can then be 
obtained via \Cpp{} interop.}

\p{As suggested by its name, \THQL{} is 
centered around the operations to 
define and store hypergraph-form data: 
information which has several levels 
or scales of structuration.  
This means that the \THQL{} protocol 
includes procedures for registering 
individual data fields 
(representing, in general, primitive 
types such as integers and decimals) in a 
database; aggregating fields into 
\q{hypernodes,} or groups of interrelated 
information; connecting pairs of 
hypernodes by identifying a specific 
connection which they have; 
adding contextual details or annotations 
(via so-called \textit{frames} and \textit{channels}) 
which refine assertions of hypernode connections; 
and constructing \q{proxies} to database elements 
(e.g. hypernodes, frames, and subgraphs) which 
can be referenced (via unique identifiers) 
as individual data fields.  Since proxies 
can then be aggregated into hypernodes in 
turn, \THQL{} graphs can have, if desired, 
arbitrarily deep nested structures.}

\p{\THQL{} also recognizes additional structures 
corresponding to conceptual details described 
earlier --- for example, fields within 
hypernodes can be linked with 
dimensional attributes (e.g. scales and 
units of measurement) and identified 
as micro-citation targets.  \lTHQL{} likewise 
supports a genre of controlled vocabularies 
applying to hypernode-types and/or connection labels 
(called \q{dominions,} for \q{Domain-Specific Mini-Ontologies}).  
Consequently, a graph can then be, if desired, 
configured to only accept hypernodes 
which conform to one of the dominion-defined 
types; and/or to only allow connections to be 
asserted between hypernodes when these connections 
can be labeled from a dominion-specific list 
of connectors.  Less restrictively, graphs 
can be defined in a more free-form style 
but use dominions to filter or 
query nodes and edges.}

\p{Another feature of \THQL{}, relevant 
to application integration, is the notion of 
configuring each database to support different 
\q{modes} of data persistence.  It is possible
to use \THQL{} for completely in-memory data management, 
with no direct data persistence at all.  This would be 
an appropriate solution when data can be read all at once 
from a static source, such as a data set.  In this guise, 
\THQL{} would be used to build a structural model 
of the data set, which can then be queried by application 
code.  Conversely, it is possible to employ \THQL{} as a 
continuously-updated data store, where changes to 
an underlying \THQL{} graph are persisted to disk 
as soon as they are registered.  Between these extremes, 
\THQL{} graphs can hold dynamically changing representations 
of a persistent database, which are only incorporated 
into the underlying database when instructed by 
the client application.  To support these different 
operational modes, \THQL{} engines need the capability 
to represent each data type in several different 
formats, tailored to different stages of processing 
through which values are routed before they can be 
stored persistently.}

\p{In DigammaDB (the \THQL{} Reference Implementation), 
persistent data storage is implemented via the \WhiteDB{} 
database engine.  \WhiteDB{} is a hybrid 
graph/record database which allocates a persistent 
data store in shared memory (allowing each database to be 
accessed from multiple applications).  \lDgDb{} encodes 
hypernodes in \WhiteDB{} records (although programmers 
can interface with the underlying \WhiteDB{} instances if 
desired).  \lDgDb{} can then use \WhiteDB{}'s index and 
query mechanism as the foundation for its own 
higher-level query system.  \lDgDb{} provides a 
convenient interface for binary-encoding user-defined \Cpp{} 
types, so that arbitrary application-level data can be 
stored via \THQL{} (we can supply more documentation 
describing hypergraph-encoding with WhiteDB if needed).  
In addition --- as an alternative 
to writing serializers for bonafide \Cpp{} types --- 
it is possible to construct \q{ghost} types, which are 
\Qt{} data structures built via the \q{QVariant} class, 
so that all (or most) hypernodes in the corresponding 
database have a single \Cpp{} type --- this technique 
is appropriate when, for example, the purpose of a \THQL{} instance 
is to read and then update \HGXF{} files with new information.   
To support different \THQL{} operational modes, \DgDb{} organizes 
a stage-structure based on encoding \WhiteDB{} values: 
at the ground level, values are simply pointers 
to in-memory \Cpp{} objects.  At an intermediate level, 
values are encoded (via the QDataStream class in \Qt{}) 
into structures which recognize the \WhiteDB{} encoding 
scheme but do not themselves interact with \WhiteDB{}. 
Finally, values may be recorded as \WhiteDB{} fields 
and records ready for persistent storage.}

\p{\WhiteDB{} also allows databases to be shared 
(including being sent over a network) by storing all 
database information in a special file format.  
\lDgDb{} instances can be shared via this 
same mechanism, although another option is to 
export the contents of a \DgDb{} database to 
\HGXF{} files, which in turn form the core 
of a research data set representing the 
database contents at a specific moment in time.  
In this guise \DgDb{} works in conjunction 
with \dsC{}, serving as the engine to 
construct a data set through which research data 
curated via a DigammaDB database can be published 
(\dsC{} will be described in a later section).  
\lDgDb{} also demonstrates how hypergraph databases --- 
as well as data generated from these databases 
for data-sharing initiatives --- can 
store data constrained by \q{hypergraph ontologies,} 
which were discussed earlier.}

\input{ho}
\section{Dataset Creator and the Annotation Exchange Format}
\p{}

\p{Rigorously integrating data sets and publications 
demands new technologies both for composing documents 
and for organizing data sets, which are provided 
through the \MOSAIC{} portal and plugin framework.  
In particular, \MOSAIC{} provides an 
\q{Annotation Exchange Format,} (\AXF{}), 
which can be used to establish a detailed 
annotation meta-model across data and document 
repositories.  At the core of an \AXF{} 
Publication Repository is a collection of files 
in a machine-readable \AXF{} Document Format (\AXFD{}), 
which are paired with human-readable \PDF{} documents 
as well as supplemental multi-media and metadata files.  
Depending on institutional requirements, an \AXF{} repository 
may be the primary storage resource for the contained 
publications, or an adjunct resource whose documents are 
linked to publications hosted elsewhere.  In the second 
scenario, the primary goal of an \AXF{} repository is to 
host manuscripts in \AXFD{} format, along with software 
to aid viewing and text-mining of the associated publications.}

\p{\lAXFD{} therefore has two distinct purposes: (1) to 
aid in text and data mining (\TDM{}) of full 
publication text (along with research data that may be 
linked to publications), and (2) to enhance the reader 
experience, given e-Reader software (canonically, 
\PDF{} viewers) which are programmed to consume 
\AXF{} information.  To (1) aid in text mining, \AXFD{} documents 
can be compiled into different structured representations, 
yielding document versions that can be registered 
on services such as \CrossREF{} \TDM{} and SemanticScholar.  Given 
a Document Object Identifier, text-mining tools can therefore 
readily obtain a highly structured, machine readable version 
of the publication, which may then be used as the basis 
for further text-mining and \NLP{} operations.  Simultaneously, 
to (2) improve reader experience, the \AXF{} platform generates 
numeric data linking semantically significant text locations 
to \PDF{} viewport coordinates 
(such text locations include annotation, quotation, or citation 
start/end points and paragraph or sentence boundaries --- 
collectively dubbed a Semantic Document Infoset, or \SDI{}).  
This \SDI{}+Viewport (\SDIV{}) information can then be used by 
\PDF{} applications to provide contexts 
for word searches, to localize context 
menus, to activate multi-media features at different points 
in the text, and in general to make \PDF{} files more 
interactive.  Data sets composed with the aid of \AXF{} 
tools may include source code for a \PDF{} viewer (an extension 
to \XPDF{}) capable of leveraging \AXF{} data.}

\p{In addition to the \AXFD{} document format, \MOSAIC{} 
includes the Annotation Exchange Format itself, a protocol for 
defining and sharing annotations on full-text publications.  
\lAXF{} differs from other annotation-representation 
strategies by (1) providing more detail concerning 
the location of annotated text segments, in the 
surrounding publication context, and (2) supplying 
annotation data in multimedia or microcitation 
formats which extend beyond conventional 
\q{controlled vocabularies}.  In terms of (1) 
publication context --- that is, the annotation 
\textit{target} (using terminology from the 
Linguistic Annotation Framework, or \LAF{}) --- \AXF{} 
represents \SDIV{} information as introduced 
above; this data supplements the node/index coordinates 
used by traditional annotation mechanisms.  
With respect to (2) annotation metadata --- 
or the annotation \textit{body} (again using 
\LAF{} terminology) 
--- \AXF{} introduces models for multimedia 
assets, software components, and data set 
content, which may be linked to annotation 
targets.  With this additional metadata, 
annotations may be used in application-development 
environments, not only for text mining.}

\p{The following subsections will (1) outline \AXF{} and \AXFD{} 
in greater detail, (2) describe how \AXF{} repositories 
can unify publications sharing similar themes, scholarly 
disciplines, or coding requirements, and (3) 
describe features for data-set publication in 
the context of \AXF{} repositories.}

\subsection{AXF and AXF Documents}

\p{The \AXFD{} format for describing document content and 
structure is designed to be a \q{Pivot Representation} in the 
sense of \LAF{}.\footnote{See \bhref{https://www.cs.vassar.edu/~ide/papers/LAW.pdf}.}  In particular, \AXFD{} can represent 
the structure of both \XML{} (including several 
\XML{} flavors used in publishing) and \LaTeX{}.  
Technically, \AXFD{} does not prescribe any 
specific input format; instead, a document is 
considered an instance of \AXFD{} if it can be 
compiled into a Document Object satisfying 
interface requirements.  A \Cpp{} reference implementation 
anchors the \AXF{} Document Object Model; nodes in 
this implementation have facets combining 
\LAF{}, \XML{}, and \LaTeX{}.  In practice, 
\AXFD{} manuscripts are then converted via \LaTeX{} to 
\PDF{}, and simultaneously compiled to \XML{} representations 
so as to generate machine-readable, structured full-text 
versions of the manuscripts.  Authors can choose to 
compose \AXFD{} papers to conform with several common 
publication \XML{} standards, such as \JATS{} 
(Journal Article Tag Suite), \SciXML{}.\footnote{See 
\bhref{https://www.cl.cam.ac.uk/~sht25/papers/Rupp_et_al.pdf}.}, 
and \IeXML{}.\footnote{See \bhref{https://www.semanticscholar.org/paper/IeXML\%3A-towards-an-annotation-framework-for-semantic-Rebholz-Schuhmann-Kirsch/1d72a56b6576117c62f388a5f2193965e4c7e293}.}}

\p{One distinct feature of \AXF{} is that \LaTeX{} and 
\XML{} generation are chained in a pipeline: the \LaTeX{} 
and subsequent \PDF{} generation steps yield auxiliary data, 
which includes \PDF{} viewport data, that can be 
subsequently incorporated into \XML{} views onto 
the documents.  Specifically, 
\AXFD{}-generated \LaTeX{} files include notations for 
semantic annotations and for sentence boundaries, implemented 
via \LaTeX{} commands which, as one processing step, 
write \PDF{} coordinates to auxiliary files.  The resulting 
data is then read by a \Cpp{} program which collates 
annotations and sentence-boundaries into a vector of 
data structures indexed by \PDF{} page numbers, creating a 
distinct file for each page, and zips those files into an 
archive which can be distributed alongside (or embedded inside) 
the \PDF{} publications.  Simultaneously, sentences, 
paragraphs, annotations, and other semantically significant 
content (such as quotations and citations) are assigned 
unique ids and compiled into their own data structures 
(from which machine-readable \XML{} full-text may be generated).  
These \XML{} files may then be hosted and/or registered on 
\TDM{}-oriented services such as \CrossRef{}.  At the same 
time, unique identifiers unify this \XML{} data 
(focused on text mining) with \PDF{} viewport data 
(focused on reader experience).  The goal of such 
integration is to incorporate text-mining results 
so as to enhance reader experience.  For example, 
Named Entity Recognizers might flag a word-sequence 
as matching a concept within a controlled vocabulary.  
Via the relevant paper's \SDI{} model, this annotation 
may be placed in a proper semantic context --- for 
example, obtaining the text of the sentence where the 
Named Entity occurs.  This semantic information may then 
be used by a \PDF{} viewer --- e.g., providing a 
context menu option to select the sentence text, when 
the context menu is activated within the rectangular 
coordinates of the annotation itself.}

\input{pics/il}
\p{As a representation of annotation data structures, 
\AXF{} ensures that \SDI{} and viewport data is included 
among annotations wherever this data is available.  
This facilitates the integration between text-mining 
tools and \PDF{} viewer software, which in turn 
enhances reader experience.  As mentioned earlier, every 
annotation can be placed in a semantic context (e.g., 
the text of the surrounding sentence), which provides 
useful reader features such as one-click copying of 
sentences to the clipboard.  Other reader-experience 
enhancements involve multimedia assets.  As a concrete example, 
suppose a paper includes mention of a chemical; that particular 
keyword can accordingly be flagged for annotation. 
As one encoding of the corresponding scientific concept, 
the annotation can include the compound's Chemical Abstract 
Service (\CAS{}) Reference Number, via which it is possible to obtain 
Protein Data Bank (\PDB{})files to view the relevant 
molecular structure in \ThreeD{}.  In sum, 
annotations supply a constellation of data 
--- in this example, concepts may be linked not only to 
identifiers in cheminformatic ontologies, but also to \CAS{} 
reference numbers and thereby to \ThreeD{} graphics files 
--- which facilitate interactive User Experience at the 
application level, not only document classification at the 
corpus level.  Once a chemical compound (mentioned in a 
publication) is linked to a \PDB{} file (or any other 
\ThreeD{} format) the \PDF{} viewer may include 
options to for the reader to connect to software or 
web applications where the corresponding visuals can 
be rendered.  Via \AXF{}, the relevant document-to-software 
connections are asserted not only on the overall document 
level, but on the granular scale of the precise character 
and \PDF{} viewport coordinates where the relevant 
annotation is grounded (Figure~\bref{fig:il} illustrates 
such capabilities in the context of a chemistry publication 
--- specifically, test-preparation materials for the 
Chemistry \GRE{} exam).}

\p{To support this kind of multimedia functionality, 
\MOSAIC{} plugins allow programmers to embed code which can parse and 
respond to \AXF{} annotations in different scientific and 
document-viewer applications.  \lMOSAIC{} plugins 
enable different 
applications to inter-operate; in particular, 
\PDF{} viewers can share data with scientific applications 
that can render files in domain-specific formats such as 
\PDB{}.  This application networking protocol is considered 
part of the \AXF{} annotation model, because application-oriented 
information is computationally relevant for many concepts 
encountered in scientific and technical environments.  For instance, 
one aspect of cheminformatic data is that many chemical 
compounds are modeled by \PDB{}, \MOL{}, or \ChemXML{} files, 
which in turn are associated with software applications that 
can load those file types.  Such inter-application 
networking data is relevant to \PDF{} viewers 
when they display manuscripts with annotations that suggest 
links to special file types and their applications; the 
viewers can employ this information to launch and/or communicate 
with the corresponding software.  \lAXF{} is designed to 
facilitate implementation of application-networking protocols 
as an operational continuation of processes related to obtaining 
and consuming annotation data.}

\p{To take full of advantage of \MOSAIC{} and \AXF{}, 
data sets and data-set applications need their own 
micro-citations, constructed so as to interoperate 
according to the \MOSAIC{} paradigm.  One way to 
achive this is via the Dataset Creator, discussed next.}

\subsection{Dataset Creator}
\p{Dataset Creator (\dsC{}) is a framework for 
building data sets which include computer code based on the 
\Qt{} application-development platform.  
Dataset Creator takes advantage of the \Qt{} platform 
to construct Research Objects with exceptional 
\GUI{} and data-mining capabilities.  \lQt{}, the 
leading native cross-platform development toolkit, 
is a comprehensive framework encompassing 
a thorough inventory of programming features 
--- networking, \GUI{} implementation, file 
management, data visualization, \ThreeD{} graphics, 
and so forth.  Data sets based on \Qt{} require 
users to obtain a copy of the \Qt{} platform, but 
\Qt{} is free for non-commercial use and easy to 
install --- importantly, \Qt{} is wholly contained 
in its own folder and does not affect any other 
files on the user's computers (in this manner \Qt{} is 
different than most software packages, which usually 
demand a \q{system install}).}

\p{By leveraging the \Qt{} platform, \dsC{} enables 
standalone, self-contained, and full-featured 
native/ desktop applications to be uniquely implemented 
for each data set, distributed in source-code fashion 
along with raw research data.  Adopting such a 
data-curation method makes data sets easier to 
use across a wide range of scientific disciplines, 
because the data sets are freed from having to rely on domain-specific 
software (software which may be commonly used in one scientific 
field but is unfamiliar outside that field).  
In addition, Research Objects composed with \dsC{} can 
be integrated into Multi-Application Networks  
(which are described above) because 
the dataset applications are autonomous native \GUI{} applications that 
can easily interoperate via \Qt{} messaging protocols.}

\p{Because every data set is unique, each Dataset Application 
will necessarily include some code specific to that one 
Research Object.  However, \dsC{} will provide a core 
code base and file layout which is shared by all 
\dsC{} data sets by default.  This common core is 
structured in part by the goal of developing 
Dataset Applications in a \Qt{} context; for 
instance, \dsC{} projects are structured to use \Qt{}'s \q{qmake} 
build system as the primary tool for compiling 
data-set code.  The common \dsC{} code therefore 
includes qmake project files which support 
compiling application with several build configurations.  
In this framework, data-set users are classified into several 
different roles --- in addition to ordinary users 
(specifically, researchers who want to work with 
and draw information from 
data sets but have no development connection to 
these data sets themselves), \dsC{} recognizes 
roles for authors, editors, testers, and other 
users who are responsible for bringing data 
sets into publication-ready form to begin with.  
Depending on the administrative role, data set 
code can be compiled with additional 
features (e.g., unit testing features).}

\p{Another core component of \dsC{} is \LaTeX{} 
code that authors may use when preparing documents 
accompanying their data set.  These \LaTeX{} files 
encompass special functionality for defining code 
annotations and semantically significant points in 
article text, such as sentence and paragraph 
boundaries.  This \LaTeX{} code can be used 
in conjunction with a pre-processor that 
generates \LaTeX{} files from a special input 
language.  The goal of these text-processing 
technologies is to improve the interoperability 
between research papers and data sets.  In particular, 
the \LaTeX{} pre-processors (and subsequent 
\LaTeX{}-to-\PDF{} converters) generate \HGXF{} 
files which store information about textual and 
\PDF{} viewport coordinates specifying the location of semantically 
meaningful elements such as sentences and annotations.  
These files are then zipped and embedded in 
\PDF{} files.  With \dsC{}, the resulting \PDF{} files can 
then be loaded into a customized \PDF{} viewer capable of 
reading the embedded \HGXF{} data.  This allows the 
\PDF{} application to utilize the embedded information 
so as to provide a more interactive reading 
experience --- for instance, viewing annotations 
or copying sentences via context menus, where viewport 
data maps cursor position to textual elements visible 
on the \PDF{} page.  These features provide an 
application-level interface between the \PDF{} viewers 
(considered as \GUI{} components) and the corresponding 
\GUI{} components in Dataset Applications.}

\p{With proper customization, both the \PDF{} viewers 
and the \dsC{} Dataset Applications can interoperate, 
with \PDF{} context menus calling up windows in 
the Dataset Application's \GUI{} implementation, 
and vice-versa.  For example, researchers reading 
the \PDF{} version of a scientific article can 
launch the Dataset Application to explore some 
detail mentioned in the text.  This is an example of 
where micro-citations are practically useful: 
any microcitable element in a document (such as a table, 
column, row/record, or analytic procedure formalized as a 
procedural asset associated with a data set) 
can be linked to a corresponding \GUI{} element 
in the Dataset Application.  For example, a statistical 
parameter --- mentioned by name in the text, and perhaps 
represented in serialization within raw data --- can 
be mapped to a \GUI{} table column, and specifically 
the column header; this is then an annotation target, 
in the sense that for readers to gain more information it 
is proper to link mentions of the relevant scientific 
concept in article text to the column header as a 
graphical element that can be made visible.  The 
link is operationalized by implementing a procedure 
to show the \GUI{} window where the table is located, 
and ensure that the column header lies in the visible 
portion of the screen, as a response to readers on the 
\PDF{} side signaling a desire for information on 
the annotated text element.  In the opposite direction, 
database elements can be annotated with 
links that can used by the Dataset Application to launch 
a \PDF{} window opened to the page and location where 
the corresponding concept is discussed in the article.}

\p{In order to properly model this semantic, 
viewport, and data set data integration, 
\dsC{} uses a new document-representation format 
called \HTXN{} (Hypergraph Text Encoding Protocol).  
With \dsC{}, \HTXN{} files are not only associated 
with data set assets; they are also machine-readable 
document encodings that can be introduced into 
publication repositories and other corpora 
oriented toward machine-readability.  Authors can 
host \HTXN{} files within data sets and link to 
them via services such as \CrossRef{}, thereby 
ensuring that a highly structured, machine-readable 
version of their papers is available for 
text and data mining.  The \HTXN{} protocol is 
also useful for encoding natural-language content 
which becomes part of a data set as data assets 
in themselves; for example, patient narratives.} 

\subsection[The Hypergraph Text Encoding Protocol (HTXN)]{The Hypergraph Text Encoding Protocol (\protect\lsHTXN{})}

\p{Within the \HTXN{} protocol, an annotation target is 
a character-index interval in the context of an 
\HTXN{} character stream.  On that basis, 
\HTXN{} treats documents as graphs whose nodes 
are ranges in a character stream, where text can 
be recovered as an operation on one or more nodes 
(e.g., the text of a sentence is derived from a 
pair of nodes representing the sentence's start 
and end).  \lHTXN{} code-points 
are distinguished in terms of their semantic 
role, which may be more granular than their 
visible appearance --- for example, 
a period glyph is assigned different code-points 
depending on whether it marks a sentence-ending 
punctuation, an abbreviation, a decimal point, 
or part of an ellipsis.  Procedures are then implemented to 
represent text in different formats, such as 
\ASCII{}, Unicode, \XML{}, or \LaTeX{}.  In 
contrast to a format such as Web Annotations, 
any particular human-readable text presentation 
(including \ASCII{}) is considered a \textit{derived} 
property of the annotation, not a foundational 
representation.}

\p{Apart from its overarching philosophy, 
\HTXN{}'s distinguishing features include:

\begin{enumerate}
\item{} A unified document-structure model which accommodates 
the idiosyncracies of different markup styles, 
such as attribute child nodes in \XML{}, 
optional arguments in \LaTeX{}, subject defaults in 
\RDF{}, and concurrent/overlapping markup as in \TeXMECS{}. 

\item{} An encoding built from the ground up to support 
multi-party editing and collaboration.  Internally, 
\HTXN{} uses \q{stand-off} annotations which logically 
separate markup content from the underlying text.  
Multiple sources can thereby contribute distinct 
markup structure, which may be used, for example, 
to differentiate document changes made by 
authors from those made by editors.

\item{} A convenient architecture for \q{subordinate} documents which 
expose some restricted portion of their \q{parent} documents.  
This is also useful for editing.  Starting with an overall 
\HTXN{} parent, one can selectively create a subordinate document 
which (for example) includes only editable text, yielding 
a simplified version of the document suitable for 
textual revisions.  After editing, changes made to the 
subordinate document are then folded into its parent.

\item{} A detailed system for pairing documents with external 
data and data sets.  In particular, \HTXN{} supports document 
anchors allowing structured data within a data set to 
reference locations in accompanying publications.  These 
back-references could be used, for example, to identify 
points in a publication where there is a definition, 
explanation, review, or visualization of technical concepts 
and/or data aggregates introduced in a data set.  Unlike 
\LaTeX{} references or \HTML{} anchors, these \HTXN{} anchors 
are data-specific and logically separated from textual 
cross-references. 

\item{} Several special features for data-based annotations, 
or ascribing metadata or interpretations to text 
segments which conform to structured data types 
(proper names, acronyms, dates, times, magnitudes, etc.).  
This includes numeric types typically unrepresented 
in most markup formats, such as range-delimited 
integer types and \q{Universal Numbers}, or \q{posits}, 
a recent alternative to floating-point decimals.  

\item{} Functionality for creating data sets directly 
from the text itself.  For instance, example 
sentences used within linguistic publications, 
to illustrate semantic or syntactic principles, 
can be compiled as a sentence-corpus to produce 
a distinct data set associated with the 
original publication.  Larger corpora can then 
be compiled from collections of linguistic 
texts.  Similar techniques can be employed 
in analogous \q{digital humanities} contexts; 
for example, extracting a corpus of discussions 
about cultural artifacts by extracting sentences 
or paragraphs pertaining to an 
externally identifiable object (such as an 
artwork held in a museum).   

\item{} A framework for \q{complex microcitations}, meaning 
individually citable locations in a document which involve 
multipart data structures.  A case study would be linguistic 
examples that display (alongside or within a sentence) supplemental 
visual cues such as intonation, prosody, lexical category, 
morphology, or grammatical-structural annotations.  
Such material needs special typesetting instructions, but 
the visual format in turn depends on formal sentence 
models.  For another example, graphics such as charts 
or plots may represent a specific statistical 
view on a data set; the construction of that distinct 
perspective marshals formal data, such as dependent 
and independent variables, axes, scales, scaling 
factors (e.g. scalar multipliers or logarithms), and 
ranges.  In these examples, textual content is not only 
a presentation layer which merely exposits or 
reviews scientific frameworks; instead, 
the code describing textual structure and appearance 
reflects scientific paradigms and is therefore 
a theoretical artifact in its own right.
In particular, textual anchors and 
typesetting reflects not only raw text but structured 
data whose format and provenance represents specific 
scientific models.  Here it is valuable to 
cross-reference text with data sources:  
typesetting code may be generated directly from a 
data set, while the generated instructions (in \LaTeX{}, 
say) can trigger anchor labels and page references 
to be \q{forward-referenced} into the data set via 
auxiliary files.  Complete cross-references 
thereby integrate raw and textual data, 
yielding \textit{bi-directional} complex microcitations.  
 
\item{} A \Qt{}-based Reference Implementation which conveniently 
documents the \HTXN{} specifications.  With no external 
dependencies apart from \Qt{} Creator (a \Cpp{} Integrated 
Development Environment), authors can construct \HTXN{} documents 
via a special high-level markup language 
(called \NGML{}, or \q{Next-Generation Markup Language}) and, 
if desired, examine all parsing and encoding code 
(both statically and, dynamically, during document creation).  
The Reference Implementation also supports importing and 
conversions to \HTXN{} from \XML{}, using \Qt{}'s \XML{} module. 

\item{} Options for stashing compiled \HTXN{} documents in a 
database, so that saving files containing text serialization 
(in \NGML{}, for example) is not a prerequisite for maintaining 
persistent copies of \HTXN{} documents.  The Reference 
Implementation supports persistent 
\HTXN{} documents via the \DigammaDB{} database engine.

\item{} A flexible parsing engine which allows input languages 
for generating \HTXN{} documents to be customized and extended.
\end{enumerate}
}

\p{\lAXFD{} manuscripts do not need to utilize \HTXN{} 
for character data, but \HTXN{} simplifies certain \AXF{} 
operations, such as identifying sentence boundaries.  
In particular, \HTXN{} provides distinct code-points for 
end-of-sentence punctuation, so that sentence-boundary 
detection reduces to a trivial search for those particular 
code-points.  Proper \HTXN{} encoding requires that authors 
follow certain simple heuristics --- e.g., that end-of-sentence 
periods should be followed by two spaces and/or a newline, 
whereas other uses of a period character should precede at 
most one space.  Aside from the goal of preparing documents 
for text-mining machine-readability, such conventions are 
appropriate even for basic typesetting, because non-punctuation 
characters have their own kerning rules (this is why 
\LaTeX{} provides a distinct command for non-punctuation glyphs 
that would otherwise be read as punctuation characters).  \lHTXN{} 
hides these typesetting details within its character-encoding 
schema, which is then useful both for producing professional-caliber 
\LaTeX{} output and for identifying \SDI{} details (such as 
sentence boundaries) which with less rigorously structured 
text would need elaborate text-mining or \NLP{} algorithms.}

\p{Each \AXFD{} document is, in sum, associated with an 
aggregate of character-encoding, annotation, document-structure, 
and \PDF{} viewport information.  The \MOSAIC{} platform uses 
code libraries to pull this information together as a 
runtime object system, so that any application which loads 
an \AXFD{} manuscript can execute queries against the 
corresponding collection of \AXF{} objects (queries such 
as obtaining the sentence text around an annotation, obtaining 
the concave-octagonal viewport coordinates for a sentence,\footnote{
In the general case, sentence coordinates are concave octagons because 
they incorporate the line height of their start and end lines; 
in the general case sentences share start and end lines with other 
sentences, while also including whole lines vertically positioned 
between these extrema.  A sentence octagon roughly corresponds with 
the screen area where a mouse/pointer action should be understood as 
occurring in the context of that sentence from the user's point 
of view --- implying that the user would benefit from context menu 
options pertaining specifically to that sentence, such as 
copy-to-clipboard.} obtaining application-networking 
information for an annotation, 
etc.)  In addition to such runtime data, \MOSAIC{} libraries can 
compile the full suite of information into machine-readable 
files for text and data mining.}

\p{\AXF{} uses a two-tier node structure similar to 
\LAF{}; at one level is an extensible text-encoding 
methodology (outlined earlier in the context of 
\HTXN{}), while a higher level defines annotations in terms 
of directed hypergraphs.  Programmatically, 
\AXF{} aims in the canonical case for a level of 
detail that is intermediate between linked-data-oriented 
projects like Web Annotations (which tend to focus 
mostly on isolatable semantic resources such 
as citations and named entities) and \NLP{}-oriented 
paradigms such as \LAF{}.  That is, \AXF{} does not 
natively serialize fine-grained \NLP{} data at the 
level of individual words (the kind of data asserting 
semantic and morphosyntactic details: lemmatization, Part of Speech, 
dependency relations, and so forth), although it does support 
queries which return sentences as (unparsed) word-sequences.  
On the other hand, \AXF{} offers \text{some} granular information about small-scale linguistic units, such as the 
role of non-alphanumeric characters, or 
\PDF{} coordinates of sentence start and end points.  
In short, \AXF{} occupies a unique space in the 
landscape of annotation tools at the intersection 
of application-development, \NLP{}, and document-preparation 
requirements.}

\p{Further documentation of text-encoding methodology (applicable 
to both patient narratives and publications associated 
with \CRtwo{} research data) is available on the \CRtwo{} 
web site: such as \href{https://raw.githubusercontent.com/Mosaic-DigammaDB/CRCR/master/cr2.pdf}{here} (this is a downloadable \PDF{} link; 
visit the repository to see the larger archive structure).  
The document referenced in this hyperlink contains more information 
on \dsC{}, \HGXF{}, \HTXN{}, and the other technologies 
discussed here.}

%\input{apx1}

\input{apx2}
\input{apx3}

%\vspace{-.75em}
%\part{Conclusion}
%\vspace{-1.25em}
%\p{Most of the code described here will be developed 
%in conjunction with the Covid-19 Elsevier Volume, 
%in part coinciding with the publication of the book 
%and continuing to be refined and expanded after the fact.  
%Some of the more technical programming areas, 
%such as cytometry and image processing libraries, 
%will hopefully be generalized to multi-purpose 
%toolkits not limited to the Covid-19 context.  
%Feedback on \CRtwo{} overall, and the more 
%specific technical areas in particular, is welcome.}

\end{document}


